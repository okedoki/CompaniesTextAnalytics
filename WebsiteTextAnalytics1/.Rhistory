library(tm)
library('tm')
librar
load
install.packages('tm')
load(tm)
library(tm)
websitesFile <- 'websitedata.txt'; df <- read.table(websitesFile, sep = '\t', header = F, skip = 2, quote = '', comment = '');
websitesFile <- 'websitesdata.txt'; df <- read.table(websitesFile, sep = '\t', header = F, skip = 2, quote = '', comment = '');
library(tm) websitesFile <- 'websitesdata.txt'; df <- read.table(websitesFile, sep = '\t', header = F, skip = 2, quote = '', comment = '');
?read.table
df <- read.table(websitesFile, sep = '\t', header = F, skip = 2, quote = '', comment = '', fill = TRUE);
df
websiteDataFrame <- read.table(websitesFile, sep = '\t', header = F, fill = TRUE);
class(websiteDataFrame)
head(websiteDataFrame)
websiteDataFrame(1)
websiteDataFrame.head(1)
head(websiteDataFrame,1)
head(websiteDataFrame,2)
head(websiteDataFrame)
str(websiteDataFrame)
websiteDataFrame <- read.table(websitesFile, sep = '\t', header = F, fill = TRUE, stringsAsFactors = FALSE);
str(websiteDataFrame)
head(websiteDataFrame[c(2:5)])
head(websiteDataFrame[c(2:6)])
head(websiteDataFrame[c(2:4, 6)])
websiteDataFrame <- head(websiteDataFrame[c(2:4, 6)]) names(websiteDataFrame) <- c('Host', 'Webpage', 'Title', 'Text')
head(websiteDataFrame)
websiteDataFrame[1, 'Text']
websiteDataFrame[2, 'Text']
websiteDataFrame[3, 'Text']
textExample <- websiteDataFrame[1, 'Text']
textExample
tm_map(textExample, removeNumbers)
docs <- Corpus(VectorSource(textExample))
docs
str(docs)
doc
docs$content
docs$content[[1]]
docs
head(docs)
str(docs)
docs[1]$content
docs[1]$content$Content
docs[1]$content[Content]
docs[1]$content['Content']
docs[[1]]
docs[[1]][1]
websiteDocs <- tm_map(websiteDocs, removeNumbers) websiteDocs <- tm_map(websiteDocs, removeWords, stopwords("english")) websiteDocs <- tm_map(websiteDocs, removePunctuation)
websiteDocs <- Corpus(VectorSource(textExample)) websiteDocs[[1]][1] websiteDocs <- tm_map(websiteDocs, removeNumbers) websiteDocs <- tm_map(websiteDocs, removeWords, stopwords("english")) websiteDocs <- tm_map(websiteDocs, removePunctuation)
websiteDocs[[1]][1]
websiteDocs <- tm_map(websiteDocs, stripWhitespace)
websiteDocs[[1]][1]
install.packages('SnowballC')
library("SnowballC")
websiteDocs <- tm_map(websiteDocs, stemDocument)
websiteDocs[[1]][1]
?tm_map
?tm_map
websiteDocs <- tm_map(websiteDocs, removeWords, c("qqpbreakqq"))
websiteDocs[[1]][1]
websiteDocs <- Corpus(VectorSource(textExample)) websiteDocs <- tm_map(websiteDocs, removeNumbers) websiteDocs <- tm_map(websiteDocs, removeWords, c(stopwords("english"), "qqpbreakqq")) #Remove qqpbreakqq websiteDocs <- tm_map(websiteDocs, removeWords, c("qqpbreakqq"))
websiteDocs[[1]][1]
websiteDocs <- tm_map(websiteDocs, stripWhitespace)
#websiteDocs[[1]][1]
websiteDocs[[1]][1]
dtmWebsiteDocs <- TermDocumentMatrix(websiteDocs)
dtmWebsiteDocs
str(dtmWebsiteDocs)
dtmWebsiteDocs <- TermDocumentMatrix(websiteDocs) m <- as.matrix(dtmWebsiteDocs) v <- sort(rowSums(m), decreasing = TRUE) d <- data.frame(word = names(v), freq = v) head(d)
d
head(d)
dtmWebsiteDocs <- TermDocumentMatrix(websiteDocs) m <- as.matrix(dtmWebsiteDocs) v <- sort(rowSums(m), decreasing = TRUE) d <- data.frame(word = names(v), count = v)
ggplot +   aes(x = 1, y = 1, size = count, label = word) +   geom_text_repel(segment.size = 0, force = 100)
library(ggplot2) library(ggrepel) ggplot +   aes(x = 1, y = 1, size = count, label = word) +   geom_text_repel(segment.size = 0, force = 100)
ggrepel
load.library(ggrepel)
install.packages('ggrepel')
library(ggplot2) library(ggrepel)
ggplot +   aes(x = 1, y = 1, size = count, label = word) +   geom_text_repel(segment.size = 0, force = 100)
ggplot +   aes(x = 1, y = 1, size = count, label = word) +   geom_text_repel(segment.size = 0, force = 100) +   scale_size(range = c(2, 15), guide = FALSE) +   scale_y_continuous(breaks = NULL) +   scale_x_continuous(breaks = NULL) +   labs(x = '', y = '') +   theme_classic()
d <- data.frame(word = names(v), count = v) ggplot(data = d) +   aes(x = 1, y = 1, size = count, label = word) +   geom_text_repel(segment.size = 0, force = 100) +   scale_size(range = c(2, 15), guide = FALSE) +   scale_y_continuous(breaks = NULL) +   scale_x_continuous(breaks = NULL) +   labs(x = '', y = '') +   theme_classic()
ggplot(data = d) +   aes(x = 1, y = 1, size = count, label = word) +   geom_text_repel(segment.size = 0, force = 100) +   scale_size(range = c(2, 15), guide = FALSE) +   scale_y_continuous(breaks = NULL) +   scale_x_continuous(breaks = NULL) +   labs(x = '', y = '') +   theme_classic()
ggplot(data = d) + +aes(x = 1, y = 1, size = count, label = word) + +geom_text_repel(segment.size = 0, force = 100)
library(tm) websitesFile <- 'websitesdata.txt'; websiteDataFrame <- read.table(websitesFile, sep = '\t', header = F, fill = TRUE, stringsAsFactors = FALSE); #deleting of not usefull fields.  websiteDataFrame <- head(websiteDataFrame[c(2:4, 6)]) names(websiteDataFrame) <- c('Host', 'Webpage', 'Title', 'Text') #TODO - create a loop textExample <- websiteDataFrame[1, 'Text'] #Create a document for each website websiteDocs <- Corpus(VectorSource(textExample)) websiteDocs <- tm_map(websiteDocs, removeNumbers) #Remove qqpbreakqq? websiteDocs <- tm_map(websiteDocs, removeWords, c(stopwords("english"), "qqpbreakqq")) #websiteDocs <- tm_map(websiteDocs, removePunctuation) websiteDocs <- tm_map(websiteDocs, stripWhitespace) library("SnowballC") websiteDocs <- tm_map(websiteDocs, stemDocument)
textExample
library(tm) websitesFile <- 'websitesdata.txt'; websiteDataFrame <- read.table(websitesFile, sep = '\t', header = F, fill = TRUE, stringsAsFactors = FALSE); #deleting of not usefull fields.  websiteDataFrame <- head(websiteDataFrame[c(2:4, 6)]) names(websiteDataFrame) <- c('Host', 'Webpage', 'Title', 'Text') #TODO - create a loop textExample <- websiteDataFrame[1, 'Text'] #Create a document for each website websiteDocs <- Corpus(VectorSource(textExample)) websiteDocs <- tm_map(websiteDocs, removeNumbers) #Remove qqpbreakqq? websiteDocs <- tm_map(websiteDocs, removeWords, c(stopwords("english"), "qqpbreakqq")) #websiteDocs <- tm_map(websiteDocs, removePunctuation) websiteDocs <- tm_map(websiteDocs, stripWhitespace) library("SnowballC") websiteDocs <- tm_map(websiteDocs, stemDocument)
websiteDocs[[1]][1]
dtmWebsiteDocs <- TermDocumentMatrix(websiteDocs) m <- as.matrix(dtmWebsiteDocs) v <- sort(rowSums(m), decreasing = TRUE) d <- data.frame(word = names(v), count = v) head(d) # Generate the WordCloud library(ggplot2) library(ggrepel)
ggplot(data = d) +   aes(x = 1, y = 1, size = count, label = word) +   geom_text_repel(segment.size = 0, force = 100)
ggplot(data = d) +   aes(x = 1, y = 1, size = count, label = word) +   geom_text_repel(segment.size = 0, force = 100) +   scale_size(range = c(2, 15), guide = FALSE) +   scale_y_continuous(breaks = NULL) +   scale_x_continuous(breaks = NULL) +   labs(x = '', y = '') +   theme_classic()
ggplot(data = d) +   aes(x = 1, y = 1, size = count, label = word) +   geom_text_repel(segment.size = 0, force = 100) +   scale_size(range = c(2, 15), guide = FALSE)
ggplot(data = d) +   aes(x = 1, y = 1, size = count, label = word) +   geom_text_repel(segment.size = 0, force = 100) +   scale_size(range = c(2, 15), guide = FALSE) +   scale_y_continuous(breaks = NULL) +   scale_x_continuous(breaks = NULL)
ggplot(data = d) +   aes(x = 1, y = 1, size = count, label = word) +   geom_text_repel(segment.size = 0, force = 100) +   scale_size(range = c(2, 15), guide = FALSE) +   scale_y_continuous(breaks = NULL) +   scale_x_continuous(breaks = NULL) +   labs(x = '', y = '') +   theme_classic()
ggplot(data = d) +   aes(x = 1, y = 1, size = count, label = word) +   geom_text_repel(segment.size = 0, force = 100) +   scale_size(range = c(2, 15), guide = FALSE) +   scale_y_continuous(breaks = NULL) +   scale_x_continuous(breaks = NULL) +   labs(x = '', y = '') +   theme_classic()
ggplot(data = d) +   aes(x = 1, y = 1, size = count, label = word, col = count) +   geom_text_repel(segment.size = 0, force = 100) +   scale_size(range = c(2, 15), guide = FALSE) +   scale_y_continuous(breaks = NULL) +   scale_x_continuous(breaks = NULL) +   labs(x = '', y = '') +   theme_classic()
ggplot(data = d) +   aes(x = 1, y = 1, size = count, label = word, col = count) +   geom_text_repel(segment.size = 0, force = 100) +   scale_size(range = c(2, 15), guide = FALSE) +   scale_y_continuous(breaks = NULL) +   scale_x_continuous(breaks = NULL) +   labs(x = '', y = '')
ggplot(data = d) +   aes(x = 1, y = 1, size = count, label = word, col = count)
ggplot(data = d) +   aes(x = 1, y = 1, size = count, label = word, col = count) + theme_classic()
ggplot(data = d) +   aes(x = 1, y = 1, size = count, label = word, col = count) +   geom_text_repel(segment.size = 0, force = 100) +   scale_size(range = c(2, 15), guide = FALSE)
ggplot(data = d) +   aes(x = 1, y = 1, size = count, label = word, col = count) +   geom_text_repel(segment.size = 0, force = 100, segment.color = 'transparent') +   scale_size(range = c(2, 15), guide = FALSE) +   scale_y_continuous(breaks = NULL) +   scale_x_continuous(breaks = NULL) +   labs(x = '', y = '') +   theme_classic()
ggplot(data = d) +   aes(x = 1, y = 1, size = count, label = word, col = count) +   geom_text_repel(segment.size = 0, force = 100, segment.color = NA) +   scale_size(range = c(2, 15), guide = FALSE) +   scale_y_continuous(breaks = NULL) +   scale_x_continuous(breaks = NULL) +   labs(x = '', y = '') +   theme_classic()
ggplot(data = d) +   aes(x = 1, y = 1, size = count, label = word, col = count) +   geom_text_repel(segment.size = 0, force = 100, segment.color = NA) +   scale_size(range = c(0, 100), guide = FALSE) +   scale_y_continuous(breaks = NULL) +   scale_x_continuous(breaks = NULL) +   labs(x = '', y = '') +   theme_classic()
ggplot(data = d) +   aes(x = 1, y = 1, size = count, label = word, col = count) +   geom_text_repel(segment.size = 0, force = 100, segment.color = NA) +   scale_size(range = c(0, 10), guide = FALSE) +   scale_y_continuous(breaks = NULL) +   scale_x_continuous(breaks = NULL) +   labs(x = '', y = '') +   theme_classic()
ggplot(data = d) +   aes(x = 1, y = 1, size = count, label = word, col = count) +   geom_text_repel(segment.size = 0, force = 100, segment.color = NA) +   scale_size(range = c(0, 50), guide = FALSE) +   scale_y_continuous(breaks = NULL) +   scale_x_continuous(breaks = NULL) +   labs(x = '', y = '') +   theme_classic()
ggplot(data = d) +   aes(x = 1, y = 1, size = count, label = word, col = count) +   geom_text_repel(segment.size = 0, force = 100, segment.color = NA) +   scale_size(range = c(0, 10), guide = FALSE) +   scale_y_continuous(breaks = NULL) +   scale_x_continuous(breaks = NULL) +   labs(x = '', y = '') +   theme_classic()
ggplot(data = d) +   aes(x = 1, y = 1, size = count, label = word, col = count) +   geom_text_repel(segment.size = 0, force = 100, segment.color = NA) +   scale_size(range = c(0, 3), guide = FALSE) +   scale_y_continuous(breaks = NULL) +   scale_x_continuous(breaks = NULL) +   labs(x = '', y = '') +   theme_classic()
ggplot(data = d) +   aes(x = 1, y = 1, size = count, label = word, col = count) +   geom_text_repel(segment.size = 0, force = 100, segment.color = NA) +   scale_size(range = c(1, 10), guide = FALSE) +   scale_y_continuous(breaks = NULL) +   scale_x_continuous(breaks = NULL) +   labs(x = '', y = '') +   theme_classic()
ggplot(data = d) +   aes(x = 1, y = 1, size = count, label = word, col = count) +   geom_text_repel(segment.size = 0, force = 100, segment.color = NA) +   scale_size(range = c(1, 15), guide = FALSE) +   scale_y_continuous(breaks = NULL) +   scale_x_continuous(breaks = NULL) +   labs(x = '', y = '') +   theme_classic()
wordcloud\
wordcloud
install.packages("wordcloud")
head(d)
wordcloud(d$word, d$count, scale = c(5, 0.5), max.words = 100, random.order = FALSE, rot.per = 0.35, use.r.layout = FALSE, colors = brewer.pal(8, 'Dark2'));
library(wordcloud)
wordcloud(d$word, d$count, scale = c(5, 0.5), max.words = 100, random.order = FALSE, rot.per = 0.35, use.r.layout = FALSE, colors = brewer.pal(8, 'Dark2'));
wordcloud(d$word, d$count, scale = c(2, 0.5), max.words = 200, random.order = FALSE, rot.per = 0.6, use.r.layout = FALSE, colors = brewer.pal(8, 'Dark2'))
wordcloud(d$word, d$count, scale = c(3, 0.5), max.words = 200, random.order = FALSE, rot.per = 0.6, use.r.layout = FALSE, colors = brewer.pal(8, 'Dark2'));
wordcloud(d$word, d$count, scale = c(5, 0.5), max.words = 200, random.order = FALSE, rot.per = 0.6, use.r.layout = FALSE, colors = brewer.pal(8, 'Dark2'));
head(d)
wordcloud(d$word, d$count, scale = c(5, 0.5), max.words = 200, random.order = FALSE, rot.per = 0.6, use.r.layout = FALSE, colors = brewer.pal(8, 'Dark2'));
head(d)
head(d, 15)
library(wordcloud) wordcloud(d$word, d$count, scale = c(5, 0.5), max.words = 200, random.order = FALSE, rot.per = 0.6, use.r.layout = FALSE, colors = brewer.pal(8, 'Dark2'));
library(tm) websitesFile <- 'websitesdata.txt'; websiteDataFrame <- read.table(websitesFile, sep = '\t', header = F, fill = TRUE, stringsAsFactors = FALSE); #deleting of not usefull fields.  websiteDataFrame <- head(websiteDataFrame[c(2:4, 6)]) names(websiteDataFrame) <- c('Host', 'Webpage', 'Title', 'Text') #TODO - create a loop textExample <- websiteDataFrame[1, 'Text'] #Create a document for each website websiteDocs <- Corpus(VectorSource(textExample)) #Remove qqpbreakqq? skipWords <- function(x) removeWords(x, c(stopwords("english"), "qqpbreakqq")) library("SnowballC") tmFuncs <- list(tolower, removePunctuation, removeNumbers, stripWhitespace, skipWords, stemDocument) websiteDocs <- tm_map(websiteDocs, FUN = tm_reduce, tmFuns = tmFuncs)
websiteDocs[[1]][1]
m <- as.matrix(dtmWebsiteDocs) v <- sort(rowSums(m), decreasing = TRUE) d <- data.frame(word = names(v), count = v) head(d)
library(tm) websitesFile <- 'websitesdata.txt'; websiteDataFrame <- read.table(websitesFile, sep = '\t', header = F, fill = TRUE, stringsAsFactors = FALSE); #deleting of not usefull fields.  websiteDataFrame <- head(websiteDataFrame[c(2:4, 6)]) names(websiteDataFrame) <- c('Host', 'Webpage', 'Title', 'Text') #TODO - create a loop textExample <- websiteDataFrame[1, 'Text'] #Create a document for each website websiteDocs <- Corpus(VectorSource(textExample)) #Remove qqpbreakqq? skipWords <- function(x) removeWords(x, c(stopwords("english"), "qqpbreakqq")) library("SnowballC") tmFuncs <- list(tolower, removePunctuation, removeNumbers, stripWhitespace, skipWords, stemDocument) websiteDocs <- tm_map(websiteDocs, FUN = tm_reduce, tmFuns = tmFuncs) #Read for testing #websiteDocs[[1]][1] #2. Term documents and word of cloud dtmWebsiteDocs <- TermDocumentMatrix(websiteDocs) m <- as.matrix(dtmWebsiteDocs) v <- sort(rowSums(m), decreasing = TRUE) d <- data.frame(word = names(v), count = v) head(d)
library(tm) websitesFile <- 'websitesdata.txt'; websiteDataFrame <- read.table(websitesFile, sep = '\t', header = F, fill = TRUE, stringsAsFactors = FALSE); #deleting of not usefull fields.  websiteDataFrame <- head(websiteDataFrame[c(2:4, 6)]) names(websiteDataFrame) <- c('Host', 'Webpage', 'Title', 'Text') #TODO - create a loop textExample <- websiteDataFrame[1, 'Text'] #Create a document for each website websiteDocs <- Corpus(VectorSource(textExample)) #Remove qqpbreakqq? skipWords <- function(x) removeWords(x, c(stopwords("english"), "qqpbreakqq")) library("SnowballC") tmFuncs <- list(tolower, removePunctuation, removeNumbers, stripWhitespace, skipWords, stemDocument) websiteDocs <- tm_map(websiteDocs, FUN = tm_reduce, tmFuns = tmFuncs)
dtmWebsiteDocs <- TermDocumentMatrix(websiteDocs) m <- as.matrix(dtmWebsiteDocs) v <- sort(rowSums(m), decreasing = TRUE) d <- data.frame(word = names(v), count = v) head(d)
dtmWebsiteDocs <- TermDocumentMatrix(websiteDocs)
str(websiteDocs)
class(websiteDocs)
websiteDocs$VCorpus
websiteDocs$Corpus
websiteDocs
websiteDocs$VCorpus
websiteDocs[[1]]
dtmWebsiteDocs <- TermDocumentMatrix(websiteDocs)
#TODO - create a loop textExample <- websiteDataFrame[1, 'Text'] #Create a document for each website websiteDocs <- Corpus(VectorSource(textExample)) #Remove qqpbreakqq? skipWords <- function(x) removeWords(x, c(stopwords("english"), "qqpbreakqq")) library("SnowballC") tmFuncs <- list(tolower, removePunctuation, removeNumbers, stripWhitespace, skipWords, stemDocument, PlainTextDocument) websiteDocs <- tm_map(websiteDocs, FUN = tm_reduce, tmFuns = tmFuncs)
dtmWebsiteDocs <- TermDocumentMatrix(websiteDocs)
library(tm) websitesFile <- 'websitesdata.txt'; websiteDataFrame <- read.table(websitesFile, sep = '\t', header = F, fill = TRUE, stringsAsFactors = FALSE); #deleting of not usefull fields.  websiteDataFrame <- head(websiteDataFrame[c(2:4, 6)]) names(websiteDataFrame) <- c('Host', 'Webpage', 'Title', 'Text') #TODO - create a loop textExample <- websiteDataFrame[1, 'Text']
#Remove qqpbreakqq? skipWords <- function(x) removeWords(x, c(stopwords("english"), "qqpbreakqq")) library("SnowballC") tmFuncs <- list(tolower, removePunctuation, removeNumbers, stripWhitespace, skipWords, stemDocument) websiteDocs <- tm_map(websiteDocs, FUN = tm_reduce, tmFuns = tmFuncs)
websiteDocs <- Corpus(VectorSource(textExample))
skipWords <- function(x) removeWords(x, c(stopwords("english"), "qqpbreakqq"))
library("SnowballC") tmFuncs <- list(tolower, removePunctuation, removeNumbers, stripWhitespace, skipWords, stemDocument)
websiteDocs <- tm_map(websiteDocs, FUN = tm_reduce, tmFuns = tmFuncs)
websiteDocs[[1]][1]
dtmWebsiteDocs <- TermDocumentMatrix(websiteDocs)
websiteDocs <- tm_map(websiteDocs, PlainTextDocument)
dtmWebsiteDocs <- TermDocumentMatrix(websiteDocs)
head(websiteDataFrame)
library(dplyr)
head(websiteDocs)
head(websiteDocs)
dtmWebsiteDocs
websiteDataFrame %>% distinct(Host, .keep_all = TRUE)
library(dplyr)
package.load('dplyr')
install.packages('dplyr')
websiteDataFrame %>% distinct(Host, .keep_all = TRUE)
websiteDataFrame%>%distinct(Host, .keep_all = TRUE)
websiteDataFrame%>%distinct(Host, .keep_all = TRUE)
library(dplyr)
websiteDataFrame %>% distinct(Host, .keep_all = TRUE)
websiteDataFrame %>% distinct(Host, .keep_all = TRUE)
websiteDataFrame
head(websiteDataFrame,15)
websiteDataFrame[1:40,]
websiteDataFrame[,1:40]
websiteDocs <- Corpus(VectorSource(textExample))
websiteDataFrame[,1:40]
websiteDataFrame[,1:40]
websiteDataFrame[,1:40]
websiteDataFrame[1:40,]
dim(websiteDocs)
dim(websiteDataFrame)
websiteDataFrame <- read.table(websitesFile, sep = '\t', header = F, fill = TRUE, stringsAsFactors = FALSE);
dim(websiteDataFrame)
#deleting of not usefull fields.  websiteDataFrame <- head(websiteDataFrame[c(2:4, 6)]) names(websiteDataFrame) <- c('Host', 'Webpage', 'Title', 'Text') library(dplyr)
dim(websiteDataFrame)
websitesFile <- 'websitesdata.txt'; websiteDataFrame <- read.table(websitesFile, sep = '\t', header = F, fill = TRUE, stringsAsFactors = FALSE);
head(websiteDataFrame)
head(websiteDataFrame[c(2:5, 6),])
head(websiteDataFrame[,c(2:5, 6)])
head(websiteDataFrame[,c(2:4, 6)])
head(websiteDataFrame[, c(2:4, 6)])
websiteDataFrame <- head(websiteDataFrame[, c(2:4, 6)])
names(websiteDataFrame)
dim(websiteDataFrame)
c(2:4, 6)
websitesFile <- 'websitesdata.txt'; websiteDataFrame <- read.table(websitesFile, sep = '\t', header = F, fill = TRUE, stringsAsFactors = FALSE);
websiteDataFrame[, c(2:4, 6)]
q s
head(websiteDataFrame[, c(2:4, 6)],15)
clean
clear
cle
library(tm) websitesFile <- 'websitesdata.txt'; websiteDataFrame <- read.table(websitesFile, sep = '\t', header = F, fill = TRUE, stringsAsFactors = FALSE); #deleting of not usefull fields.  websiteDataFrame <- websiteDataFrame[, c(2:4, 6)])
library(tm) websitesFile <- 'websitesdata.txt'; websiteDataFrame <- read.table(websitesFile, sep = '\t', header = F, fill = TRUE, stringsAsFactors = FALSE); #deleting of not usefull fields.  websiteDataFrame <- websiteDataFrame[, c(2:4, 6)]
names(websiteDataFrame) <- c('Host', 'Webpage', 'Title', 'Text')
library(dplyr)
websiteDataFrame %>% distinct(Host, .keep_all = TRUE)
websitesFile <- 'websitesdata.txt'; websiteDataFrame <- read.table(websitesFile, sep = '\t', header = F, fill = TRUE, stringsAsFactors = FALSE); #deleting of not usefull fields.  websiteDataFrame <- websiteDataFrame[, c(2:4, 6)] names(websiteDataFrame) <- c('Host', 'Webpage', 'Title', 'Text')
library(dplyr) #TODO - create a loop textExample <- websiteDataFrame[1, 'Text'] websiteDataFrame %>% distinct(Host, .keep_all = TRUE)
websiteDataFrame %>% distinct(Host, .keep_all = TRUE)
dim(websiteDataFrame)
dim(websiteDataFrame %>% distinct(Host, .keep_all = TRUE))
#TODO - create a loop textExample <- websiteDataFrame[1, 'Text'] websiteDataFrame <- websiteDataFrame %>% distinct(Host, .keep_all = TRUE)
head(websiteDataFrame)
websiteDataFrame
websiteDataFrame[, 'Text']
textExample <- websiteDataFrame[, 'Text']
websiteDocs <- Corpus(VectorSource(textExample))
library(tm)
websiteDocs <- Corpus(VectorSource(textExample))
websiteDocs[[1]][1]
websiteDocs[[1]][2]
websiteDocs[[2]][1]
websiteDocs[[2]][1]
websiteDocs[[2]][2]
websiteDocs[[2]][1]
websiteDocs[[2]][2]
?corpus_frame
?VectorSource
?Corpus
websiteDocs
str(websiteDocs)
str(websiteDocs)
websiteDocs[[1]]
websiteDocs$heading
websiteDocs$[heading]
websiteDocs$['heading']
websiteDocs[['heading']]
websiteDocs['heading']
websiteDocs
websiteDocs
websiteDocs$Metadata
websiteDocs[[Metadata]]
websiteDocs[['Metadata']]
websiteDocs
websiteDocs$ 1
websiteDocs$`1`
str(websiteDocs)
skipWords <- function(x) removeWords(x, c(stopwords("english"), "qqpbreakqq")) library("SnowballC") tmFuncs <- list(tolower, removePunctuation, removeNumbers, stripWhitespace, skipWords, stemDocument) websiteDocs <- tm_map(websiteDocs, FUN = tm_reduce, tmFuns = tmFuncs) websiteDocs <- tm_map(websiteDocs, PlainTextDocument)
websiteDocs[[1]][1]
dtmWebsiteDocs <- TermDocumentMatrix(websiteDocs)
dtmWebsiteDocs
m <- as.matrix(dtmWebsiteDocs) v <- sort(rowSums(m), decreasing = TRUE) d <- data.frame(word = names(v), count = v)
head(d)
str(d)
m
head(m)
str(dtmWebsiteDocs)
head(dtmWebsiteDocs)
dtmWebsiteDocs$dimnames
dtmWebsiteDocs[['Docs']]
websiteDocs
dtmWebsiteDocs
websiteDocs
dtmWebsiteDocs$ dimnames
head(d)
websitesFile <- 'websitesdata.txt'; websiteDataFrame <- read.table(websitesFile, sep = '\t', header = F, fill = TRUE, stringsAsFactors = FALSE); #deleting of not usefull fields.  websiteDataFrame <- websiteDataFrame[, c(2:4, 6)] names(websiteDataFrame) <- c('Host', 'Webpage', 'Title', 'Text') library(dplyr) #TODO - create a loop websiteDataFrame <- websiteDataFrame %>% distinct(Host, .keep_all = TRUE)
head(websiteDataFrame)
websiteDocs
websiteDocs
websitesFile <- 'websitesdata.txt'; websiteDataFrame <- read.table(websitesFile, sep = '\t', header = F, fill = TRUE, stringsAsFactors = FALSE); #deleting of not usefull fields.  websiteDataFrame <- websiteDataFrame[, c(2:4, 6)] names(websiteDataFrame) <- c('Host', 'Webpage', 'Title', 'Text') library(dplyr) #TODO - create a loop websiteDataFrame <- websiteDataFrame %>% distinct(Host, .keep_all = TRUE)
websiteDataFrame
textExample
textExample <- websiteDataFrame[, 'Text']
textExample
lapply(textExample, performTmFunctions)
performTmFunctions <- function(websiteText) {     #Create a document for each website     websiteDoc <- Corpus(VectorSource(websiteText))     #Remove qqpbreakqq?     skipWords <- function(x) removeWords(x, c(stopwords("english"), "qqpbreakqq"))     tmFuncs <- list(tolower, removePunctuation, removeNumbers, stripWhitespace, skipWords, stemDocument)     websiteDoc <- tm_map(websiteDocs, FUN = tm_reduce, tmFuns = tmFuncs)     websiteDoc <- tm_map(websiteDocs, PlainTextDocument)     #Read for testing     #websiteDoc[[1]][1] }
lapply(textExample, performTmFunctions)
library(tm)
performTmFunctions <- function(websiteText) {     #Create a document for each website     websiteDoc <- Corpus(VectorSource(websiteText))     #Remove qqpbreakqq?     skipWords <- function(x) removeWords(x, c(stopwords("english"), "qqpbreakqq"))     tmFuncs <- list(tolower, removePunctuation, removeNumbers, stripWhitespace, skipWords, stemDocument)     websiteDoc <- tm_map(websiteDocs, FUN = tm_reduce, tmFuns = tmFuncs)     websiteDoc <- tm_map(websiteDocs, PlainTextDocument)     #Read for testing     #websiteDoc[[1]][1] }
lapply(textExample, performTmFunctions)
performTmFunctions <- function(websiteText) {     #Create a document for each website     websiteDoc <- Corpus(VectorSource(websiteText))     #Remove qqpbreakqq?     skipWords <- function(x) removeWords(x, c(stopwords("english"), "qqpbreakqq"))     tmFuncs <- list(tolower, removePunctuation, removeNumbers, stripWhitespace, skipWords, stemDocument)     websiteDoc <- tm_map(websiteDoc, FUN = tm_reduce, tmFuns = tmFuncs)     websiteDoc <- tm_map(websiteDoc, PlainTextDocument)     #Read for testing     #websiteDoc[[1]][1] }
lapply(textExample, performTmFunctions)
performTmFunctions <- function(websiteText) {     #Create a document for each website     websiteDoc <- Corpus(VectorSource(websiteText))     #Remove qqpbreakqq?     skipWords <- function(x) removeWords(x, c(stopwords("english"), "qqpbreakqq"))     tmFuncs <- list(tolower, removePunctuation, removeNumbers, stripWhitespace, skipWords, stemDocument)     websiteDoc <- tm_map(websiteDoc, FUN = tm_reduce, tmFuns = tmFuncs)     websiteDoc <- tm_map(websiteDoc, PlainTextDocument)     #Read for testing     websiteDoc[[1]][1] }
lapply(textExample, performTmFunctions)
#Create a document for each website websiteDoc <- Corpus(VectorSource(websiteText)) #Remove qqpbreakqq? skipWords <- function(x) removeWords(x, c(stopwords("english"), "qqpbreakqq")) tmFuncs <- list(tolower, removePunctuation, removeNumbers, stripWhitespace, skipWords, stemDocument) websiteDoc <- tm_map(websiteDoc, FUN = tm_reduce, tmFuns = tmFuncs) websiteDoc <- tm_map(websiteDoc, PlainTextDocument) #Read for testing # websiteDoc[[1]][1] websiteDoc }
performTmFunctions <- function(websiteText) {     #Create a document for each website     websiteDoc <- Corpus(VectorSource(websiteText))     #Remove qqpbreakqq?     skipWords <- function(x) removeWords(x, c(stopwords("english"), "qqpbreakqq"))     tmFuncs <- list(tolower, removePunctuation, removeNumbers, stripWhitespace, skipWords, stemDocument)     websiteDoc <- tm_map(websiteDoc, FUN = tm_reduce, tmFuns = tmFuncs)     websiteDoc <- tm_map(websiteDoc, PlainTextDocument)     #Read for testing     # websiteDoc[[1]][1]     websiteDoc }
lapply(textExample, performTmFunctions)
websiteDocs <- lapply(textExample, performTmFunctions)
websiteDocs
#Create a document for each website websiteDoc <- Corpus(VectorSource(websiteText)) #Remove qqpbreakqq? skipWords <- function(x) removeWords(x, c(stopwords("english"), "qqpbreakqq")) tmFuncs <- list(tolower, removePunctuation, removeNumbers, stripWhitespace, skipWords, stemDocument) websiteDoc <- tm_map(websiteDoc, FUN = tm_reduce, tmFuns = tmFuncs) websiteDoc <- tm_map(websiteDoc, PlainTextDocument) #Read for testing # websiteDoc[[1]][1] #2. Term documents and word of cloud dtmWebsiteDocs <- TermDocumentMatrix(websiteDocs) m <- as.matrix(dtmWebsiteDocs) v <- sort(rowSums(m), decreasing = TRUE) d <- data.frame(word = names(v), count = v) head(d) }
performTmFunctions <- function(websiteText) {     #Create a document for each website     websiteDoc <- Corpus(VectorSource(websiteText))     #Remove qqpbreakqq?     skipWords <- function(x) removeWords(x, c(stopwords("english"), "qqpbreakqq"))     tmFuncs <- list(tolower, removePunctuation, removeNumbers, stripWhitespace, skipWords, stemDocument)     websiteDoc <- tm_map(websiteDoc, FUN = tm_reduce, tmFuns = tmFuncs)     websiteDoc <- tm_map(websiteDoc, PlainTextDocument)     #Read for testing     # websiteDoc[[1]][1]     #2. Term documents and word of cloud     dtmWebsiteDocs <- TermDocumentMatrix(websiteDocs)     m <- as.matrix(dtmWebsiteDocs)     v <- sort(rowSums(m), decreasing = TRUE)     d <- data.frame(word = names(v), count = v)     head(d) }
websiteDocs <- lapply(textExample, performTmFunctions)
performTmFunctions <- function(websiteText) {     #Create a document for each website     websiteDoc <- Corpus(VectorSource(websiteText))     #Remove qqpbreakqq?     skipWords <- function(x) removeWords(x, c(stopwords("english"), "qqpbreakqq"))     tmFuncs <- list(tolower, removePunctuation, removeNumbers, stripWhitespace, skipWords, stemDocument)     websiteDoc <- tm_map(websiteDoc, FUN = tm_reduce, tmFuns = tmFuncs)     websiteDoc <- tm_map(websiteDoc, PlainTextDocument)     #Read for testing     # websiteDoc[[1]][1]     #2. Term documents and word of cloud     dtmWebsiteDoc <- TermDocumentMatrix(websiteDoc)     m <- as.matrix(dtmWebsiteDoc)     v <- sort(rowSums(m), decreasing = TRUE)     d <- data.frame(word = names(v), count = v)     head(d) }
websiteDocs <- lapply(textExample, performTmFunctions)
websiteDocs
performTmFunctions <- function(websiteText) {     #Create a document for each website     websiteDoc <- Corpus(VectorSource(websiteText))     #Remove qqpbreakqq?     skipWords <- function(x) removeWords(x, c(stopwords("english"), "qqpbreakqq"))     tmFuncs <- list(tolower, removePunctuation, removeNumbers, stripWhitespace, skipWords, stemDocument)     websiteDoc <- tm_map(websiteDoc, FUN = tm_reduce, tmFuns = tmFuncs)     websiteDoc <- tm_map(websiteDoc, PlainTextDocument)     #Read for testing     # websiteDoc[[1]][1]     #2. Term documents and word of cloud     dtmWebsiteDoc <- TermDocumentMatrix(websiteDoc)     m <- as.matrix(dtmWebsiteDoc)     v <- sort(rowSums(m), decreasing = TRUE)     d <- data.frame(word = names(v), count = v)     d }
websiteDocs <- lapply(textExample, performTmFunctions)
websiteDocs
performTmFunctions <- function(websiteText) {     #Create a document for each website     websiteDoc <- Corpus(VectorSource(websiteText))     #Remove qqpbreakqq?     skipWords <- function(x) removeWords(x, c(stopwords("english"), "qqpbreakqq"))     tmFuncs <- list(tolower, removePunctuation, removeNumbers, stripWhitespace, skipWords, stemDocument)     websiteDoc <- tm_map(websiteDoc, FUN = tm_reduce, tmFuns = tmFuncs)     websiteDoc <- tm_map(websiteDoc, PlainTextDocument)     #Read for testing     # websiteDoc[[1]][1]     #2. Term documents and word of cloud     dtmWebsiteDoc <- TermDocumentMatrix(websiteDoc)     m <- as.matrix(dtmWebsiteDoc)     v <- sort(rowSums(m), decreasing = TRUE)     d <- data.frame(word = names(v), count = v)     head(d, 50) }
lapply(textExample, performTmFunctions)
performTmFunctions <- function(websiteText) {     #Create a document for each website     websiteDoc <- Corpus(VectorSource(websiteText))     #Remove qqpbreakqq?     skipWords <- function(x) removeWords(x, c(stopwords("english"), "qqpbreakqq"))     tmFuncs <- list(tolower, removePunctuation, removeNumbers, stripWhitespace, skipWords, stemDocument)     websiteDoc <- tm_map(websiteDoc, FUN = tm_reduce, tmFuns = tmFuncs)     websiteDoc <- tm_map(websiteDoc, PlainTextDocument)     #Read for testing     # websiteDoc[[1]][1]     #2. Term documents and word of cloud     dtmWebsiteDoc <- TermDocumentMatrix(websiteDoc)     dtmWebsiteDoc     m <- as.matrix(dtmWebsiteDoc)     v <- sort(rowSums(m), decreasing = TRUE)     d <- data.frame(word = names(v), count = v)     head(d, 50) }
lapply(textExample, performTmFunctions)
performTmFunctions <- function(websiteText) {     #Create a document for each website     websiteDoc <- Corpus(VectorSource(websiteText))     #Remove qqpbreakqq?     skipWords <- function(x) removeWords(x, c(stopwords("english"), "qqpbreakqq"))     tmFuncs <- list(tolower, removePunctuation, removeNumbers, stripWhitespace, skipWords, stemDocument)     websiteDoc <- tm_map(websiteDoc, FUN = tm_reduce, tmFuns = tmFuncs)     websiteDoc <- tm_map(websiteDoc, PlainTextDocument)     #Read for testing     # websiteDoc[[1]][1]     #2. Term documents and word of cloud     dtmWebsiteDoc <- TermDocumentMatrix(websiteDoc)     dtmWebsiteDoc     m <- as.matrix(dtmWebsiteDoc)     v <- sort(rowSums(m), decreasing = TRUE)     d <- data.frame(word = names(v), count = v)     #  head(d,50) }
lapply(textExample, performTmFunctions)
performTmFunctions <- function(websiteText) {     #Create a document for each website     websiteDoc <- Corpus(VectorSource(websiteText))     #Remove qqpbreakqq?     skipWords <- function(x) removeWords(x, c(stopwords("english"), "qqpbreakqq"))     tmFuncs <- list(tolower, removePunctuation, removeNumbers, stripWhitespace, skipWords, stemDocument)     websiteDoc <- tm_map(websiteDoc, FUN = tm_reduce, tmFuns = tmFuncs)     websiteDoc <- tm_map(websiteDoc, PlainTextDocument)     #Read for testing     # websiteDoc[[1]][1]     #2. Term documents and word of cloud     dtmWebsiteDoc <- TermDocumentMatrix(websiteDoc)     m <- as.matrix(dtmWebsiteDoc)     v <- sort(rowSums(m), decreasing = TRUE)     d <- data.frame(word = names(v), count = v)     #  head(d,50) }
lapply(textExample, performTmFunctions)
performTmFunctions <- function(websiteText) {     #Create a document for each website     websiteDoc <- Corpus(VectorSource(websiteText))     #Remove qqpbreakqq?     skipWords <- function(x) removeWords(x, c(stopwords("english"), "qqpbreakqq"))     tmFuncs <- list(tolower, removePunctuation, removeNumbers, stripWhitespace, skipWords, stemDocument)     websiteDoc <- tm_map(websiteDoc, FUN = tm_reduce, tmFuns = tmFuncs)     websiteDoc <- tm_map(websiteDoc, PlainTextDocument)     #Read for testing     # websiteDoc[[1]][1]     #2. Term documents and word of cloud     dtmWebsiteDoc <- TermDocumentMatrix(websiteDoc)     m <- as.matrix(dtmWebsiteDoc)     v <- sort(rowSums(m), decreasing = TRUE)     d <- data.frame(word = names(v), count = v)     head(d, 50) }
lapply(textExample, performTmFunctions)
wordFrequenciesWebsites <- lapply(textExample, performTmFunctions)
wordFrequenciesWebsites
wordFrequenciesWebsites[[1]]
wordFrequenciesWebsites[[2]]
d <- wordFrequenciesWebsites[[2]]
library(wordcloud) wordcloud(d$word, d$count, scale = c(5, 0.5), max.words = 200, random.order = FALSE, rot.per = 0.6, use.r.layout = FALSE, colors = brewer.pal(8, 'Dark2'));
?svm
??svm
sapply(textExample, performTmFunctions)
str(sapply(textExample, performTmFunctions))
lapply(textExample, performTmFunctions)
performTmFunctions <- function(websiteText) {     #Create a document for each website     websiteDoc <- Corpus(VectorSource(websiteText))     #Remove qqpbreakqq?     skipWords <- function(x) removeWords(x, c(stopwords("english"), "qqpbreakqq"))     tmFuncs <- list(tolower, removePunctuation, removeNumbers, stripWhitespace, skipWords, stemDocument)     websiteDoc <- tm_map(websiteDoc, FUN = tm_reduce, tmFuns = tmFuncs)     websiteDoc <- tm_map(websiteDoc, PlainTextDocument)     #Read for testing     # websiteDoc[[1]][1]     #2. Term documents and word of cloud     #dtmWebsiteDoc <- TermDocumentMatrix(websiteDoc)     #m <- as.matrix(dtmWebsiteDoc)     #v <- sort(rowSums(m), decreasing = TRUE)     #d <- data.frame(word = names(v), count = v)     #head(d,50) }
sapply(textExample, performTmFunctions)
lapply(textExample, performTmFunctions)
lapply(textExample, performTmFunctions)
websitesFile <- 'websitesdata.txt'; websiteDataFrame <- read.table(websitesFile, sep = '\t', header = F, fill = TRUE, stringsAsFactors = FALSE);
websiteDataFrame <- websiteDataFrame[, c(2:4, 6)] names(websiteDataFrame) <- c('Host', 'Webpage', 'Title', 'Text')
websiteDataFrame <- websiteDataFrame %>% distinct(Host, .keep_all = TRUE)
performTmFunctions(websitesText)
websitesText <- websiteDataFrame[, 'Text']
performTmFunctions(websitesText)
performTmFunctions <- function(websiteText) {     #Create a document for each website     websiteDoc <- Corpus(VectorSource(websiteText))     #Remove qqpbreakqq?     skipWords <- function(x) removeWords(x, c(stopwords("english"), "qqpbreakqq"))     tmFuncs <- list(tolower, removePunctuation, removeNumbers, stripWhitespace, skipWords, stemDocument)     websiteDoc <- tm_map(websiteDoc, FUN = tm_reduce, tmFuns = tmFuncs)     websiteDoc <- tm_map(websiteDoc, PlainTextDocument)     #Read for testing     # websiteDoc[[1]][1]     #2. Term documents and word of cloud     dtmWebsiteDoc <- TermDocumentMatrix(websiteDoc)     dtmWebsiteDoc     #m <- as.matrix(dtmWebsiteDoc)     #v <- sort(rowSums(m), decreasing = TRUE)     #d <- data.frame(word = names(v), count = v)     #head(d,50) }
performTmFunctions(websitesText)
library(e1071)
install.packages('e1071')
library(e1071)
?SVM
??SVM
?svm
as.matrix(performTmFunctions(websitesText))
head(as.matrix(performTmFunctions))
head(as.matrix(performTmFunctions(websitesText)))
head(websiteDataFrame)
performTmFunctions <- function(websiteText) {     #Create a document for each website     #websiteDoc <- Corpus(VectorSource(websiteText))     websiteDoc <- Corpus(DataframeSource(websiteText))     #Remove qqpbreakqq?     skipWords <- function(x) removeWords(x, c(stopwords("english"), "qqpbreakqq"))     tmFuncs <- list(tolower, removePunctuation, removeNumbers, stripWhitespace, skipWords, stemDocument)     websiteDoc <- tm_map(websiteDoc, FUN = tm_reduce, tmFuns = tmFuncs)     websiteDoc <- tm_map(websiteDoc, PlainTextDocument)     #Read for testing     # websiteDoc[[1]][1]     #2. Term documents and word of cloud     dtmWebsiteDoc <- TermDocumentMatrix(websiteDoc)     dtmWebsiteDoc     #m <- as.matrix(dtmWebsiteDoc)     #v <- sort(rowSums(m), decreasing = TRUE)     #d <- data.frame(word = names(v), count = v)     #head(d,50) }
performTmFunctions(websiteDataFrame)
TDWebsitesData <- performTmFunctions(websiteDataFrame)
TDWebsitesData
TDWebsitesData[[1]]
str(TDWebsitesData)
performTmFunctions <- function(websiteText) {     #Create a document for each website     #websiteDoc <- Corpus(VectorSource(websiteText))     websiteDoc <- Corpus(DataframeSource(websiteText))     websiteDoc     #Remove qqpbreakqq?     skipWords <- function(x) removeWords(x, c(stopwords("english"), "qqpbreakqq"))     tmFuncs <- list(tolower, removePunctuation, removeNumbers, stripWhitespace, skipWords, stemDocument)     websiteDoc <- tm_map(websiteDoc, FUN = tm_reduce, tmFuns = tmFuncs)     websiteDoc <- tm_map(websiteDoc, PlainTextDocument)     #Read for testing     # websiteDoc[[1]][1]     #2. Term documents and word of cloud     dtmWebsiteDoc <- TermDocumentMatrix(websiteDoc)     #m <- as.matrix(dtmWebsiteDoc)     #v <- sort(rowSums(m), decreasing = TRUE)     #d <- data.frame(word = names(v), count = v)     #head(d,50) }
performTmFunctions(websiteDataFrame)
performTmFunctions(websiteDataFrame)
performTmFunctions <- function(websiteText) {     #Create a document for each website     #websiteDoc <- Corpus(VectorSource(websiteText))     websiteDoc <- Corpus(DataframeSource(websiteText))     #Remove qqpbreakqq?     skipWords <- function(x) removeWords(x, c(stopwords("english"), "qqpbreakqq"))     tmFuncs <- list(tolower, removePunctuation, removeNumbers, stripWhitespace, skipWords, stemDocument)     websiteDoc <- tm_map(websiteDoc, FUN = tm_reduce, tmFuns = tmFuncs)     websiteDoc <- tm_map(websiteDoc, PlainTextDocument)     #Read for testing     # websiteDoc[[1]][1]     #2. Term documents and word of cloud     dtmWebsiteDoc <- TermDocumentMatrix(websiteDoc)     #m <- as.matrix(dtmWebsiteDoc)     #v <- sort(rowSums(m), decreasing = TRUE)     #d <- data.frame(word = names(v), count = v)     #head(d,50) }
performTmFunctions(websiteDataFrame)
TDWebsitesData <- performTmFunctions(websiteDataFrame)
TDWebsitesData
websiteDataFrame
library(tm) library(dplyr) library("SnowballC") library(e1071) #1.Read from file and preprocessing. 1 and 2 is inspired by https://www.springboard.com/blog/text-mining-in-r/ websitesFile <- 'websitesdata.txt'; websiteDataFrame <- read.table(websitesFile, sep = '\t', header = F, fill = TRUE, stringsAsFactors = FALSE);
#deleting of not usefull fields.  websiteDataFrame <- websiteDataFrame[, c(2:4, 6)] names(websiteDataFrame) <- c('Host', 'Webpage', 'Title', 'text')
head(websiteDataFrame)
names(websiteDataFrame) <- c('Host', 'Webpage', 'Title', 'text') #Take only first page from each website websiteDataFrame <- websiteDataFrame %>% distinct(Host, .keep_all = TRUE) %>% mutate(doc_id = Host)
head(websiteDataFrame)
TDWebsitesData <- performTmFunctions(websiteDataFrame)
performTmFunctions <- function(websiteText) {     #Create a document for each website     #websiteDoc <- Corpus(VectorSource(websiteText))     websiteDoc <- Corpus(DataframeSource(websiteText))     #Remove qqpbreakqq?     skipWords <- function(x) removeWords(x, c(stopwords("english"), "qqpbreakqq"))     tmFuncs <- list(tolower, removePunctuation, removeNumbers, stripWhitespace, skipWords, stemDocument)     websiteDoc <- tm_map(websiteDoc, FUN = tm_reduce, tmFuns = tmFuncs)     websiteDoc <- tm_map(websiteDoc, PlainTextDocument)     #Read for testing     # websiteDoc[[1]][1]     #2. Term documents and word of cloud     dtmWebsiteDoc <- TermDocumentMatrix(websiteDoc)     #m <- as.matrix(dtmWebsiteDoc)     #v <- sort(rowSums(m), decreasing = TRUE)     #d <- data.frame(word = names(v), count = v)     #head(d,50) }
websiteDoc <- Corpus(DataframeSource(websiteDataFrame))
websiteDoc
websiteDoc$`1`
head(websiteDataFrame)
websiteDataFrame <- websiteDataFrame %>% distinct(Host, .keep_all = TRUE) %>% mutate(doc_id = Host) %>% select(doc_id, text)
websiteDoc <- Corpus(DataframeSource(websiteDataFrame))
websiteDoc
websiteDoc$`1`
websiteDoc[[1]]
websiteDataFrame
TDWebsitesData <- performTmFunctions(websiteDataFrame)
TDWebsitesData
TDWebsitesData[[1]]
as.matrix(dtmWebsiteDoc)
as.matrix(TDWebsitesData)
head(as.matrix(TDWebsitesData))
head(websiteDataFrame)
inspect(websiteDataFrame)
inspect(TDWebsitesData)
head(websiteDataFrame)
names(websiteDataFrame)
websiteDataFrame["doc_id"]
str(websiteDataFrame["doc_id"])
meta(TDWebsitesData)
websiteDataFrame <- websiteDataFrame %>% distinct(Host, .keep_all = TRUE) %>% mutate(doc_id = Host) %>% select(doc_id, text) #websitesText <- websiteDataFrame[, 'Text'] TDWebsitesData <- performTmFunctions(websiteDataFrame)
TDWebsitesData <- performTmFunctions(websiteDataFrame)
meta(TDWebsitesData)
websiteDoc <- Corpus(DataframeSource(websiteDataFrame))
meta(websiteDoc)
websiteDoc
websiteDoc[[1]]
websiteDoc[[2]]
websiteDoc
inspect(websiteDoc)
inspect(websiteDoc)[[8]]
inspect(websiteDoc)[[8]]
websiteDoc[[8]]
websiteDoc[[8]]["Content"]
websiteDoc[[8]]
websiteDoc[[8]]$Content
str(websiteDataFrame)
head(websiteDataFrame)
performTmFunctions(websiteDataFrame)
Corpus(DataframeSource(websiteDataFrame))
meta(Corpus(DataframeSource(websiteDataFrame)))
meta(Corpus(DataframeSource(websiteDataFrame)))
inspect(Corpus(DataframeSource(websiteDataFrame)))
inspect(Corpus(DataframeSource(websiteDataFrame)))
websiteDoc <- Corpus(DataframeSource(websiteDataFrame))
inspect(websiteDoc)
websiteDoc$`1`
websiteDoc$`2`
websiteDoc <- Corpus(DataframeSource(websiteDataFrame))
str(websiteDoc)
websitesFile <- 'websitesdata.txt'; websiteDataFrame <- read.table(websitesFile, sep = '\t', header = F, fill = TRUE, stringsAsFactors = FALSE); #deleting of not usefull fields.  websiteDataFrame <- websiteDataFrame[, c(2:4, 6)] names(websiteDataFrame) <- c('Host', 'Webpage', 'Title', 'text') #Take only first page from each website websiteDataFrame <- websiteDataFrame %>% distinct(Host, .keep_all = TRUE) %>% mutate(doc_id = Host, heading = "dfsf") %>% select(doc_id, text)
websiteDoc <- Corpus(DataframeSource(websiteDataFrame))
str(websiteDoc)
docs <- data.frame(doc_id = c("doc_1", "doc_2"), text = c("This is a text.", "This another one."), dmeta1 = 1:2, dmeta2 = letters[1:2], stringsAsFactors = FALSE) (ds <- DataframeSource(docs)) x <- Corpus(ds) inspect(x) meta(x)
names(websiteDataFrame)
websiteDoc <- Corpus(DataframeSource(websiteDataFrame))
websiteDoc
websiteDataFrame <- websiteDataFrame %>% distinct(Host, .keep_all = TRUE) %>% mutate(doc_id = Host, heading = "dfsf") %>% select(doc_id, text, heading)
#1.Read from file and preprocessing. 1 and 2 is inspired by https://www.springboard.com/blog/text-mining-in-r/ websitesFile <- 'websitesdata.txt'; websiteDataFrame <- read.table(websitesFile, sep = '\t', header = F, fill = TRUE, stringsAsFactors = FALSE); #deleting of not usefull fields.  websiteDataFrame <- websiteDataFrame[, c(2:4, 6)] names(websiteDataFrame) <- c('Host', 'Webpage', 'Title', 'text') #Take only first page from each website websiteDataFrame <- websiteDataFrame %>% distinct(Host, .keep_all = TRUE) %>% mutate(doc_id = Host, heading = "dfsf") %>% select(doc_id, text, heading) #websitesText <- websiteDataFrame[, 'Text']
head(websiteDataFrame)
websiteDoc <- Corpus(DataframeSource(websiteDataFrame))
websiteDoc
websiteDoc$`3`
meta(websiteDoc)
inspect(websiteDoc)
inspect(websiteDoc)[[1]]
inspect(websiteDoc)[[1]][1]
inspect(websiteDoc)[[1]][1]
docs <- data.frame(doc_id = c("doc_1", "doc_2"), text = c("This is a text.", "This another one."), dmeta1 = 1:2, dmeta2 = letters[1:2], stringsAsFactors = FALSE)
head(docs)
(ds <- DataframeSource(docs))
library(dplyr)
library(tm) library(dplyr) library("SnowballC")
(ds <- DataframeSource(docs))
x <- Corpus(ds)
inspect(x)
meta(x)
meta(x[[1]])
meta(x[[1]])
docs <- data.frame(doc_id = c("doc_1", "doc_2"), text = c("This is a text.", "This another one."), id = 2:3, dmeta2 = letters[1:2], stringsAsFactors = FALSE) (ds <- DataframeSource(docs)) x <- Corpus(ds) meta(x[[1]])
TDWebsitesData <- performTmFunctions(websiteDataFrame)
websiteDataFrame %>% distinct(Host, .keep_all = TRUE) %>% mutate(doc_id = Host, heading = "dfsf") %>% select(doc_id, text, heading)
library(tm) library(dplyr) library("SnowballC") library(e1071) #1.Read from file and preprocessing. 1 and 2 is inspired by https://www.springboard.com/blog/text-mining-in-r/ websitesFile <- 'websitesdata.txt'; websiteDataFrame <- read.table(websitesFile, sep = '\t', header = F, fill = TRUE, stringsAsFactors = FALSE); #deleting of not usefull fields.  websiteDataFrame <- websiteDataFrame[, c(2:4, 6)] names(websiteDataFrame) <- c('Host', 'Webpage', 'Title', 'text') #Take only first page from each website websiteDataFrame <- websiteDataFrame %>% distinct(Host, .keep_all = TRUE) %>% mutate(doc_id = Host, heading = "dfsf") %>% select(doc_id, text, heading) #websitesText <- websiteDataFrame[, 'Text'] TDWebsitesData <- performTmFunctions(websiteDataFrame)
performTmFunctions <- function(websiteText) {     #Create a document for each website     #websiteDoc <- Corpus(VectorSource(websiteText))     websiteDoc <- Corpus(DataframeSource(websiteText))     #Remove qqpbreakqq?     skipWords <- function(x) removeWords(x, c(stopwords("english"), "qqpbreakqq"))     tmFuncs <- list(tolower, removePunctuation, removeNumbers, stripWhitespace, skipWords, stemDocument)     websiteDoc <- tm_map(websiteDoc, FUN = tm_reduce, tmFuns = tmFuncs)     websiteDoc <- tm_map(websiteDoc, PlainTextDocument)     #Read for testing     # websiteDoc[[1]][1]     #2. Term documents and word of cloud     dtmWebsiteDoc <- TermDocumentMatrix(websiteDoc)     #m <- as.matrix(dtmWebsiteDoc)     #v <- sort(rowSums(m), decreasing = TRUE)     #d <- data.frame(word = names(v), count = v)     #head(d,50) }
library(tm) library(dplyr) library("SnowballC") library(e1071) #1.Read from file and preprocessing. 1 and 2 is inspired by https://www.springboard.com/blog/text-mining-in-r/ websitesFile <- 'websitesdata.txt'; websiteDataFrame <- read.table(websitesFile, sep = '\t', header = F, fill = TRUE, stringsAsFactors = FALSE); #deleting of not usefull fields.  websiteDataFrame <- websiteDataFrame[, c(2:4, 6)] names(websiteDataFrame) <- c('Host', 'Webpage', 'Title', 'text') #Take only first page from each website websiteDataFrame <- websiteDataFrame %>% distinct(Host, .keep_all = TRUE) %>% mutate(doc_id = Host, heading = "dfsf") %>% select(doc_id, text, heading) #websitesText <- websiteDataFrame[, 'Text'] TDWebsitesData <- performTmFunctions(websiteDataFrame)
TDWebsitesData
as.matrix(TDWebsitesData)
as.matrix(head(TDWebsitesData))
m <- as.matrix(TDWebsitesData)
head(m)
performTmFunctions <- function(websiteText) {     #Create a document for each website     #websiteDoc <- Corpus(VectorSource(websiteText))     myReader <- readTabular(mapping = list(content = "Text", id = "Heading"))     websiteDoc <- Corpus(DataframeSource(websiteText), readerControl = list(reader = myReader))     #Remove qqpbreakqq?     skipWords <- function(x) removeWords(x, c(stopwords("english"), "qqpbreakqq"))     tmFuncs <- list(tolower, removePunctuation, removeNumbers, stripWhitespace, skipWords, stemDocument)     websiteDoc <- tm_map(websiteDoc, FUN = tm_reduce, tmFuns = tmFuncs)     websiteDoc <- tm_map(websiteDoc, PlainTextDocument)     #Read for testing     # websiteDoc[[1]][1]     #2. Term documents and word of cloud     dtmWebsiteDoc <- TermDocumentMatrix(websiteDoc)     #m <- as.matrix(dtmWebsiteDoc)     #v <- sort(rowSums(m), decreasing = TRUE)     #d <- data.frame(word = names(v), count = v)     #head(d,50) }
library(tm) library(dplyr) library("SnowballC") library(e1071) #1.Read from file and preprocessing. 1 and 2 is inspired by https://www.springboard.com/blog/text-mining-in-r/ websitesFile <- 'websitesdata.txt'; websiteDataFrame <- read.table(websitesFile, sep = '\t', header = F, fill = TRUE, stringsAsFactors = FALSE); #deleting of not usefull fields.  websiteDataFrame <- websiteDataFrame[, c(2:4, 6)] names(websiteDataFrame) <- c('Host', 'Webpage', 'Title', 'Text') #Take only first page from each website websiteDataFrame <- websiteDataFrame %>% distinct(Host, .keep_all = TRUE) %>% mutate(doc_id = Host) %>% select(Host, Text, Heading)
head(websiteDataFrame)
TDWebsitesData <- performTmFunctions(websiteDataFrame)
websiteDataFrame <- websiteDataFrame %>% distinct(Host, .keep_all = TRUE) %>% mutate(doc_id = Host) %>% select(Host, Text, Title)
performTmFunctions <- function(websiteText) {     #Create a document for each website     #websiteDoc <- Corpus(VectorSource(websiteText))     myReader <- readTabular(mapping = list(content = "Text", id = "Title"))     websiteDoc <- Corpus(DataframeSource(websiteText), readerControl = list(reader = myReader))     #Remove qqpbreakqq?     skipWords <- function(x) removeWords(x, c(stopwords("english"), "qqpbreakqq"))     tmFuncs <- list(tolower, removePunctuation, removeNumbers, stripWhitespace, skipWords, stemDocument)     websiteDoc <- tm_map(websiteDoc, FUN = tm_reduce, tmFuns = tmFuncs)     websiteDoc <- tm_map(websiteDoc, PlainTextDocument)     #Read for testing     # websiteDoc[[1]][1]     #2. Term documents and word of cloud     dtmWebsiteDoc <- TermDocumentMatrix(websiteDoc)     #m <- as.matrix(dtmWebsiteDoc)     #v <- sort(rowSums(m), decreasing = TRUE)     #d <- data.frame(word = names(v), count = v)     #head(d,50) }
TDWebsitesData <- performTmFunctions(websiteDataFrame)
m <- as.matrix(TDWebsitesData)
head(m)
TDWebsitesData
TDWebsitesData[[1]]
TDWebsitesData[[2]]
TDWebsitesData[[3]]
TDWebsitesData[[1]][1]
TDWebsitesData[[1]][2]
TDWebsitesData[[]][2]
TDWebsitesData[[]][1]
TDWebsitesData$dimnames
websiteDataFrame %>% distinct(Host, .keep_all = TRUE) %>% mutate(doc_id = Host) %>% select(Host, Text, Title)
websiteDataFrame <- websiteDataFrame %>% distinct(Host, .keep_all = TRUE) %>% mutate(doc_id = Host) %>% select(Host, Text, Title)
head(websiteDataFrame)
head(websiteDataFrame)
performTmFunctions <- function(websiteText) {     #Create a document for each website     #websiteDoc <- Corpus(VectorSource(websiteText))     myReader <- readTabular(mapping = list(content = "Text", id = "Title"))     websiteDoc <- VCorpus(DataframeSource(websiteText), readerControl = list(reader = myReader))     #Remove qqpbreakqq?     skipWords <- function(x) removeWords(x, c(stopwords("english"), "qqpbreakqq"))     tmFuncs <- list(tolower, removePunctuation, removeNumbers, stripWhitespace, skipWords, stemDocument)     websiteDoc <- tm_map(websiteDoc, FUN = tm_reduce, tmFuns = tmFuncs)     websiteDoc <- tm_map(websiteDoc, PlainTextDocument)     #Read for testing     # websiteDoc[[1]][1]     #2. Term documents and word of cloud     dtmWebsiteDoc <- TermDocumentMatrix(websiteDoc)     #m <- as.matrix(dtmWebsiteDoc)     #v <- sort(rowSums(m), decreasing = TRUE)     #d <- data.frame(word = names(v), count = v)     #head(d,50) }
library(tm) library(dplyr) library("SnowballC") library(e1071) #1.Read from file and preprocessing. 1 and 2 is inspired by https://www.springboard.com/blog/text-mining-in-r/ websitesFile <- 'websitesdata.txt'; websiteDataFrame <- read.table(websitesFile, sep = '\t', header = F, fill = TRUE, stringsAsFactors = FALSE); #deleting of not usefull fields.  websiteDataFrame <- websiteDataFrame[, c(2:4, 6)] names(websiteDataFrame) <- c('Host', 'Webpage', 'Title', 'Text') #Take only first page from each website websiteDataFrame <- websiteDataFrame %>% distinct(Host, .keep_all = TRUE) %>% mutate(doc_id = Host) %>% select(Host, Text, Title) #websitesText <- websiteDataFrame[, 'Text']
TDWebsitesData <- performTmFunctions(websiteDataFrame)
m <- as.matrix(TDWebsitesData)
m
head(m)
library(tm) library(dplyr) library("SnowballC") library(e1071) #1.Read from file and preprocessing. 1 and 2 is inspired by https://www.springboard.com/blog/text-mining-in-r/ websitesFile <- 'websitesdata.txt'; websiteDataFrame <- read.table(websitesFile, sep = '\t', header = F, fill = TRUE, stringsAsFactors = FALSE); #deleting of not usefull fields.  websiteDataFrame <- websiteDataFrame[, c(2:4, 6)] names(websiteDataFrame) <- c('Host', 'Webpage', 'Title', 'Text') #Take only first page from each website websiteDataFrame <- websiteDataFrame %>% distinct(Host, .keep_all = TRUE) %>% mutate(doc_id = Host) %>% select(Host, Text, Title)
websiteDataFrame
websiteDataFrame$Id = 1:8
websiteDataFrame
performTmFunctions <- function(websiteText) {     #Create a document for each website     #websiteDoc <- Corpus(VectorSource(websiteText))     myReader <- readTabular(mapping = list(content = "Text", id = "Id"))     websiteDoc <- VCorpus(DataframeSource(websiteText), readerControl = list(reader = myReader))     #Remove qqpbreakqq?     skipWords <- function(x) removeWords(x, c(stopwords("english"), "qqpbreakqq"))     tmFuncs <- list(tolower, removePunctuation, removeNumbers, stripWhitespace, skipWords, stemDocument)     websiteDoc <- tm_map(websiteDoc, FUN = tm_reduce, tmFuns = tmFuncs)     websiteDoc <- tm_map(websiteDoc, PlainTextDocument)     #Read for testing     # websiteDoc[[1]][1]     #2. Term documents and word of cloud     dtmWebsiteDoc <- TermDocumentMatrix(websiteDoc)     #m <- as.matrix(dtmWebsiteDoc)     #v <- sort(rowSums(m), decreasing = TRUE)     #d <- data.frame(word = names(v), count = v)     #head(d,50) }
TDWebsitesData <- performTmFunctions(websiteDataFrame)
m <- as.matrix(TDWebsitesData)
head(m)
performTmFunctions <- function(websiteText) {     #Create a document for each website     #websiteDoc <- Corpus(VectorSource(websiteText))     myReader <- readTabular(mapping = list(content = "Text", id = "Id"))     websiteDoc <- VCorpus(DataframeSource(websiteText), readerControl = list(reader = myReader))     #Remove qqpbreakqq?     #skipWords <- function(x) removeWords(x, c(stopwords("english"), "qqpbreakqq"))     #tmFuncs <- list(tolower, removePunctuation, removeNumbers, stripWhitespace, skipWords, stemDocument)     #websiteDoc <- tm_map(websiteDoc, FUN = tm_reduce, tmFuns = tmFuncs)     #websiteDoc <- tm_map(websiteDoc, PlainTextDocument)     #Read for testing     # websiteDoc[[1]][1]     #2. Term documents and word of cloud     dtmWebsiteDoc <- TermDocumentMatrix(websiteDoc)     #m <- as.matrix(dtmWebsiteDoc)     #v <- sort(rowSums(m), decreasing = TRUE)     #d <- data.frame(word = names(v), count = v)     #head(d,50) }
performTmFunctions <- function(websiteText) {     #Create a document for each website     #websiteDoc <- Corpus(VectorSource(websiteText))     myReader <- readTabular(mapping = list(content = "Text", id = "Id"))     websiteDoc <- VCorpus(DataframeSource(websiteText), readerControl = list(reader = myReader))     #Remove qqpbreakqq?     #skipWords <- function(x) removeWords(x, c(stopwords("english"), "qqpbreakqq"))     #tmFuncs <- list(tolower, removePunctuation, removeNumbers, stripWhitespace, skipWords, stemDocument)     #websiteDoc <- tm_map(websiteDoc, FUN = tm_reduce, tmFuns = tmFuncs)     #websiteDoc <- tm_map(websiteDoc, PlainTextDocument)     #Read for testing     # websiteDoc[[1]][1]     #2. Term documents and word of cloud     dtmWebsiteDoc <- TermDocumentMatrix(websiteDoc)     #m <- as.matrix(dtmWebsiteDoc)     #v <- sort(rowSums(m), decreasing = TRUE)     #d <- data.frame(word = names(v), count = v)     #head(d,50) }
library(tm) library(dplyr) library("SnowballC") library(e1071) #1.Read from file and preprocessing. 1 and 2 is inspired by https://www.springboard.com/blog/text-mining-in-r/ websitesFile <- 'websitesdata.txt'; websiteDataFrame <- read.table(websitesFile, sep = '\t', header = F, fill = TRUE, stringsAsFactors = FALSE); #deleting of not usefull fields.  websiteDataFrame <- websiteDataFrame[, c(2:4, 6)] names(websiteDataFrame) <- c('Host', 'Webpage', 'Title', 'Text')
websiteDataFrame <- websiteDataFrame %>% distinct(Host, .keep_all = TRUE) %>% mutate(doc_id = Host) %>% select(Host, Text, Title) websiteDataFrame$Id = 1:8
TDWebsitesData <- performTmFunctions(websiteDataFrame)
m <- as.matrix(TDWebsitesData)
руфв(ь)
head(m)
library(tm) library(dplyr) library("SnowballC") library(e1071) #1.Read from file and preprocessing. 1 and 2 is inspired by https://www.springboard.com/blog/text-mining-in-r/ websitesFile <- 'websitesdata.txt'; websiteDataFrame <- read.table(websitesFile, sep = '\t', header = F, fill = TRUE, stringsAsFactors = FALSE); #deleting of not usefull fields.  websiteDataFrame <- websiteDataFrame[, c(2:4, 6)] names(websiteDataFrame) <- c('Host', 'Webpage', 'Title', 'Text') #Take only first page from each website #websiteDataFrame <- websiteDataFrame %>% distinct(Host, .keep_all = TRUE) %>% mutate(doc_id = Host) %>% select(Host, Text, Title) websiteDataFrame <- websiteDataFrame %>% distinct(Host, .keep_all = TRUE) %>% mutate(doc_id = 1:8) %>% select(doc_id, Text, Title)
websiteDataFrame
library(tm) library(dplyr) library("SnowballC") library(e1071) #1.Read from file and preprocessing. 1 and 2 is inspired by https://www.springboard.com/blog/text-mining-in-r/ websitesFile <- 'websitesdata.txt'; websiteDataFrame <- read.table(websitesFile, sep = '\t', header = F, fill = TRUE, stringsAsFactors = FALSE); #deleting of not usefull fields.  websiteDataFrame <- websiteDataFrame[, c(2:4, 6)] names(websiteDataFrame) <- c('Host', 'Webpage', 'Title', 'text') #Take only first page from each website #websiteDataFrame <- websiteDataFrame %>% distinct(Host, .keep_all = TRUE) %>% mutate(doc_id = Host) %>% select(Host, Text, Title) websiteDataFrame <- websiteDataFrame %>% distinct(Host, .keep_all = TRUE) %>% mutate(doc_id = 1:8) %>% select(doc_id, text, Title)
websiteDataFrame
websiteDoc <- Corpus(DataframeSource(websiteDataFrame))
websiteDoc
websiteDoc[[1]]
websiteDoc[[1]]$Metadata
websiteDoc[[1]]['Metadata']
head(as.matrix(TermDocumentMatrix(websiteDoc)))
library(tm) library(dplyr) library("SnowballC") library(e1071) #1.Read from file and preprocessing. 1 and 2 is inspired by https://www.springboard.com/blog/text-mining-in-r/ websitesFile <- 'websitesdata.txt'; websiteDataFrame <- read.table(websitesFile, sep = '\t', header = F, fill = TRUE, stringsAsFactors = FALSE); #deleting of not usefull fields.  websiteDataFrame <- websiteDataFrame[, c(2:4, 6)] names(websiteDataFrame) <- c('Host', 'Webpage', 'Title', 'text')
websiteDataFrame <- websiteDataFrame %>% distinct(Host, .keep_all = TRUE) %>% mutate(doc_id = 1:8 + "asd") %>% select(doc_id, text, Title)
library(tm) library(dplyr) library("SnowballC") library(e1071) #1.Read from file and preprocessing. 1 and 2 is inspired by https://www.springboard.com/blog/text-mining-in-r/ websitesFile <- 'websitesdata.txt'; websiteDataFrame <- read.table(websitesFile, sep = '\t', header = F, fill = TRUE, stringsAsFactors = FALSE); #deleting of not usefull fields.  websiteDataFrame <- websiteDataFrame[, c(2:4, 6)] names(websiteDataFrame) <- c('Host', 'Webpage', 'Title', 'text') #Take only first page from each website #websiteDataFrame <- websiteDataFrame %>% distinct(Host, .keep_all = TRUE) %>% mutate(doc_id = Host) %>% select(Host, Text, Title) websiteDataFrame <- websiteDataFrame %>% distinct(Host, .keep_all = TRUE) %>% mutate(doc_id = sprintf("hi %d", 1:8)) %>% select(doc_id, text, Title)
websiteDataFrame
websiteDoc <- Corpus(DataframeSource(websiteDataFrame))
head(as.matrix(TermDocumentMatrix(websiteDoc)))
websiteDataFrame
library(tm) library(dplyr) library("SnowballC") library(e1071) #1.Read from file and preprocessing. 1 and 2 is inspired by https://www.springboard.com/blog/text-mining-in-r/ websitesFile <- 'websitesdata.txt'; websiteDataFrame <- read.table(websitesFile, sep = '\t', header = F, fill = TRUE, stringsAsFactors = FALSE); #deleting of not usefull fields.  websiteDataFrame <- websiteDataFrame[, c(2:4, 6)] names(websiteDataFrame) <- c('Host', 'Webpage', 'Title', 'text')
websiteDataFrame %>% distinct(Host, .keep_all = TRUE) %>% mutate(doc_id = sprintf("hi %d", 1:8)) %>% select(doc_id, text, Title)
websiteDataFrame %>% distinct(Host, .keep_all = TRUE) %>% mutate(doc_id = 50:58) %>% select(doc_id, text, Title)
websiteDataFrame %>% distinct(Host, .keep_all = TRUE) %>% mutate(doc_id = 50:59) %>% select(doc_id, text, Title)
websiteDataFrame %>% distinct(Host, .keep_all = TRUE) %>% mutate(doc_id = 50:57) %>% select(doc_id, text, Title)
websiteDataFrame <- websiteDataFrame %>% distinct(Host, .keep_all = TRUE) %>% mutate(doc_id = 50:57) %>% select(doc_id, text, Title)
websiteDoc <- Corpus(DataframeSource(websiteDataFrame)) head(as.matrix(TermDocumentMatrix(websiteDoc)))
library(tm) library(dplyr) library("SnowballC") library(e1071) #1.Read from file and preprocessing. 1 and 2 is inspired by https://www.springboard.com/blog/text-mining-in-r/ websitesFile <- 'websitesdata.txt'; websiteDataFrame <- read.table(websitesFile, sep = '\t', header = F, fill = TRUE, stringsAsFactors = FALSE); #deleting of not usefull fields.  websiteDataFrame <- websiteDataFrame[, c(2:4, 6)] names(websiteDataFrame) <- c('Host', 'Webpage', 'Title', 'text') #Take only first page from each website websiteDataFrame <- websiteDataFrame %>% distinct(Host, .keep_all = TRUE) %>% mutate(id = 50:57) %>% select(id, Host, Text, Title)
library(tm) library(dplyr) library("SnowballC") library(e1071) #1.Read from file and preprocessing. 1 and 2 is inspired by https://www.springboard.com/blog/text-mining-in-r/ websitesFile <- 'websitesdata.txt'; websiteDataFrame <- read.table(websitesFile, sep = '\t', header = F, fill = TRUE, stringsAsFactors = FALSE); #deleting of not usefull fields.  websiteDataFrame <- websiteDataFrame[, c(2:4, 6)] names(websiteDataFrame) <- c('Host', 'Webpage', 'Title', 'Text') #Take only first page from each website websiteDataFrame <- websiteDataFrame %>% distinct(Host, .keep_all = TRUE) %>% mutate(id = 50:57) %>% select(id, Host, Text, Title)
websiteDataFrame
performTmFunctions <- function(websiteText) {     #Create a document for each website     #websiteDoc <- Corpus(VectorSource(websiteText))     myReader <- readTabular(mapping = list(content = "Text", id = "Id"))     websiteDoc <- VCorpus(DataframeSource(websiteText), readerControl = list(reader = myReader))     dtmWebsiteDoc <- TermDocumentMatrix(websiteDoc)     #Remove qqpbreakqq?     #skipWords <- function(x) removeWords(x, c(stopwords("english"), "qqpbreakqq"))     #tmFuncs <- list(tolower, removePunctuation, removeNumbers, stripWhitespace, skipWords, stemDocument)     #websiteDoc <- tm_map(websiteDoc, FUN = tm_reduce, tmFuns = tmFuncs)     #websiteDoc <- tm_map(websiteDoc, PlainTextDocument)     #Read for testing     # websiteDoc[[1]][1]     #2. Term documents and word of cloud     #m <- as.matrix(dtmWebsiteDoc)     #v <- sort(rowSums(m), decreasing = TRUE)     #d <- data.frame(word = names(v), count = v)     #head(d,50)     # Generate the WordCloud     #Ggplotversion     #library(ggplot2)     #library(ggrepel)     #ggplot(data = d) +     #aes(x = 1, y = 1, size = count, label = word, col = count) +     #geom_text_repel(segment.size = 0, force = 100, segment.color = NA) +     #scale_size(range = c(1, 15), guide = FALSE) +     #scale_y_continuous(breaks = NULL) +     #scale_x_continuous(breaks = NULL) +     #labs(x = '', y = '') +     #theme_classic()     #Wordcloud      #library(wordcloud)     #wordcloud(d$word, d$count, scale = c(5, 0.5), max.words = 200, random.order = FALSE, rot.per = 0.6, use.r.layout = FALSE, colors = brewer.pal(8, 'Dark2')); }
TDWebsitesData <- performTmFunctions(websiteDataFrame) m <- as.matrix(TDWebsitesData)
TDWebsitesData <- performTmFunctions(websiteDataFrame)
library(tm) library(dplyr) library("SnowballC") library(e1071) #1.Read from file and preprocessing. 1 and 2 is inspired by https://www.springboard.com/blog/text-mining-in-r/ websitesFile <- 'websitesdata.txt'; websiteDataFrame <- read.table(websitesFile, sep = '\t', header = F, fill = TRUE, stringsAsFactors = FALSE); #deleting of not usefull fields.  websiteDataFrame <- websiteDataFrame[, c(2:4, 6)] names(websiteDataFrame) <- c('Host', 'Webpage', 'Title', 'Text') #Take only first page from each website websiteDataFrame <- websiteDataFrame %>% distinct(Host, .keep_all = TRUE) %>% mutate(id = 50:57) %>% select(Id, Host, Text, Title)
library(tm) library(dplyr) library("SnowballC") library(e1071) #1.Read from file and preprocessing. 1 and 2 is inspired by https://www.springboard.com/blog/text-mining-in-r/ websitesFile <- 'websitesdata.txt'; websiteDataFrame <- read.table(websitesFile, sep = '\t', header = F, fill = TRUE, stringsAsFactors = FALSE); #deleting of not usefull fields.  websiteDataFrame <- websiteDataFrame[, c(2:4, 6)] names(websiteDataFrame) <- c('Host', 'Webpage', 'Title', 'Text') #Take only first page from each website websiteDataFrame <- websiteDataFrame %>% distinct(Host, .keep_all = TRUE) %>% mutate(Id = 50:57) %>% select(Id, Host, Text, Title)
TDWebsitesData <- performTmFunctions(websiteDataFrame) m <- as.matrix(TDWebsitesData)
head(m)
performTmFunctions <- function(websiteText) {     #Create a document for each website     #websiteDoc <- Corpus(VectorSource(websiteText))     myReader <- readTabular(mapping = list(content = "Text", id = "Id"))     websiteDoc <- VCorpus(DataframeSource(websiteText), readerControl = list(reader = myReader))     dtmWebsiteDoc <- TermDocumentMatrix(websiteDoc)     #Remove qqpbreakqq?     skipWords <- function(x) removeWords(x, c(stopwords("english"), "qqpbreakqq"))     tmFuncs <- list(content_transformer(tolower), removePunctuation, removeNumbers, stripWhitespace, skipWords, stemDocument)     websiteDoc <- tm_map(websiteDoc, FUN = tm_reduce, tmFuns = tmFuncs)     websiteDoc <- tm_map(websiteDoc, PlainTextDocument)     websiteDoc     #Read for testing     # websiteDoc[[1]][1]     #2. Term documents and word of cloud     #m <- as.matrix(dtmWebsiteDoc)     #v <- sort(rowSums(m), decreasing = TRUE)     #d <- data.frame(word = names(v), count = v)     #head(d,50)     # Generate the WordCloud     #Ggplotversion     #library(ggplot2)     #library(ggrepel)     #ggplot(data = d) +     #aes(x = 1, y = 1, size = count, label = word, col = count) +     #geom_text_repel(segment.size = 0, force = 100, segment.color = NA) +     #scale_size(range = c(1, 15), guide = FALSE) +     #scale_y_continuous(breaks = NULL) +     #scale_x_continuous(breaks = NULL) +     #labs(x = '', y = '') +     #theme_classic()     #Wordcloud      #library(wordcloud)     #wordcloud(d$word, d$count, scale = c(5, 0.5), max.words = 200, random.order = FALSE, rot.per = 0.6, use.r.layout = FALSE, colors = brewer.pal(8, 'Dark2')); }
library(tm) library(dplyr) library("SnowballC") library(e1071) #1.Read from file and preprocessing. 1 and 2 is inspired by https://www.springboard.com/blog/text-mining-in-r/ websitesFile <- 'websitesdata.txt'; websiteDataFrame <- read.table(websitesFile, sep = '\t', header = F, fill = TRUE, stringsAsFactors = FALSE); #deleting of not usefull fields.  websiteDataFrame <- websiteDataFrame[, c(2:4, 6)] names(websiteDataFrame) <- c('Host', 'Webpage', 'Title', 'Text') #Take only first page from each website websiteDataFrame <- websiteDataFrame %>% distinct(Host, .keep_all = TRUE) %>% mutate(Id = 50:57) %>% select(Id, Host, Text, Title)
websiteDataFrame
TDWebsitesData <- performTmFunctions(websiteDataFrame) m <- as.matrix(TDWebsitesData)
performTmFunctions <- function(websiteText) {     #Create a document for each website     #websiteDoc <- Corpus(VectorSource(websiteText))     myReader <- readTabular(mapping = list(content = "Text", id = "Id"))     websiteDoc <- VCorpus(DataframeSource(websiteText), readerControl = list(reader = myReader))     dtmWebsiteDoc <- TermDocumentMatrix(websiteDoc)     #Remove qqpbreakqq?     skipWords <- function(x) removeWords(x, c(stopwords("english"), "qqpbreakqq"))     #tmFuncs <- list(content_transformer(tolower), removePunctuation, removeNumbers, stripWhitespace, skipWords, stemDocument)     tmFuncs <- list(tolower, removePunctuation, removeNumbers, stripWhitespace, skipWords, stemDocument)     websiteDoc <- tm_map(websiteDoc, FUN = tm_reduce, tmFuns = tmFuncs)     websiteDoc <- tm_map(websiteDoc, PlainTextDocument)     websiteDoc     #Read for testing     # websiteDoc[[1]][1]     #2. Term documents and word of cloud     #m <- as.matrix(dtmWebsiteDoc)     #v <- sort(rowSums(m), decreasing = TRUE)     #d <- data.frame(word = names(v), count = v)     #head(d,50)     # Generate the WordCloud     #Ggplotversion     #library(ggplot2)     #library(ggrepel)     #ggplot(data = d) +     #aes(x = 1, y = 1, size = count, label = word, col = count) +     #geom_text_repel(segment.size = 0, force = 100, segment.color = NA) +     #scale_size(range = c(1, 15), guide = FALSE) +     #scale_y_continuous(breaks = NULL) +     #scale_x_continuous(breaks = NULL) +     #labs(x = '', y = '') +     #theme_classic()     #Wordcloud      #library(wordcloud)     #wordcloud(d$word, d$count, scale = c(5, 0.5), max.words = 200, random.order = FALSE, rot.per = 0.6, use.r.layout = FALSE, colors = brewer.pal(8, 'Dark2')); }
TDWebsitesData <- performTmFunctions(websiteDataFrame) m <- as.matrix(TDWebsitesData)
performTmFunctions <- function(websiteText) {     #Create a document for each website     #websiteDoc <- Corpus(VectorSource(websiteText))     myReader <- readTabular(mapping = list(content = "Text", id = "Id"))     websiteDoc <- VCorpus(DataframeSource(websiteText), readerControl = list(reader = myReader))     dtmWebsiteDoc <- TermDocumentMatrix(websiteDoc)     #Remove qqpbreakqq?     skipWords <- function(x) removeWords(x, c(stopwords("english"), "qqpbreakqq"))     #tmFuncs <- list(content_transformer(tolower), removePunctuation, removeNumbers, stripWhitespace, skipWords, stemDocument)     tmFuncs <- list(content_transformer(tolower), removePunctuation, removeNumbers, stripWhitespace, skipWords, stemDocument)     websiteDoc <- tm_map(websiteDoc, FUN = tm_reduce, tmFuns = tmFuncs)     websiteDoc <- tm_map(websiteDoc, PlainTextDocument)     websiteDoc     #Read for testing     # websiteDoc[[1]][1]     #2. Term documents and word of cloud     #m <- as.matrix(dtmWebsiteDoc)     #v <- sort(rowSums(m), decreasing = TRUE)     #d <- data.frame(word = names(v), count = v)     #head(d,50)     # Generate the WordCloud     #Ggplotversion     #library(ggplot2)     #library(ggrepel)     #ggplot(data = d) +     #aes(x = 1, y = 1, size = count, label = word, col = count) +     #geom_text_repel(segment.size = 0, force = 100, segment.color = NA) +     #scale_size(range = c(1, 15), guide = FALSE) +     #scale_y_continuous(breaks = NULL) +     #scale_x_continuous(breaks = NULL) +     #labs(x = '', y = '') +     #theme_classic()     #Wordcloud      #library(wordcloud)     #wordcloud(d$word, d$count, scale = c(5, 0.5), max.words = 200, random.order = FALSE, rot.per = 0.6, use.r.layout = FALSE, colors = brewer.pal(8, 'Dark2')); }
library(tm) library(dplyr) library("SnowballC") library(e1071) #1.Read from file and preprocessing. 1 and 2 is inspired by https://www.springboard.com/blog/text-mining-in-r/ websitesFile <- 'websitesdata.txt'; websiteDataFrame <- read.table(websitesFile, sep = '\t', header = F, fill = TRUE, stringsAsFactors = FALSE);
ebsiteDataFrame <- websiteDataFrame[, c(2:4, 6)] names(websiteDataFrame) <- c('Host', 'Webpage', 'Title', 'Text') #Take only first page from each website websiteDataFrame <- websiteDataFrame %>% distinct(Host, .keep_all = TRUE) %>% mutate(Id = 50:57) %>% select(Id, Host, Text, Title)
head(websiteDataFrame)
websiteDataFrame <- websiteDataFrame %>% distinct(Host, .keep_all = TRUE) %>% mutate(Id = 50:57) %>% select(Id, Host, Text, Title)
websiteDataFrame %>% distinct(Host, .keep_all = TRUE)
library(tm) library(dplyr) library("SnowballC") library(e1071) #1.Read from file and preprocessing. 1 and 2 is inspired by https://www.springboard.com/blog/text-mining-in-r/ websitesFile <- 'websitesdata.txt'; websiteDataFrame <- read.table(websitesFile, sep = '\t', header = F, fill = TRUE, stringsAsFactors = FALSE);
#deleting of not usefull fields.  websiteDataFrame <- websiteDataFrame[, c(2:4, 6)] names(websiteDataFrame) <- c('Host', 'Webpage', 'Title', 'Text')
websiteDataFrame %>% distinct(Host, .keep_all = TRUE     )
websiteDataFrame <- websiteDataFrame %>% distinct(Host, .keep_all = TRUE) %>% mutate(Id = 50:57) %>% select(Id, Host, Text, Title)
websiteDataFrame
performTmFunctions <- function(websiteText) {     #Create a document for each website     #websiteDoc <- Corpus(VectorSource(websiteText))     myReader <- readTabular(mapping = list(content = "Text", id = "Id"))     websiteDoc <- VCorpus(DataframeSource(websiteText), readerControl = list(reader = myReader))     dtmWebsiteDoc <- TermDocumentMatrix(websiteDoc)     #Remove qqpbreakqq?     skipWords <- function(x) removeWords(x, c(stopwords("english"), "qqpbreakqq"))     #tmFuncs <- list(content_transformer(tolower), removePunctuation, removeNumbers, stripWhitespace, skipWords, stemDocument)     tmFuncs <- list(content_transformer(tolower), removePunctuation, removeNumbers, stripWhitespace, skipWords, stemDocument)     websiteDoc <- tm_map(websiteDoc, FUN = tm_reduce, tmFuns = tmFuncs)     websiteDoc <- tm_map(websiteDoc, PlainTextDocument)     websiteDoc     #Read for testing     # websiteDoc[[1]][1]     #2. Term documents and word of cloud     #m <- as.matrix(dtmWebsiteDoc)     #v <- sort(rowSums(m), decreasing = TRUE)     #d <- data.frame(word = names(v), count = v)     #head(d,50)     # Generate the WordCloud     #Ggplotversion     #library(ggplot2)     #library(ggrepel)     #ggplot(data = d) +     #aes(x = 1, y = 1, size = count, label = word, col = count) +     #geom_text_repel(segment.size = 0, force = 100, segment.color = NA) +     #scale_size(range = c(1, 15), guide = FALSE) +     #scale_y_continuous(breaks = NULL) +     #scale_x_continuous(breaks = NULL) +     #labs(x = '', y = '') +     #theme_classic()     #Wordcloud      #library(wordcloud)     #wordcloud(d$word, d$count, scale = c(5, 0.5), max.words = 200, random.order = FALSE, rot.per = 0.6, use.r.layout = FALSE, colors = brewer.pal(8, 'Dark2')); }
TDWebsitesData <- performTmFunctions(websiteDataFrame) m <- as.matrix(TDWebsitesData)
websiteDataFrame
performTmFunctions <- function(websiteText) {     #Create a document for each website     #websiteDoc <- Corpus(VectorSource(websiteText))     myReader <- readTabular(mapping = list(content = "Text", id = "Id"))     websiteDoc <- VCorpus(DataframeSource(websiteText), readerControl = list(reader = myReader))     #Remove qqpbreakqq?     skipWords <- function(x) removeWords(x, c(stopwords("english"), "qqpbreakqq"))     #tmFuncs <- list(content_transformer(tolower), removePunctuation, removeNumbers, stripWhitespace, skipWords, stemDocument)     tmFuncs <- list(content_transformer(tolower), removePunctuation, removeNumbers, stripWhitespace, skipWords, stemDocument)     websiteDoc <- tm_map(websiteDoc, FUN = tm_reduce, tmFuns = tmFuncs)     websiteDoc <- tm_map(websiteDoc, PlainTextDocument)     websiteDoc     #Read for testing     # websiteDoc[[1]][1]     #2. Term documents and word of cloud     #m <- as.matrix(dtmWebsiteDoc)     #v <- sort(rowSums(m), decreasing = TRUE)     #d <- data.frame(word = names(v), count = v)     #head(d,50)     # Generate the WordCloud     #Ggplotversion     #library(ggplot2)     #library(ggrepel)     #ggplot(data = d) +     #aes(x = 1, y = 1, size = count, label = word, col = count) +     #geom_text_repel(segment.size = 0, force = 100, segment.color = NA) +     #scale_size(range = c(1, 15), guide = FALSE) +     #scale_y_continuous(breaks = NULL) +     #scale_x_continuous(breaks = NULL) +     #labs(x = '', y = '') +     #theme_classic()     #Wordcloud      #library(wordcloud)     #wordcloud(d$word, d$count, scale = c(5, 0.5), max.words = 200, random.order = FALSE, rot.per = 0.6, use.r.layout = FALSE, colors = brewer.pal(8, 'Dark2')); }
TDWebsitesData <- performTmFunctions(websiteDataFrame)
m <- as.matrix(TDWebsitesData)
as.matrix(TDWebsitesData)
performTmFunctions <- function(websiteText) {     #Create a document for each website     #websiteDoc <- Corpus(VectorSource(websiteText))     myReader <- readTabular(mapping = list(content = "Text", id = "Id"))     websiteDoc <- VCorpus(DataframeSource(websiteText), readerControl = list(reader = myReader))     #Remove qqpbreakqq?     skipWords <- function(x) removeWords(x, c(stopwords("english"), "qqpbreakqq"))     #tmFuncs <- list(content_transformer(tolower), removePunctuation, removeNumbers, stripWhitespace, skipWords, stemDocument)     tmFuncs <- list(content_transformer(tolower), removePunctuation, removeNumbers, stripWhitespace, skipWords, stemDocument)     websiteDoc <- tm_map(websiteDoc, FUN = tm_reduce, tmFuns = tmFuncs)     websiteDoc <- tm_map(websiteDoc, PlainTextDocument)     TermDocumentMatrix(websiteDoc)     #Read for testing     # websiteDoc[[1]][1]     #2. Term documents and word of cloud     #m <- as.matrix(dtmWebsiteDoc)     #v <- sort(rowSums(m), decreasing = TRUE)     #d <- data.frame(word = names(v), count = v)     #head(d,50)     # Generate the WordCloud     #Ggplotversion     #library(ggplot2)     #library(ggrepel)     #ggplot(data = d) +     #aes(x = 1, y = 1, size = count, label = word, col = count) +     #geom_text_repel(segment.size = 0, force = 100, segment.color = NA) +     #scale_size(range = c(1, 15), guide = FALSE) +     #scale_y_continuous(breaks = NULL) +     #scale_x_continuous(breaks = NULL) +     #labs(x = '', y = '') +     #theme_classic()     #Wordcloud      #library(wordcloud)     #wordcloud(d$word, d$count, scale = c(5, 0.5), max.words = 200, random.order = FALSE, rot.per = 0.6, use.r.layout = FALSE, colors = brewer.pal(8, 'Dark2')); }
library(tm) library(dplyr) library("SnowballC") library(e1071) #1.Read from file and preprocessing. 1 and 2 is inspired by https://www.springboard.com/blog/text-mining-in-r/ websitesFile <- 'websitesdata.txt'; websiteDataFrame <- read.table(websitesFile, sep = '\t', header = F, fill = TRUE, stringsAsFactors = FALSE); #deleting of not usefull fields.  websiteDataFrame <- websiteDataFrame[, c(2:4, 6)] names(websiteDataFrame) <- c('Host', 'Webpage', 'Title', 'Text') #Take only first page from each website websiteDataFrame <- websiteDataFrame %>% distinct(Host, .keep_all = TRUE) %>% mutate(Id = 50:57) %>% select(Id, Host, Text, Title)
#websitesText <- websiteDataFrame[, 'Text'] TDWebsitesData <- performTmFunctions(websiteDataFrame) m <- as.matrix(TDWebsitesData)
head(m)
library(tm) library(dplyr) library("SnowballC") library(e1071) #1.Read from file and preprocessing. 1 and 2 is inspired by https://www.springboard.com/blog/text-mining-in-r/ websitesFile <- 'websitesdata.txt'; websiteDataFrame <- read.table(websitesFile, sep = '\t', header = F, fill = TRUE, stringsAsFactors = FALSE); #deleting of not usefull fields.  websiteDataFrame <- websiteDataFrame[, c(2:4, 6)] names(websiteDataFrame) <- c('Host', 'Webpage', 'Title', 'Text') #Take only first page from each website websiteDataFrame <- websiteDataFrame %>% distinct(Host, .keep_all = TRUE) %>% mutate(Id = 50:57) %>% select(Id, Host, Text, Title) #websitesText <- websiteDataFrame[, 'Text'] TDWebsitesData <- performTmFunctions(websiteDataFrame) m <- as.matrix(TDWebsitesData) performTmFunctions <- function(websiteText) {     #Create a document for each website     #websiteDoc <- Corpus(VectorSource(websiteText))     myReader <- readTabular(mapping = list(content = "Text", id = "Id"))     websiteDoc <- VCorpus(DataframeSource(websiteText), readerControl = list(reader = myReader))     #Remove qqpbreakqq?     skipWords <- function(x) removeWords(x, c(stopwords("english"), "qqpbreakqq"))     #tmFuncs <- list(content_transformer(tolower), removePunctuation, removeNumbers, stripWhitespace, skipWords, stemDocument)     tmFuncs <- list(content_transformer(tolower), removePunctuation, removeNumbers, stripWhitespace, skipWords, stemDocument)     # websiteDoc <- tm_map(websiteDoc, FUN = tm_reduce, tmFuns = tmFuncs)     #websiteDoc <- tm_map(websiteDoc, PlainTextDocument)     TermDocumentMatrix(websiteDoc)     #Read for testing     # websiteDoc[[1]][1]     #2. Term documents and word of cloud     #m <- as.matrix(dtmWebsiteDoc)     #v <- sort(rowSums(m), decreasing = TRUE)     #d <- data.frame(word = names(v), count = v)     #head(d,50)     # Generate the WordCloud     #Ggplotversion     #library(ggplot2)     #library(ggrepel)     #ggplot(data = d) +     #aes(x = 1, y = 1, size = count, label = word, col = count) +     #geom_text_repel(segment.size = 0, force = 100, segment.color = NA) +     #scale_size(range = c(1, 15), guide = FALSE) +     #scale_y_continuous(breaks = NULL) +     #scale_x_continuous(breaks = NULL) +     #labs(x = '', y = '') +     #theme_classic()     #Wordcloud      #library(wordcloud)     #wordcloud(d$word, d$count, scale = c(5, 0.5), max.words = 200, random.order = FALSE, rot.per = 0.6, use.r.layout = FALSE, colors = brewer.pal(8, 'Dark2')); }
performTmFunctions <- function(websiteText) {     #Create a document for each website     #websiteDoc <- Corpus(VectorSource(websiteText))     myReader <- readTabular(mapping = list(content = "Text", id = "Id"))     websiteDoc <- VCorpus(DataframeSource(websiteText), readerControl = list(reader = myReader))     #Remove qqpbreakqq?     skipWords <- function(x) removeWords(x, c(stopwords("english"), "qqpbreakqq"))     #tmFuncs <- list(content_transformer(tolower), removePunctuation, removeNumbers, stripWhitespace, skipWords, stemDocument)     tmFuncs <- list(content_transformer(tolower), removePunctuation, removeNumbers, stripWhitespace, skipWords, stemDocument)     # websiteDoc <- tm_map(websiteDoc, FUN = tm_reduce, tmFuns = tmFuncs)     #websiteDoc <- tm_map(websiteDoc, PlainTextDocument)     TermDocumentMatrix(websiteDoc)     #Read for testing     # websiteDoc[[1]][1]     #2. Term documents and word of cloud     #m <- as.matrix(dtmWebsiteDoc)     #v <- sort(rowSums(m), decreasing = TRUE)     #d <- data.frame(word = names(v), count = v)     #head(d,50)     # Generate the WordCloud     #Ggplotversion     #library(ggplot2)     #library(ggrepel)     #ggplot(data = d) +     #aes(x = 1, y = 1, size = count, label = word, col = count) +     #geom_text_repel(segment.size = 0, force = 100, segment.color = NA) +     #scale_size(range = c(1, 15), guide = FALSE) +     #scale_y_continuous(breaks = NULL) +     #scale_x_continuous(breaks = NULL) +     #labs(x = '', y = '') +     #theme_classic()     #Wordcloud      #library(wordcloud)     #wordcloud(d$word, d$count, scale = c(5, 0.5), max.words = 200, random.order = FALSE, rot.per = 0.6, use.r.layout = FALSE, colors = brewer.pal(8, 'Dark2')); }
library(tm) library(dplyr) library("SnowballC") library(e1071) #1.Read from file and preprocessing. 1 and 2 is inspired by https://www.springboard.com/blog/text-mining-in-r/ websitesFile <- 'websitesdata.txt'; websiteDataFrame <- read.table(websitesFile, sep = '\t', header = F, fill = TRUE, stringsAsFactors = FALSE); #deleting of not usefull fields.  websiteDataFrame <- websiteDataFrame[, c(2:4, 6)] names(websiteDataFrame) <- c('Host', 'Webpage', 'Title', 'Text') #Take only first page from each website websiteDataFrame <- websiteDataFrame %>% distinct(Host, .keep_all = TRUE) %>% mutate(Id = 50:57) %>% select(Id, Host, Text, Title) #websitesText <- websiteDataFrame[, 'Text'] TDWebsitesData <- performTmFunctions(websiteDataFrame) m <- as.matrix(TDWebsitesData)
head(m)
performTmFunctions <- function(websiteText) {     #Create a document for each website     #websiteDoc <- Corpus(VectorSource(websiteText))     myReader <- readTabular(mapping = list(content = "Text", id = "Id"))     websiteDoc <- VCorpus(DataframeSource(websiteText), readerControl = list(reader = myReader))     #Remove qqpbreakqq?     skipWords <- function(x) removeWords(x, c(stopwords("english"), "qqpbreakqq"))     #tmFuncs <- list(content_transformer(tolower), removePunctuation, removeNumbers, stripWhitespace, skipWords, stemDocument)     tmFuncs <- list(removePunctuation, removeNumbers, stripWhitespace, skipWords, stemDocument)     websiteDoc <- tm_map(websiteDoc, FUN = tm_reduce, tmFuns = tmFuncs)     #websiteDoc <- tm_map(websiteDoc, PlainTextDocument)     TermDocumentMatrix(websiteDoc)     #Read for testing     # websiteDoc[[1]][1]     #2. Term documents and word of cloud     #m <- as.matrix(dtmWebsiteDoc)     #v <- sort(rowSums(m), decreasing = TRUE)     #d <- data.frame(word = names(v), count = v)     #head(d,50)     # Generate the WordCloud     #Ggplotversion     #library(ggplot2)     #library(ggrepel)     #ggplot(data = d) +     #aes(x = 1, y = 1, size = count, label = word, col = count) +     #geom_text_repel(segment.size = 0, force = 100, segment.color = NA) +     #scale_size(range = c(1, 15), guide = FALSE) +     #scale_y_continuous(breaks = NULL) +     #scale_x_continuous(breaks = NULL) +     #labs(x = '', y = '') +     #theme_classic()     #Wordcloud      #library(wordcloud)     #wordcloud(d$word, d$count, scale = c(5, 0.5), max.words = 200, random.order = FALSE, rot.per = 0.6, use.r.layout = FALSE, colors = brewer.pal(8, 'Dark2')); }
library(tm) library(dplyr) library("SnowballC") library(e1071) #1.Read from file and preprocessing. 1 and 2 is inspired by https://www.springboard.com/blog/text-mining-in-r/ websitesFile <- 'websitesdata.txt'; websiteDataFrame <- read.table(websitesFile, sep = '\t', header = F, fill = TRUE, stringsAsFactors = FALSE); #deleting of not usefull fields.  websiteDataFrame <- websiteDataFrame[, c(2:4, 6)] names(websiteDataFrame) <- c('Host', 'Webpage', 'Title', 'Text') #Take only first page from each website websiteDataFrame <- websiteDataFrame %>% distinct(Host, .keep_all = TRUE) %>% mutate(Id = 50:57) %>% select(Id, Host, Text, Title) #websitesText <- websiteDataFrame[, 'Text'] TDWebsitesData <- performTmFunctions(websiteDataFrame) m <- as.matrix(TDWebsitesData)
head(m)
performTmFunctions <- function(websiteText) {     #Create a document for each website     #websiteDoc <- Corpus(VectorSource(websiteText))     myReader <- readTabular(mapping = list(content = "Text", id = "Id"))     websiteDoc <- VCorpus(DataframeSource(websiteText), readerControl = list(reader = myReader))     #Remove qqpbreakqq?     skipWords <- function(x) removeWords(x, c(stopwords("english"), "qqpbreakqq"))     #tmFuncs <- list(content_transformer(tolower), removePunctuation, removeNumbers, stripWhitespace, skipWords, stemDocument)     tmFuncs <- list(tolower, removePunctuation, removeNumbers, stripWhitespace, skipWords, stemDocument)     websiteDoc <- tm_map(websiteDoc, FUN = tm_reduce, tmFuns = tmFuncs)     #websiteDoc <- tm_map(websiteDoc, PlainTextDocument)     TermDocumentMatrix(websiteDoc)     #Read for testing     # websiteDoc[[1]][1]     #2. Term documents and word of cloud     #m <- as.matrix(dtmWebsiteDoc)     #v <- sort(rowSums(m), decreasing = TRUE)     #d <- data.frame(word = names(v), count = v)     #head(d,50)     # Generate the WordCloud     #Ggplotversion     #library(ggplot2)     #library(ggrepel)     #ggplot(data = d) +     #aes(x = 1, y = 1, size = count, label = word, col = count) +     #geom_text_repel(segment.size = 0, force = 100, segment.color = NA) +     #scale_size(range = c(1, 15), guide = FALSE) +     #scale_y_continuous(breaks = NULL) +     #scale_x_continuous(breaks = NULL) +     #labs(x = '', y = '') +     #theme_classic()     #Wordcloud      #library(wordcloud)     #wordcloud(d$word, d$count, scale = c(5, 0.5), max.words = 200, random.order = FALSE, rot.per = 0.6, use.r.layout = FALSE, colors = brewer.pal(8, 'Dark2')); }
library(tm) library(dplyr) library("SnowballC") library(e1071) #1.Read from file and preprocessing. 1 and 2 is inspired by https://www.springboard.com/blog/text-mining-in-r/ websitesFile <- 'websitesdata.txt'; websiteDataFrame <- read.table(websitesFile, sep = '\t', header = F, fill = TRUE, stringsAsFactors = FALSE); #deleting of not usefull fields.  websiteDataFrame <- websiteDataFrame[, c(2:4, 6)] names(websiteDataFrame) <- c('Host', 'Webpage', 'Title', 'Text') #Take only first page from each website websiteDataFrame <- websiteDataFrame %>% distinct(Host, .keep_all = TRUE) %>% mutate(Id = 50:57) %>% select(Id, Host, Text, Title) #websitesText <- websiteDataFrame[, 'Text'] TDWebsitesData <- performTmFunctions(websiteDataFrame) m <- as.matrix(TDWebsitesData)
performTmFunctions <- function(websiteText) {     #Create a document for each website     #websiteDoc <- Corpus(VectorSource(websiteText))     myReader <- readTabular(mapping = list(content = "Text", id = "Id"))     websiteDoc <- VCorpus(DataframeSource(websiteText), readerControl = list(reader = myReader))     #Remove qqpbreakqq?     skipWords <- function(x) removeWords(x, c(stopwords("english"), "qqpbreakqq"))     #tmFuncs <- list(content_transformer(tolower), removePunctuation, removeNumbers, stripWhitespace, skipWords, stemDocument)     tmFuncs <- list(tolower, removePunctuation, removeNumbers, stripWhitespace, skipWords, stemDocument)     websiteDoc <- tm_map(websiteDoc, FUN = tm_reduce, tmFuns = tmFuncs)     #websiteDoc <- tm_map(websiteDoc, PlainTextDocument)     TermDocumentMatrix(websiteDoc)     #Read for testing     # websiteDoc[[1]][1]     #2. Term documents and word of cloud     #m <- as.matrix(dtmWebsiteDoc)     #v <- sort(rowSums(m), decreasing = TRUE)     #d <- data.frame(word = names(v), count = v)     #head(d,50)     # Generate the WordCloud     #Ggplotversion     #library(ggplot2)     #library(ggrepel)     #ggplot(data = d) +     #aes(x = 1, y = 1, size = count, label = word, col = count) +     #geom_text_repel(segment.size = 0, force = 100, segment.color = NA) +     #scale_size(range = c(1, 15), guide = FALSE) +     #scale_y_continuous(breaks = NULL) +     #scale_x_continuous(breaks = NULL) +     #labs(x = '', y = '') +     #theme_classic()     #Wordcloud      #library(wordcloud)     #wordcloud(d$word, d$count, scale = c(5, 0.5), max.words = 200, random.order = FALSE, rot.per = 0.6, use.r.layout = FALSE, colors = brewer.pal(8, 'Dark2')); }
library(tm) library(dplyr) library("SnowballC") library(e1071) #1.Read from file and preprocessing. 1 and 2 is inspired by https://www.springboard.com/blog/text-mining-in-r/ websitesFile <- 'websitesdata.txt'; websiteDataFrame <- read.table(websitesFile, sep = '\t', header = F, fill = TRUE, stringsAsFactors = FALSE); #deleting of not usefull fields.  websiteDataFrame <- websiteDataFrame[, c(2:4, 6)] names(websiteDataFrame) <- c('Host', 'Webpage', 'Title', 'Text') #Take only first page from each website websiteDataFrame <- websiteDataFrame %>% distinct(Host, .keep_all = TRUE) %>% mutate(Id = 50:57) %>% select(Id, Host, Text, Title) #websitesText <- websiteDataFrame[, 'Text'] TDWebsitesData <- performTmFunctions(websiteDataFrame) m <- as.matrix(TDWebsitesData)
performTmFunctions <- function(websiteText) {     #Create a document for each website     #websiteDoc <- Corpus(VectorSource(websiteText))     myReader <- readTabular(mapping = list(content = "Text", id = "Id"))     websiteDoc <- VCorpus(DataframeSource(websiteText), readerControl = list(reader = myReader))     #Remove qqpbreakqq?     skipWords <- function(x) removeWords(x, c(stopwords("english"), "qqpbreakqq"))     #tmFuncs <- list(content_transformer(tolower), removePunctuation, removeNumbers, stripWhitespace, skipWords, stemDocument)     tmFuncs <- list(removePunctuation, removeNumbers, stripWhitespace, skipWords, stemDocument)     websiteDoc <- tm_map(websiteDoc, FUN = tm_reduce, tmFuns = tmFuncs)     #websiteDoc <- tm_map(websiteDoc, PlainTextDocument)     TermDocumentMatrix(websiteDoc)     #Read for testing     # websiteDoc[[1]][1]     #2. Term documents and word of cloud     #m <- as.matrix(dtmWebsiteDoc)     #v <- sort(rowSums(m), decreasing = TRUE)     #d <- data.frame(word = names(v), count = v)     #head(d,50)     # Generate the WordCloud     #Ggplotversion     #library(ggplot2)     #library(ggrepel)     #ggplot(data = d) +     #aes(x = 1, y = 1, size = count, label = word, col = count) +     #geom_text_repel(segment.size = 0, force = 100, segment.color = NA) +     #scale_size(range = c(1, 15), guide = FALSE) +     #scale_y_continuous(breaks = NULL) +     #scale_x_continuous(breaks = NULL) +     #labs(x = '', y = '') +     #theme_classic()     #Wordcloud      #library(wordcloud)     #wordcloud(d$word, d$count, scale = c(5, 0.5), max.words = 200, random.order = FALSE, rot.per = 0.6, use.r.layout = FALSE, colors = brewer.pal(8, 'Dark2')); }
library(tm) library(dplyr) library("SnowballC") library(e1071) #1.Read from file and preprocessing. 1 and 2 is inspired by https://www.springboard.com/blog/text-mining-in-r/ websitesFile <- 'websitesdata.txt'; websiteDataFrame <- read.table(websitesFile, sep = '\t', header = F, fill = TRUE, stringsAsFactors = FALSE); #deleting of not usefull fields.  websiteDataFrame <- websiteDataFrame[, c(2:4, 6)] names(websiteDataFrame) <- c('Host', 'Webpage', 'Title', 'Text') #Take only first page from each website websiteDataFrame <- websiteDataFrame %>% distinct(Host, .keep_all = TRUE) %>% mutate(Id = 50:57) %>% select(Id, Host, Text, Title) #websitesText <- websiteDataFrame[, 'Text'] TDWebsitesData <- performTmFunctions(websiteDataFrame) m <- as.matrix(TDWebsitesData)
head(m)
m
create_container ? ? ??create_container
?create_container
?train_model
??train_model
