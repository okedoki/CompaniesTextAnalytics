library(tm)
library('tm')
librar
load
install.packages('tm')
load(tm)
library(tm)
websitesFile <- 'websitedata.txt'; df <- read.table(websitesFile, sep = '\t', header = F, skip = 2, quote = '', comment = '');
websitesFile <- 'websitesdata.txt'; df <- read.table(websitesFile, sep = '\t', header = F, skip = 2, quote = '', comment = '');
library(tm) websitesFile <- 'websitesdata.txt'; df <- read.table(websitesFile, sep = '\t', header = F, skip = 2, quote = '', comment = '');
?read.table
df <- read.table(websitesFile, sep = '\t', header = F, skip = 2, quote = '', comment = '', fill = TRUE);
df
websiteDataFrame <- read.table(websitesFile, sep = '\t', header = F, fill = TRUE);
class(websiteDataFrame)
head(websiteDataFrame)
websiteDataFrame(1)
websiteDataFrame.head(1)
head(websiteDataFrame,1)
head(websiteDataFrame,2)
head(websiteDataFrame)
str(websiteDataFrame)
websiteDataFrame <- read.table(websitesFile, sep = '\t', header = F, fill = TRUE, stringsAsFactors = FALSE);
str(websiteDataFrame)
head(websiteDataFrame[c(2:5)])
head(websiteDataFrame[c(2:6)])
head(websiteDataFrame[c(2:4, 6)])
websiteDataFrame <- head(websiteDataFrame[c(2:4, 6)]) names(websiteDataFrame) <- c('Host', 'Webpage', 'Title', 'Text')
head(websiteDataFrame)
websiteDataFrame[1, 'Text']
websiteDataFrame[2, 'Text']
websiteDataFrame[3, 'Text']
textExample <- websiteDataFrame[1, 'Text']
textExample
tm_map(textExample, removeNumbers)
docs <- Corpus(VectorSource(textExample))
docs
str(docs)
doc
docs$content
docs$content[[1]]
docs
head(docs)
str(docs)
docs[1]$content
docs[1]$content$Content
docs[1]$content[Content]
docs[1]$content['Content']
docs[[1]]
docs[[1]][1]
websiteDocs <- tm_map(websiteDocs, removeNumbers) websiteDocs <- tm_map(websiteDocs, removeWords, stopwords("english")) websiteDocs <- tm_map(websiteDocs, removePunctuation)
websiteDocs <- Corpus(VectorSource(textExample)) websiteDocs[[1]][1] websiteDocs <- tm_map(websiteDocs, removeNumbers) websiteDocs <- tm_map(websiteDocs, removeWords, stopwords("english")) websiteDocs <- tm_map(websiteDocs, removePunctuation)
websiteDocs[[1]][1]
websiteDocs <- tm_map(websiteDocs, stripWhitespace)
websiteDocs[[1]][1]
install.packages('SnowballC')
library("SnowballC")
websiteDocs <- tm_map(websiteDocs, stemDocument)
websiteDocs[[1]][1]
?tm_map
?tm_map
websiteDocs <- tm_map(websiteDocs, removeWords, c("qqpbreakqq"))
websiteDocs[[1]][1]
websiteDocs <- Corpus(VectorSource(textExample)) websiteDocs <- tm_map(websiteDocs, removeNumbers) websiteDocs <- tm_map(websiteDocs, removeWords, c(stopwords("english"), "qqpbreakqq")) #Remove qqpbreakqq websiteDocs <- tm_map(websiteDocs, removeWords, c("qqpbreakqq"))
websiteDocs[[1]][1]
websiteDocs <- tm_map(websiteDocs, stripWhitespace)
#websiteDocs[[1]][1]
websiteDocs[[1]][1]
dtmWebsiteDocs <- TermDocumentMatrix(websiteDocs)
dtmWebsiteDocs
str(dtmWebsiteDocs)
dtmWebsiteDocs <- TermDocumentMatrix(websiteDocs) m <- as.matrix(dtmWebsiteDocs) v <- sort(rowSums(m), decreasing = TRUE) d <- data.frame(word = names(v), freq = v) head(d)
d
head(d)
dtmWebsiteDocs <- TermDocumentMatrix(websiteDocs) m <- as.matrix(dtmWebsiteDocs) v <- sort(rowSums(m), decreasing = TRUE) d <- data.frame(word = names(v), count = v)
ggplot +   aes(x = 1, y = 1, size = count, label = word) +   geom_text_repel(segment.size = 0, force = 100)
library(ggplot2) library(ggrepel) ggplot +   aes(x = 1, y = 1, size = count, label = word) +   geom_text_repel(segment.size = 0, force = 100)
ggrepel
load.library(ggrepel)
install.packages('ggrepel')
library(ggplot2) library(ggrepel)
ggplot +   aes(x = 1, y = 1, size = count, label = word) +   geom_text_repel(segment.size = 0, force = 100)
ggplot +   aes(x = 1, y = 1, size = count, label = word) +   geom_text_repel(segment.size = 0, force = 100) +   scale_size(range = c(2, 15), guide = FALSE) +   scale_y_continuous(breaks = NULL) +   scale_x_continuous(breaks = NULL) +   labs(x = '', y = '') +   theme_classic()
d <- data.frame(word = names(v), count = v) ggplot(data = d) +   aes(x = 1, y = 1, size = count, label = word) +   geom_text_repel(segment.size = 0, force = 100) +   scale_size(range = c(2, 15), guide = FALSE) +   scale_y_continuous(breaks = NULL) +   scale_x_continuous(breaks = NULL) +   labs(x = '', y = '') +   theme_classic()
ggplot(data = d) +   aes(x = 1, y = 1, size = count, label = word) +   geom_text_repel(segment.size = 0, force = 100) +   scale_size(range = c(2, 15), guide = FALSE) +   scale_y_continuous(breaks = NULL) +   scale_x_continuous(breaks = NULL) +   labs(x = '', y = '') +   theme_classic()
ggplot(data = d) + +aes(x = 1, y = 1, size = count, label = word) + +geom_text_repel(segment.size = 0, force = 100)
library(tm) websitesFile <- 'websitesdata.txt'; websiteDataFrame <- read.table(websitesFile, sep = '\t', header = F, fill = TRUE, stringsAsFactors = FALSE); #deleting of not usefull fields.  websiteDataFrame <- head(websiteDataFrame[c(2:4, 6)]) names(websiteDataFrame) <- c('Host', 'Webpage', 'Title', 'Text') #TODO - create a loop textExample <- websiteDataFrame[1, 'Text'] #Create a document for each website websiteDocs <- Corpus(VectorSource(textExample)) websiteDocs <- tm_map(websiteDocs, removeNumbers) #Remove qqpbreakqq? websiteDocs <- tm_map(websiteDocs, removeWords, c(stopwords("english"), "qqpbreakqq")) #websiteDocs <- tm_map(websiteDocs, removePunctuation) websiteDocs <- tm_map(websiteDocs, stripWhitespace) library("SnowballC") websiteDocs <- tm_map(websiteDocs, stemDocument)
textExample
library(tm) websitesFile <- 'websitesdata.txt'; websiteDataFrame <- read.table(websitesFile, sep = '\t', header = F, fill = TRUE, stringsAsFactors = FALSE); #deleting of not usefull fields.  websiteDataFrame <- head(websiteDataFrame[c(2:4, 6)]) names(websiteDataFrame) <- c('Host', 'Webpage', 'Title', 'Text') #TODO - create a loop textExample <- websiteDataFrame[1, 'Text'] #Create a document for each website websiteDocs <- Corpus(VectorSource(textExample)) websiteDocs <- tm_map(websiteDocs, removeNumbers) #Remove qqpbreakqq? websiteDocs <- tm_map(websiteDocs, removeWords, c(stopwords("english"), "qqpbreakqq")) #websiteDocs <- tm_map(websiteDocs, removePunctuation) websiteDocs <- tm_map(websiteDocs, stripWhitespace) library("SnowballC") websiteDocs <- tm_map(websiteDocs, stemDocument)
websiteDocs[[1]][1]
dtmWebsiteDocs <- TermDocumentMatrix(websiteDocs) m <- as.matrix(dtmWebsiteDocs) v <- sort(rowSums(m), decreasing = TRUE) d <- data.frame(word = names(v), count = v) head(d) # Generate the WordCloud library(ggplot2) library(ggrepel)
ggplot(data = d) +   aes(x = 1, y = 1, size = count, label = word) +   geom_text_repel(segment.size = 0, force = 100)
ggplot(data = d) +   aes(x = 1, y = 1, size = count, label = word) +   geom_text_repel(segment.size = 0, force = 100) +   scale_size(range = c(2, 15), guide = FALSE) +   scale_y_continuous(breaks = NULL) +   scale_x_continuous(breaks = NULL) +   labs(x = '', y = '') +   theme_classic()
ggplot(data = d) +   aes(x = 1, y = 1, size = count, label = word) +   geom_text_repel(segment.size = 0, force = 100) +   scale_size(range = c(2, 15), guide = FALSE)
ggplot(data = d) +   aes(x = 1, y = 1, size = count, label = word) +   geom_text_repel(segment.size = 0, force = 100) +   scale_size(range = c(2, 15), guide = FALSE) +   scale_y_continuous(breaks = NULL) +   scale_x_continuous(breaks = NULL)
ggplot(data = d) +   aes(x = 1, y = 1, size = count, label = word) +   geom_text_repel(segment.size = 0, force = 100) +   scale_size(range = c(2, 15), guide = FALSE) +   scale_y_continuous(breaks = NULL) +   scale_x_continuous(breaks = NULL) +   labs(x = '', y = '') +   theme_classic()
ggplot(data = d) +   aes(x = 1, y = 1, size = count, label = word) +   geom_text_repel(segment.size = 0, force = 100) +   scale_size(range = c(2, 15), guide = FALSE) +   scale_y_continuous(breaks = NULL) +   scale_x_continuous(breaks = NULL) +   labs(x = '', y = '') +   theme_classic()
ggplot(data = d) +   aes(x = 1, y = 1, size = count, label = word, col = count) +   geom_text_repel(segment.size = 0, force = 100) +   scale_size(range = c(2, 15), guide = FALSE) +   scale_y_continuous(breaks = NULL) +   scale_x_continuous(breaks = NULL) +   labs(x = '', y = '') +   theme_classic()
ggplot(data = d) +   aes(x = 1, y = 1, size = count, label = word, col = count) +   geom_text_repel(segment.size = 0, force = 100) +   scale_size(range = c(2, 15), guide = FALSE) +   scale_y_continuous(breaks = NULL) +   scale_x_continuous(breaks = NULL) +   labs(x = '', y = '')
ggplot(data = d) +   aes(x = 1, y = 1, size = count, label = word, col = count)
ggplot(data = d) +   aes(x = 1, y = 1, size = count, label = word, col = count) + theme_classic()
ggplot(data = d) +   aes(x = 1, y = 1, size = count, label = word, col = count) +   geom_text_repel(segment.size = 0, force = 100) +   scale_size(range = c(2, 15), guide = FALSE)
ggplot(data = d) +   aes(x = 1, y = 1, size = count, label = word, col = count) +   geom_text_repel(segment.size = 0, force = 100, segment.color = 'transparent') +   scale_size(range = c(2, 15), guide = FALSE) +   scale_y_continuous(breaks = NULL) +   scale_x_continuous(breaks = NULL) +   labs(x = '', y = '') +   theme_classic()
ggplot(data = d) +   aes(x = 1, y = 1, size = count, label = word, col = count) +   geom_text_repel(segment.size = 0, force = 100, segment.color = NA) +   scale_size(range = c(2, 15), guide = FALSE) +   scale_y_continuous(breaks = NULL) +   scale_x_continuous(breaks = NULL) +   labs(x = '', y = '') +   theme_classic()
ggplot(data = d) +   aes(x = 1, y = 1, size = count, label = word, col = count) +   geom_text_repel(segment.size = 0, force = 100, segment.color = NA) +   scale_size(range = c(0, 100), guide = FALSE) +   scale_y_continuous(breaks = NULL) +   scale_x_continuous(breaks = NULL) +   labs(x = '', y = '') +   theme_classic()
ggplot(data = d) +   aes(x = 1, y = 1, size = count, label = word, col = count) +   geom_text_repel(segment.size = 0, force = 100, segment.color = NA) +   scale_size(range = c(0, 10), guide = FALSE) +   scale_y_continuous(breaks = NULL) +   scale_x_continuous(breaks = NULL) +   labs(x = '', y = '') +   theme_classic()
ggplot(data = d) +   aes(x = 1, y = 1, size = count, label = word, col = count) +   geom_text_repel(segment.size = 0, force = 100, segment.color = NA) +   scale_size(range = c(0, 50), guide = FALSE) +   scale_y_continuous(breaks = NULL) +   scale_x_continuous(breaks = NULL) +   labs(x = '', y = '') +   theme_classic()
ggplot(data = d) +   aes(x = 1, y = 1, size = count, label = word, col = count) +   geom_text_repel(segment.size = 0, force = 100, segment.color = NA) +   scale_size(range = c(0, 10), guide = FALSE) +   scale_y_continuous(breaks = NULL) +   scale_x_continuous(breaks = NULL) +   labs(x = '', y = '') +   theme_classic()
ggplot(data = d) +   aes(x = 1, y = 1, size = count, label = word, col = count) +   geom_text_repel(segment.size = 0, force = 100, segment.color = NA) +   scale_size(range = c(0, 3), guide = FALSE) +   scale_y_continuous(breaks = NULL) +   scale_x_continuous(breaks = NULL) +   labs(x = '', y = '') +   theme_classic()
ggplot(data = d) +   aes(x = 1, y = 1, size = count, label = word, col = count) +   geom_text_repel(segment.size = 0, force = 100, segment.color = NA) +   scale_size(range = c(1, 10), guide = FALSE) +   scale_y_continuous(breaks = NULL) +   scale_x_continuous(breaks = NULL) +   labs(x = '', y = '') +   theme_classic()
ggplot(data = d) +   aes(x = 1, y = 1, size = count, label = word, col = count) +   geom_text_repel(segment.size = 0, force = 100, segment.color = NA) +   scale_size(range = c(1, 15), guide = FALSE) +   scale_y_continuous(breaks = NULL) +   scale_x_continuous(breaks = NULL) +   labs(x = '', y = '') +   theme_classic()
wordcloud\
wordcloud
install.packages("wordcloud")
head(d)
wordcloud(d$word, d$count, scale = c(5, 0.5), max.words = 100, random.order = FALSE, rot.per = 0.35, use.r.layout = FALSE, colors = brewer.pal(8, 'Dark2'));
library(wordcloud)
wordcloud(d$word, d$count, scale = c(5, 0.5), max.words = 100, random.order = FALSE, rot.per = 0.35, use.r.layout = FALSE, colors = brewer.pal(8, 'Dark2'));
wordcloud(d$word, d$count, scale = c(2, 0.5), max.words = 200, random.order = FALSE, rot.per = 0.6, use.r.layout = FALSE, colors = brewer.pal(8, 'Dark2'))
wordcloud(d$word, d$count, scale = c(3, 0.5), max.words = 200, random.order = FALSE, rot.per = 0.6, use.r.layout = FALSE, colors = brewer.pal(8, 'Dark2'));
wordcloud(d$word, d$count, scale = c(5, 0.5), max.words = 200, random.order = FALSE, rot.per = 0.6, use.r.layout = FALSE, colors = brewer.pal(8, 'Dark2'));
head(d)
wordcloud(d$word, d$count, scale = c(5, 0.5), max.words = 200, random.order = FALSE, rot.per = 0.6, use.r.layout = FALSE, colors = brewer.pal(8, 'Dark2'));
head(d)
head(d, 15)
library(wordcloud) wordcloud(d$word, d$count, scale = c(5, 0.5), max.words = 200, random.order = FALSE, rot.per = 0.6, use.r.layout = FALSE, colors = brewer.pal(8, 'Dark2'));
library(tm) websitesFile <- 'websitesdata.txt'; websiteDataFrame <- read.table(websitesFile, sep = '\t', header = F, fill = TRUE, stringsAsFactors = FALSE); #deleting of not usefull fields.  websiteDataFrame <- head(websiteDataFrame[c(2:4, 6)]) names(websiteDataFrame) <- c('Host', 'Webpage', 'Title', 'Text') #TODO - create a loop textExample <- websiteDataFrame[1, 'Text'] #Create a document for each website websiteDocs <- Corpus(VectorSource(textExample)) #Remove qqpbreakqq? skipWords <- function(x) removeWords(x, c(stopwords("english"), "qqpbreakqq")) library("SnowballC") tmFuncs <- list(tolower, removePunctuation, removeNumbers, stripWhitespace, skipWords, stemDocument) websiteDocs <- tm_map(websiteDocs, FUN = tm_reduce, tmFuns = tmFuncs)
websiteDocs[[1]][1]
m <- as.matrix(dtmWebsiteDocs) v <- sort(rowSums(m), decreasing = TRUE) d <- data.frame(word = names(v), count = v) head(d)
library(tm) websitesFile <- 'websitesdata.txt'; websiteDataFrame <- read.table(websitesFile, sep = '\t', header = F, fill = TRUE, stringsAsFactors = FALSE); #deleting of not usefull fields.  websiteDataFrame <- head(websiteDataFrame[c(2:4, 6)]) names(websiteDataFrame) <- c('Host', 'Webpage', 'Title', 'Text') #TODO - create a loop textExample <- websiteDataFrame[1, 'Text'] #Create a document for each website websiteDocs <- Corpus(VectorSource(textExample)) #Remove qqpbreakqq? skipWords <- function(x) removeWords(x, c(stopwords("english"), "qqpbreakqq")) library("SnowballC") tmFuncs <- list(tolower, removePunctuation, removeNumbers, stripWhitespace, skipWords, stemDocument) websiteDocs <- tm_map(websiteDocs, FUN = tm_reduce, tmFuns = tmFuncs) #Read for testing #websiteDocs[[1]][1] #2. Term documents and word of cloud dtmWebsiteDocs <- TermDocumentMatrix(websiteDocs) m <- as.matrix(dtmWebsiteDocs) v <- sort(rowSums(m), decreasing = TRUE) d <- data.frame(word = names(v), count = v) head(d)
library(tm) websitesFile <- 'websitesdata.txt'; websiteDataFrame <- read.table(websitesFile, sep = '\t', header = F, fill = TRUE, stringsAsFactors = FALSE); #deleting of not usefull fields.  websiteDataFrame <- head(websiteDataFrame[c(2:4, 6)]) names(websiteDataFrame) <- c('Host', 'Webpage', 'Title', 'Text') #TODO - create a loop textExample <- websiteDataFrame[1, 'Text'] #Create a document for each website websiteDocs <- Corpus(VectorSource(textExample)) #Remove qqpbreakqq? skipWords <- function(x) removeWords(x, c(stopwords("english"), "qqpbreakqq")) library("SnowballC") tmFuncs <- list(tolower, removePunctuation, removeNumbers, stripWhitespace, skipWords, stemDocument) websiteDocs <- tm_map(websiteDocs, FUN = tm_reduce, tmFuns = tmFuncs)
dtmWebsiteDocs <- TermDocumentMatrix(websiteDocs) m <- as.matrix(dtmWebsiteDocs) v <- sort(rowSums(m), decreasing = TRUE) d <- data.frame(word = names(v), count = v) head(d)
dtmWebsiteDocs <- TermDocumentMatrix(websiteDocs)
str(websiteDocs)
class(websiteDocs)
websiteDocs$VCorpus
websiteDocs$Corpus
websiteDocs
websiteDocs$VCorpus
websiteDocs[[1]]
dtmWebsiteDocs <- TermDocumentMatrix(websiteDocs)
#TODO - create a loop textExample <- websiteDataFrame[1, 'Text'] #Create a document for each website websiteDocs <- Corpus(VectorSource(textExample)) #Remove qqpbreakqq? skipWords <- function(x) removeWords(x, c(stopwords("english"), "qqpbreakqq")) library("SnowballC") tmFuncs <- list(tolower, removePunctuation, removeNumbers, stripWhitespace, skipWords, stemDocument, PlainTextDocument) websiteDocs <- tm_map(websiteDocs, FUN = tm_reduce, tmFuns = tmFuncs)
dtmWebsiteDocs <- TermDocumentMatrix(websiteDocs)
library(tm) websitesFile <- 'websitesdata.txt'; websiteDataFrame <- read.table(websitesFile, sep = '\t', header = F, fill = TRUE, stringsAsFactors = FALSE); #deleting of not usefull fields.  websiteDataFrame <- head(websiteDataFrame[c(2:4, 6)]) names(websiteDataFrame) <- c('Host', 'Webpage', 'Title', 'Text') #TODO - create a loop textExample <- websiteDataFrame[1, 'Text']
#Remove qqpbreakqq? skipWords <- function(x) removeWords(x, c(stopwords("english"), "qqpbreakqq")) library("SnowballC") tmFuncs <- list(tolower, removePunctuation, removeNumbers, stripWhitespace, skipWords, stemDocument) websiteDocs <- tm_map(websiteDocs, FUN = tm_reduce, tmFuns = tmFuncs)
websiteDocs <- Corpus(VectorSource(textExample))
skipWords <- function(x) removeWords(x, c(stopwords("english"), "qqpbreakqq"))
library("SnowballC") tmFuncs <- list(tolower, removePunctuation, removeNumbers, stripWhitespace, skipWords, stemDocument)
websiteDocs <- tm_map(websiteDocs, FUN = tm_reduce, tmFuns = tmFuncs)
websiteDocs[[1]][1]
dtmWebsiteDocs <- TermDocumentMatrix(websiteDocs)
websiteDocs <- tm_map(websiteDocs, PlainTextDocument)
dtmWebsiteDocs <- TermDocumentMatrix(websiteDocs)
head(websiteDataFrame)
library(dplyr)
head(websiteDocs)
head(websiteDocs)
dtmWebsiteDocs
websiteDataFrame %>% distinct(Host, .keep_all = TRUE)
library(dplyr)
package.load('dplyr')
install.packages('dplyr')
websiteDataFrame %>% distinct(Host, .keep_all = TRUE)
websiteDataFrame%>%distinct(Host, .keep_all = TRUE)
websiteDataFrame%>%distinct(Host, .keep_all = TRUE)
library(dplyr)
websiteDataFrame %>% distinct(Host, .keep_all = TRUE)
websiteDataFrame %>% distinct(Host, .keep_all = TRUE)
websiteDataFrame
head(websiteDataFrame,15)
websiteDataFrame[1:40,]
websiteDataFrame[,1:40]
websiteDocs <- Corpus(VectorSource(textExample))
websiteDataFrame[,1:40]
websiteDataFrame[,1:40]
websiteDataFrame[,1:40]
websiteDataFrame[1:40,]
dim(websiteDocs)
dim(websiteDataFrame)
websiteDataFrame <- read.table(websitesFile, sep = '\t', header = F, fill = TRUE, stringsAsFactors = FALSE);
dim(websiteDataFrame)
#deleting of not usefull fields.  websiteDataFrame <- head(websiteDataFrame[c(2:4, 6)]) names(websiteDataFrame) <- c('Host', 'Webpage', 'Title', 'Text') library(dplyr)
dim(websiteDataFrame)
websitesFile <- 'websitesdata.txt'; websiteDataFrame <- read.table(websitesFile, sep = '\t', header = F, fill = TRUE, stringsAsFactors = FALSE);
head(websiteDataFrame)
head(websiteDataFrame[c(2:5, 6),])
head(websiteDataFrame[,c(2:5, 6)])
head(websiteDataFrame[,c(2:4, 6)])
head(websiteDataFrame[, c(2:4, 6)])
websiteDataFrame <- head(websiteDataFrame[, c(2:4, 6)])
names(websiteDataFrame)
dim(websiteDataFrame)
c(2:4, 6)
websitesFile <- 'websitesdata.txt'; websiteDataFrame <- read.table(websitesFile, sep = '\t', header = F, fill = TRUE, stringsAsFactors = FALSE);
websiteDataFrame[, c(2:4, 6)]
q s
head(websiteDataFrame[, c(2:4, 6)],15)
clean
clear
cle
library(tm) websitesFile <- 'websitesdata.txt'; websiteDataFrame <- read.table(websitesFile, sep = '\t', header = F, fill = TRUE, stringsAsFactors = FALSE); #deleting of not usefull fields.  websiteDataFrame <- websiteDataFrame[, c(2:4, 6)])
library(tm) websitesFile <- 'websitesdata.txt'; websiteDataFrame <- read.table(websitesFile, sep = '\t', header = F, fill = TRUE, stringsAsFactors = FALSE); #deleting of not usefull fields.  websiteDataFrame <- websiteDataFrame[, c(2:4, 6)]
names(websiteDataFrame) <- c('Host', 'Webpage', 'Title', 'Text')
library(dplyr)
websiteDataFrame %>% distinct(Host, .keep_all = TRUE)
websitesFile <- 'websitesdata.txt'; websiteDataFrame <- read.table(websitesFile, sep = '\t', header = F, fill = TRUE, stringsAsFactors = FALSE); #deleting of not usefull fields.  websiteDataFrame <- websiteDataFrame[, c(2:4, 6)] names(websiteDataFrame) <- c('Host', 'Webpage', 'Title', 'Text')
library(dplyr) #TODO - create a loop textExample <- websiteDataFrame[1, 'Text'] websiteDataFrame %>% distinct(Host, .keep_all = TRUE)
websiteDataFrame %>% distinct(Host, .keep_all = TRUE)
dim(websiteDataFrame)
dim(websiteDataFrame %>% distinct(Host, .keep_all = TRUE))
#TODO - create a loop textExample <- websiteDataFrame[1, 'Text'] websiteDataFrame <- websiteDataFrame %>% distinct(Host, .keep_all = TRUE)
head(websiteDataFrame)
websiteDataFrame
websiteDataFrame[, 'Text']
textExample <- websiteDataFrame[, 'Text']
websiteDocs <- Corpus(VectorSource(textExample))
library(tm)
websiteDocs <- Corpus(VectorSource(textExample))
websiteDocs[[1]][1]
websiteDocs[[1]][2]
websiteDocs[[2]][1]
websiteDocs[[2]][1]
websiteDocs[[2]][2]
websiteDocs[[2]][1]
websiteDocs[[2]][2]
?corpus_frame
?VectorSource
?Corpus
websiteDocs
str(websiteDocs)
str(websiteDocs)
websiteDocs[[1]]
websiteDocs$heading
websiteDocs$[heading]
websiteDocs$['heading']
websiteDocs[['heading']]
websiteDocs['heading']
websiteDocs
websiteDocs
websiteDocs$Metadata
websiteDocs[[Metadata]]
websiteDocs[['Metadata']]
websiteDocs
websiteDocs$ 1
websiteDocs$`1`
str(websiteDocs)
skipWords <- function(x) removeWords(x, c(stopwords("english"), "qqpbreakqq")) library("SnowballC") tmFuncs <- list(tolower, removePunctuation, removeNumbers, stripWhitespace, skipWords, stemDocument) websiteDocs <- tm_map(websiteDocs, FUN = tm_reduce, tmFuns = tmFuncs) websiteDocs <- tm_map(websiteDocs, PlainTextDocument)
websiteDocs[[1]][1]
dtmWebsiteDocs <- TermDocumentMatrix(websiteDocs)
dtmWebsiteDocs
m <- as.matrix(dtmWebsiteDocs) v <- sort(rowSums(m), decreasing = TRUE) d <- data.frame(word = names(v), count = v)
head(d)
str(d)
m
head(m)
str(dtmWebsiteDocs)
head(dtmWebsiteDocs)
dtmWebsiteDocs$dimnames
dtmWebsiteDocs[['Docs']]
websiteDocs
dtmWebsiteDocs
websiteDocs
dtmWebsiteDocs$ dimnames
head(d)
websitesFile <- 'websitesdata.txt'; websiteDataFrame <- read.table(websitesFile, sep = '\t', header = F, fill = TRUE, stringsAsFactors = FALSE); #deleting of not usefull fields.  websiteDataFrame <- websiteDataFrame[, c(2:4, 6)] names(websiteDataFrame) <- c('Host', 'Webpage', 'Title', 'Text') library(dplyr) #TODO - create a loop websiteDataFrame <- websiteDataFrame %>% distinct(Host, .keep_all = TRUE)
head(websiteDataFrame)
websiteDocs
websiteDocs
websitesFile <- 'websitesdata.txt'; websiteDataFrame <- read.table(websitesFile, sep = '\t', header = F, fill = TRUE, stringsAsFactors = FALSE); #deleting of not usefull fields.  websiteDataFrame <- websiteDataFrame[, c(2:4, 6)] names(websiteDataFrame) <- c('Host', 'Webpage', 'Title', 'Text') library(dplyr) #TODO - create a loop websiteDataFrame <- websiteDataFrame %>% distinct(Host, .keep_all = TRUE)
websiteDataFrame
textExample
textExample <- websiteDataFrame[, 'Text']
textExample
lapply(textExample, performTmFunctions)
performTmFunctions <- function(websiteText) {     #Create a document for each website     websiteDoc <- Corpus(VectorSource(websiteText))     #Remove qqpbreakqq?     skipWords <- function(x) removeWords(x, c(stopwords("english"), "qqpbreakqq"))     tmFuncs <- list(tolower, removePunctuation, removeNumbers, stripWhitespace, skipWords, stemDocument)     websiteDoc <- tm_map(websiteDocs, FUN = tm_reduce, tmFuns = tmFuncs)     websiteDoc <- tm_map(websiteDocs, PlainTextDocument)     #Read for testing     #websiteDoc[[1]][1] }
lapply(textExample, performTmFunctions)
library(tm)
performTmFunctions <- function(websiteText) {     #Create a document for each website     websiteDoc <- Corpus(VectorSource(websiteText))     #Remove qqpbreakqq?     skipWords <- function(x) removeWords(x, c(stopwords("english"), "qqpbreakqq"))     tmFuncs <- list(tolower, removePunctuation, removeNumbers, stripWhitespace, skipWords, stemDocument)     websiteDoc <- tm_map(websiteDocs, FUN = tm_reduce, tmFuns = tmFuncs)     websiteDoc <- tm_map(websiteDocs, PlainTextDocument)     #Read for testing     #websiteDoc[[1]][1] }
lapply(textExample, performTmFunctions)
performTmFunctions <- function(websiteText) {     #Create a document for each website     websiteDoc <- Corpus(VectorSource(websiteText))     #Remove qqpbreakqq?     skipWords <- function(x) removeWords(x, c(stopwords("english"), "qqpbreakqq"))     tmFuncs <- list(tolower, removePunctuation, removeNumbers, stripWhitespace, skipWords, stemDocument)     websiteDoc <- tm_map(websiteDoc, FUN = tm_reduce, tmFuns = tmFuncs)     websiteDoc <- tm_map(websiteDoc, PlainTextDocument)     #Read for testing     #websiteDoc[[1]][1] }
lapply(textExample, performTmFunctions)
performTmFunctions <- function(websiteText) {     #Create a document for each website     websiteDoc <- Corpus(VectorSource(websiteText))     #Remove qqpbreakqq?     skipWords <- function(x) removeWords(x, c(stopwords("english"), "qqpbreakqq"))     tmFuncs <- list(tolower, removePunctuation, removeNumbers, stripWhitespace, skipWords, stemDocument)     websiteDoc <- tm_map(websiteDoc, FUN = tm_reduce, tmFuns = tmFuncs)     websiteDoc <- tm_map(websiteDoc, PlainTextDocument)     #Read for testing     websiteDoc[[1]][1] }
lapply(textExample, performTmFunctions)
#Create a document for each website websiteDoc <- Corpus(VectorSource(websiteText)) #Remove qqpbreakqq? skipWords <- function(x) removeWords(x, c(stopwords("english"), "qqpbreakqq")) tmFuncs <- list(tolower, removePunctuation, removeNumbers, stripWhitespace, skipWords, stemDocument) websiteDoc <- tm_map(websiteDoc, FUN = tm_reduce, tmFuns = tmFuncs) websiteDoc <- tm_map(websiteDoc, PlainTextDocument) #Read for testing # websiteDoc[[1]][1] websiteDoc }
performTmFunctions <- function(websiteText) {     #Create a document for each website     websiteDoc <- Corpus(VectorSource(websiteText))     #Remove qqpbreakqq?     skipWords <- function(x) removeWords(x, c(stopwords("english"), "qqpbreakqq"))     tmFuncs <- list(tolower, removePunctuation, removeNumbers, stripWhitespace, skipWords, stemDocument)     websiteDoc <- tm_map(websiteDoc, FUN = tm_reduce, tmFuns = tmFuncs)     websiteDoc <- tm_map(websiteDoc, PlainTextDocument)     #Read for testing     # websiteDoc[[1]][1]     websiteDoc }
lapply(textExample, performTmFunctions)
websiteDocs <- lapply(textExample, performTmFunctions)
websiteDocs
#Create a document for each website websiteDoc <- Corpus(VectorSource(websiteText)) #Remove qqpbreakqq? skipWords <- function(x) removeWords(x, c(stopwords("english"), "qqpbreakqq")) tmFuncs <- list(tolower, removePunctuation, removeNumbers, stripWhitespace, skipWords, stemDocument) websiteDoc <- tm_map(websiteDoc, FUN = tm_reduce, tmFuns = tmFuncs) websiteDoc <- tm_map(websiteDoc, PlainTextDocument) #Read for testing # websiteDoc[[1]][1] #2. Term documents and word of cloud dtmWebsiteDocs <- TermDocumentMatrix(websiteDocs) m <- as.matrix(dtmWebsiteDocs) v <- sort(rowSums(m), decreasing = TRUE) d <- data.frame(word = names(v), count = v) head(d) }
performTmFunctions <- function(websiteText) {     #Create a document for each website     websiteDoc <- Corpus(VectorSource(websiteText))     #Remove qqpbreakqq?     skipWords <- function(x) removeWords(x, c(stopwords("english"), "qqpbreakqq"))     tmFuncs <- list(tolower, removePunctuation, removeNumbers, stripWhitespace, skipWords, stemDocument)     websiteDoc <- tm_map(websiteDoc, FUN = tm_reduce, tmFuns = tmFuncs)     websiteDoc <- tm_map(websiteDoc, PlainTextDocument)     #Read for testing     # websiteDoc[[1]][1]     #2. Term documents and word of cloud     dtmWebsiteDocs <- TermDocumentMatrix(websiteDocs)     m <- as.matrix(dtmWebsiteDocs)     v <- sort(rowSums(m), decreasing = TRUE)     d <- data.frame(word = names(v), count = v)     head(d) }
websiteDocs <- lapply(textExample, performTmFunctions)
performTmFunctions <- function(websiteText) {     #Create a document for each website     websiteDoc <- Corpus(VectorSource(websiteText))     #Remove qqpbreakqq?     skipWords <- function(x) removeWords(x, c(stopwords("english"), "qqpbreakqq"))     tmFuncs <- list(tolower, removePunctuation, removeNumbers, stripWhitespace, skipWords, stemDocument)     websiteDoc <- tm_map(websiteDoc, FUN = tm_reduce, tmFuns = tmFuncs)     websiteDoc <- tm_map(websiteDoc, PlainTextDocument)     #Read for testing     # websiteDoc[[1]][1]     #2. Term documents and word of cloud     dtmWebsiteDoc <- TermDocumentMatrix(websiteDoc)     m <- as.matrix(dtmWebsiteDoc)     v <- sort(rowSums(m), decreasing = TRUE)     d <- data.frame(word = names(v), count = v)     head(d) }
websiteDocs <- lapply(textExample, performTmFunctions)
websiteDocs
performTmFunctions <- function(websiteText) {     #Create a document for each website     websiteDoc <- Corpus(VectorSource(websiteText))     #Remove qqpbreakqq?     skipWords <- function(x) removeWords(x, c(stopwords("english"), "qqpbreakqq"))     tmFuncs <- list(tolower, removePunctuation, removeNumbers, stripWhitespace, skipWords, stemDocument)     websiteDoc <- tm_map(websiteDoc, FUN = tm_reduce, tmFuns = tmFuncs)     websiteDoc <- tm_map(websiteDoc, PlainTextDocument)     #Read for testing     # websiteDoc[[1]][1]     #2. Term documents and word of cloud     dtmWebsiteDoc <- TermDocumentMatrix(websiteDoc)     m <- as.matrix(dtmWebsiteDoc)     v <- sort(rowSums(m), decreasing = TRUE)     d <- data.frame(word = names(v), count = v)     d }
websiteDocs <- lapply(textExample, performTmFunctions)
websiteDocs
performTmFunctions <- function(websiteText) {     #Create a document for each website     websiteDoc <- Corpus(VectorSource(websiteText))     #Remove qqpbreakqq?     skipWords <- function(x) removeWords(x, c(stopwords("english"), "qqpbreakqq"))     tmFuncs <- list(tolower, removePunctuation, removeNumbers, stripWhitespace, skipWords, stemDocument)     websiteDoc <- tm_map(websiteDoc, FUN = tm_reduce, tmFuns = tmFuncs)     websiteDoc <- tm_map(websiteDoc, PlainTextDocument)     #Read for testing     # websiteDoc[[1]][1]     #2. Term documents and word of cloud     dtmWebsiteDoc <- TermDocumentMatrix(websiteDoc)     m <- as.matrix(dtmWebsiteDoc)     v <- sort(rowSums(m), decreasing = TRUE)     d <- data.frame(word = names(v), count = v)     head(d, 50) }
lapply(textExample, performTmFunctions)
performTmFunctions <- function(websiteText) {     #Create a document for each website     websiteDoc <- Corpus(VectorSource(websiteText))     #Remove qqpbreakqq?     skipWords <- function(x) removeWords(x, c(stopwords("english"), "qqpbreakqq"))     tmFuncs <- list(tolower, removePunctuation, removeNumbers, stripWhitespace, skipWords, stemDocument)     websiteDoc <- tm_map(websiteDoc, FUN = tm_reduce, tmFuns = tmFuncs)     websiteDoc <- tm_map(websiteDoc, PlainTextDocument)     #Read for testing     # websiteDoc[[1]][1]     #2. Term documents and word of cloud     dtmWebsiteDoc <- TermDocumentMatrix(websiteDoc)     dtmWebsiteDoc     m <- as.matrix(dtmWebsiteDoc)     v <- sort(rowSums(m), decreasing = TRUE)     d <- data.frame(word = names(v), count = v)     head(d, 50) }
lapply(textExample, performTmFunctions)
performTmFunctions <- function(websiteText) {     #Create a document for each website     websiteDoc <- Corpus(VectorSource(websiteText))     #Remove qqpbreakqq?     skipWords <- function(x) removeWords(x, c(stopwords("english"), "qqpbreakqq"))     tmFuncs <- list(tolower, removePunctuation, removeNumbers, stripWhitespace, skipWords, stemDocument)     websiteDoc <- tm_map(websiteDoc, FUN = tm_reduce, tmFuns = tmFuncs)     websiteDoc <- tm_map(websiteDoc, PlainTextDocument)     #Read for testing     # websiteDoc[[1]][1]     #2. Term documents and word of cloud     dtmWebsiteDoc <- TermDocumentMatrix(websiteDoc)     dtmWebsiteDoc     m <- as.matrix(dtmWebsiteDoc)     v <- sort(rowSums(m), decreasing = TRUE)     d <- data.frame(word = names(v), count = v)     #  head(d,50) }
lapply(textExample, performTmFunctions)
performTmFunctions <- function(websiteText) {     #Create a document for each website     websiteDoc <- Corpus(VectorSource(websiteText))     #Remove qqpbreakqq?     skipWords <- function(x) removeWords(x, c(stopwords("english"), "qqpbreakqq"))     tmFuncs <- list(tolower, removePunctuation, removeNumbers, stripWhitespace, skipWords, stemDocument)     websiteDoc <- tm_map(websiteDoc, FUN = tm_reduce, tmFuns = tmFuncs)     websiteDoc <- tm_map(websiteDoc, PlainTextDocument)     #Read for testing     # websiteDoc[[1]][1]     #2. Term documents and word of cloud     dtmWebsiteDoc <- TermDocumentMatrix(websiteDoc)     m <- as.matrix(dtmWebsiteDoc)     v <- sort(rowSums(m), decreasing = TRUE)     d <- data.frame(word = names(v), count = v)     #  head(d,50) }
lapply(textExample, performTmFunctions)
performTmFunctions <- function(websiteText) {     #Create a document for each website     websiteDoc <- Corpus(VectorSource(websiteText))     #Remove qqpbreakqq?     skipWords <- function(x) removeWords(x, c(stopwords("english"), "qqpbreakqq"))     tmFuncs <- list(tolower, removePunctuation, removeNumbers, stripWhitespace, skipWords, stemDocument)     websiteDoc <- tm_map(websiteDoc, FUN = tm_reduce, tmFuns = tmFuncs)     websiteDoc <- tm_map(websiteDoc, PlainTextDocument)     #Read for testing     # websiteDoc[[1]][1]     #2. Term documents and word of cloud     dtmWebsiteDoc <- TermDocumentMatrix(websiteDoc)     m <- as.matrix(dtmWebsiteDoc)     v <- sort(rowSums(m), decreasing = TRUE)     d <- data.frame(word = names(v), count = v)     head(d, 50) }
lapply(textExample, performTmFunctions)
wordFrequenciesWebsites <- lapply(textExample, performTmFunctions)
wordFrequenciesWebsites
wordFrequenciesWebsites[[1]]
wordFrequenciesWebsites[[2]]
d <- wordFrequenciesWebsites[[2]]
library(wordcloud) wordcloud(d$word, d$count, scale = c(5, 0.5), max.words = 200, random.order = FALSE, rot.per = 0.6, use.r.layout = FALSE, colors = brewer.pal(8, 'Dark2'));
?svm
??svm
sapply(textExample, performTmFunctions)
str(sapply(textExample, performTmFunctions))
lapply(textExample, performTmFunctions)
performTmFunctions <- function(websiteText) {     #Create a document for each website     websiteDoc <- Corpus(VectorSource(websiteText))     #Remove qqpbreakqq?     skipWords <- function(x) removeWords(x, c(stopwords("english"), "qqpbreakqq"))     tmFuncs <- list(tolower, removePunctuation, removeNumbers, stripWhitespace, skipWords, stemDocument)     websiteDoc <- tm_map(websiteDoc, FUN = tm_reduce, tmFuns = tmFuncs)     websiteDoc <- tm_map(websiteDoc, PlainTextDocument)     #Read for testing     # websiteDoc[[1]][1]     #2. Term documents and word of cloud     #dtmWebsiteDoc <- TermDocumentMatrix(websiteDoc)     #m <- as.matrix(dtmWebsiteDoc)     #v <- sort(rowSums(m), decreasing = TRUE)     #d <- data.frame(word = names(v), count = v)     #head(d,50) }
sapply(textExample, performTmFunctions)
lapply(textExample, performTmFunctions)
lapply(textExample, performTmFunctions)
websitesFile <- 'websitesdata.txt'; websiteDataFrame <- read.table(websitesFile, sep = '\t', header = F, fill = TRUE, stringsAsFactors = FALSE);
websiteDataFrame <- websiteDataFrame[, c(2:4, 6)] names(websiteDataFrame) <- c('Host', 'Webpage', 'Title', 'Text')
websiteDataFrame <- websiteDataFrame %>% distinct(Host, .keep_all = TRUE)
performTmFunctions(websitesText)
websitesText <- websiteDataFrame[, 'Text']
performTmFunctions(websitesText)
performTmFunctions <- function(websiteText) {     #Create a document for each website     websiteDoc <- Corpus(VectorSource(websiteText))     #Remove qqpbreakqq?     skipWords <- function(x) removeWords(x, c(stopwords("english"), "qqpbreakqq"))     tmFuncs <- list(tolower, removePunctuation, removeNumbers, stripWhitespace, skipWords, stemDocument)     websiteDoc <- tm_map(websiteDoc, FUN = tm_reduce, tmFuns = tmFuncs)     websiteDoc <- tm_map(websiteDoc, PlainTextDocument)     #Read for testing     # websiteDoc[[1]][1]     #2. Term documents and word of cloud     dtmWebsiteDoc <- TermDocumentMatrix(websiteDoc)     dtmWebsiteDoc     #m <- as.matrix(dtmWebsiteDoc)     #v <- sort(rowSums(m), decreasing = TRUE)     #d <- data.frame(word = names(v), count = v)     #head(d,50) }
performTmFunctions(websitesText)
library(e1071)
install.packages('e1071')
library(e1071)
?SVM
??SVM
?svm
as.matrix(performTmFunctions(websitesText))
head(as.matrix(performTmFunctions))
head(as.matrix(performTmFunctions(websitesText)))
head(websiteDataFrame)
performTmFunctions <- function(websiteText) {     #Create a document for each website     #websiteDoc <- Corpus(VectorSource(websiteText))     websiteDoc <- Corpus(DataframeSource(websiteText))     #Remove qqpbreakqq?     skipWords <- function(x) removeWords(x, c(stopwords("english"), "qqpbreakqq"))     tmFuncs <- list(tolower, removePunctuation, removeNumbers, stripWhitespace, skipWords, stemDocument)     websiteDoc <- tm_map(websiteDoc, FUN = tm_reduce, tmFuns = tmFuncs)     websiteDoc <- tm_map(websiteDoc, PlainTextDocument)     #Read for testing     # websiteDoc[[1]][1]     #2. Term documents and word of cloud     dtmWebsiteDoc <- TermDocumentMatrix(websiteDoc)     dtmWebsiteDoc     #m <- as.matrix(dtmWebsiteDoc)     #v <- sort(rowSums(m), decreasing = TRUE)     #d <- data.frame(word = names(v), count = v)     #head(d,50) }
performTmFunctions(websiteDataFrame)
TDWebsitesData <- performTmFunctions(websiteDataFrame)
TDWebsitesData
TDWebsitesData[[1]]
str(TDWebsitesData)
performTmFunctions <- function(websiteText) {     #Create a document for each website     #websiteDoc <- Corpus(VectorSource(websiteText))     websiteDoc <- Corpus(DataframeSource(websiteText))     websiteDoc     #Remove qqpbreakqq?     skipWords <- function(x) removeWords(x, c(stopwords("english"), "qqpbreakqq"))     tmFuncs <- list(tolower, removePunctuation, removeNumbers, stripWhitespace, skipWords, stemDocument)     websiteDoc <- tm_map(websiteDoc, FUN = tm_reduce, tmFuns = tmFuncs)     websiteDoc <- tm_map(websiteDoc, PlainTextDocument)     #Read for testing     # websiteDoc[[1]][1]     #2. Term documents and word of cloud     dtmWebsiteDoc <- TermDocumentMatrix(websiteDoc)     #m <- as.matrix(dtmWebsiteDoc)     #v <- sort(rowSums(m), decreasing = TRUE)     #d <- data.frame(word = names(v), count = v)     #head(d,50) }
performTmFunctions(websiteDataFrame)
performTmFunctions(websiteDataFrame)
performTmFunctions <- function(websiteText) {     #Create a document for each website     #websiteDoc <- Corpus(VectorSource(websiteText))     websiteDoc <- Corpus(DataframeSource(websiteText))     #Remove qqpbreakqq?     skipWords <- function(x) removeWords(x, c(stopwords("english"), "qqpbreakqq"))     tmFuncs <- list(tolower, removePunctuation, removeNumbers, stripWhitespace, skipWords, stemDocument)     websiteDoc <- tm_map(websiteDoc, FUN = tm_reduce, tmFuns = tmFuncs)     websiteDoc <- tm_map(websiteDoc, PlainTextDocument)     #Read for testing     # websiteDoc[[1]][1]     #2. Term documents and word of cloud     dtmWebsiteDoc <- TermDocumentMatrix(websiteDoc)     #m <- as.matrix(dtmWebsiteDoc)     #v <- sort(rowSums(m), decreasing = TRUE)     #d <- data.frame(word = names(v), count = v)     #head(d,50) }
performTmFunctions(websiteDataFrame)
TDWebsitesData <- performTmFunctions(websiteDataFrame)
TDWebsitesData
websiteDataFrame
library(tm) library(dplyr) library("SnowballC") library(e1071) #1.Read from file and preprocessing. 1 and 2 is inspired by https://www.springboard.com/blog/text-mining-in-r/ websitesFile <- 'websitesdata.txt'; websiteDataFrame <- read.table(websitesFile, sep = '\t', header = F, fill = TRUE, stringsAsFactors = FALSE);
#deleting of not usefull fields.  websiteDataFrame <- websiteDataFrame[, c(2:4, 6)] names(websiteDataFrame) <- c('Host', 'Webpage', 'Title', 'text')
head(websiteDataFrame)
names(websiteDataFrame) <- c('Host', 'Webpage', 'Title', 'text') #Take only first page from each website websiteDataFrame <- websiteDataFrame %>% distinct(Host, .keep_all = TRUE) %>% mutate(doc_id = Host)
head(websiteDataFrame)
TDWebsitesData <- performTmFunctions(websiteDataFrame)
performTmFunctions <- function(websiteText) {     #Create a document for each website     #websiteDoc <- Corpus(VectorSource(websiteText))     websiteDoc <- Corpus(DataframeSource(websiteText))     #Remove qqpbreakqq?     skipWords <- function(x) removeWords(x, c(stopwords("english"), "qqpbreakqq"))     tmFuncs <- list(tolower, removePunctuation, removeNumbers, stripWhitespace, skipWords, stemDocument)     websiteDoc <- tm_map(websiteDoc, FUN = tm_reduce, tmFuns = tmFuncs)     websiteDoc <- tm_map(websiteDoc, PlainTextDocument)     #Read for testing     # websiteDoc[[1]][1]     #2. Term documents and word of cloud     dtmWebsiteDoc <- TermDocumentMatrix(websiteDoc)     #m <- as.matrix(dtmWebsiteDoc)     #v <- sort(rowSums(m), decreasing = TRUE)     #d <- data.frame(word = names(v), count = v)     #head(d,50) }
websiteDoc <- Corpus(DataframeSource(websiteDataFrame))
websiteDoc
websiteDoc$`1`
head(websiteDataFrame)
websiteDataFrame <- websiteDataFrame %>% distinct(Host, .keep_all = TRUE) %>% mutate(doc_id = Host) %>% select(doc_id, text)
websiteDoc <- Corpus(DataframeSource(websiteDataFrame))
websiteDoc
websiteDoc$`1`
websiteDoc[[1]]
websiteDataFrame
TDWebsitesData <- performTmFunctions(websiteDataFrame)
TDWebsitesData
TDWebsitesData[[1]]
as.matrix(dtmWebsiteDoc)
as.matrix(TDWebsitesData)
head(as.matrix(TDWebsitesData))
head(websiteDataFrame)
inspect(websiteDataFrame)
inspect(TDWebsitesData)
head(websiteDataFrame)
names(websiteDataFrame)
websiteDataFrame["doc_id"]
str(websiteDataFrame["doc_id"])
meta(TDWebsitesData)
websiteDataFrame <- websiteDataFrame %>% distinct(Host, .keep_all = TRUE) %>% mutate(doc_id = Host) %>% select(doc_id, text) #websitesText <- websiteDataFrame[, 'Text'] TDWebsitesData <- performTmFunctions(websiteDataFrame)
TDWebsitesData <- performTmFunctions(websiteDataFrame)
meta(TDWebsitesData)
websiteDoc <- Corpus(DataframeSource(websiteDataFrame))
meta(websiteDoc)
websiteDoc
websiteDoc[[1]]
websiteDoc[[2]]
websiteDoc
inspect(websiteDoc)
inspect(websiteDoc)[[8]]
inspect(websiteDoc)[[8]]
websiteDoc[[8]]
websiteDoc[[8]]["Content"]
websiteDoc[[8]]
websiteDoc[[8]]$Content
str(websiteDataFrame)
head(websiteDataFrame)
performTmFunctions(websiteDataFrame)
Corpus(DataframeSource(websiteDataFrame))
meta(Corpus(DataframeSource(websiteDataFrame)))
meta(Corpus(DataframeSource(websiteDataFrame)))
inspect(Corpus(DataframeSource(websiteDataFrame)))
inspect(Corpus(DataframeSource(websiteDataFrame)))
websiteDoc <- Corpus(DataframeSource(websiteDataFrame))
inspect(websiteDoc)
websiteDoc$`1`
websiteDoc$`2`
websiteDoc <- Corpus(DataframeSource(websiteDataFrame))
str(websiteDoc)
websitesFile <- 'websitesdata.txt'; websiteDataFrame <- read.table(websitesFile, sep = '\t', header = F, fill = TRUE, stringsAsFactors = FALSE); #deleting of not usefull fields.  websiteDataFrame <- websiteDataFrame[, c(2:4, 6)] names(websiteDataFrame) <- c('Host', 'Webpage', 'Title', 'text') #Take only first page from each website websiteDataFrame <- websiteDataFrame %>% distinct(Host, .keep_all = TRUE) %>% mutate(doc_id = Host, heading = "dfsf") %>% select(doc_id, text)
websiteDoc <- Corpus(DataframeSource(websiteDataFrame))
str(websiteDoc)
docs <- data.frame(doc_id = c("doc_1", "doc_2"), text = c("This is a text.", "This another one."), dmeta1 = 1:2, dmeta2 = letters[1:2], stringsAsFactors = FALSE) (ds <- DataframeSource(docs)) x <- Corpus(ds) inspect(x) meta(x)
names(websiteDataFrame)
websiteDoc <- Corpus(DataframeSource(websiteDataFrame))
websiteDoc
websiteDataFrame <- websiteDataFrame %>% distinct(Host, .keep_all = TRUE) %>% mutate(doc_id = Host, heading = "dfsf") %>% select(doc_id, text, heading)
#1.Read from file and preprocessing. 1 and 2 is inspired by https://www.springboard.com/blog/text-mining-in-r/ websitesFile <- 'websitesdata.txt'; websiteDataFrame <- read.table(websitesFile, sep = '\t', header = F, fill = TRUE, stringsAsFactors = FALSE); #deleting of not usefull fields.  websiteDataFrame <- websiteDataFrame[, c(2:4, 6)] names(websiteDataFrame) <- c('Host', 'Webpage', 'Title', 'text') #Take only first page from each website websiteDataFrame <- websiteDataFrame %>% distinct(Host, .keep_all = TRUE) %>% mutate(doc_id = Host, heading = "dfsf") %>% select(doc_id, text, heading) #websitesText <- websiteDataFrame[, 'Text']
head(websiteDataFrame)
websiteDoc <- Corpus(DataframeSource(websiteDataFrame))
websiteDoc
websiteDoc$`3`
meta(websiteDoc)
inspect(websiteDoc)
inspect(websiteDoc)[[1]]
inspect(websiteDoc)[[1]][1]
inspect(websiteDoc)[[1]][1]
docs <- data.frame(doc_id = c("doc_1", "doc_2"), text = c("This is a text.", "This another one."), dmeta1 = 1:2, dmeta2 = letters[1:2], stringsAsFactors = FALSE)
head(docs)
(ds <- DataframeSource(docs))
library(dplyr)
library(tm) library(dplyr) library("SnowballC")
(ds <- DataframeSource(docs))
x <- Corpus(ds)
inspect(x)
meta(x)
meta(x[[1]])
meta(x[[1]])
docs <- data.frame(doc_id = c("doc_1", "doc_2"), text = c("This is a text.", "This another one."), id = 2:3, dmeta2 = letters[1:2], stringsAsFactors = FALSE) (ds <- DataframeSource(docs)) x <- Corpus(ds) meta(x[[1]])
TDWebsitesData <- performTmFunctions(websiteDataFrame)
websiteDataFrame %>% distinct(Host, .keep_all = TRUE) %>% mutate(doc_id = Host, heading = "dfsf") %>% select(doc_id, text, heading)
library(tm) library(dplyr) library("SnowballC") library(e1071) #1.Read from file and preprocessing. 1 and 2 is inspired by https://www.springboard.com/blog/text-mining-in-r/ websitesFile <- 'websitesdata.txt'; websiteDataFrame <- read.table(websitesFile, sep = '\t', header = F, fill = TRUE, stringsAsFactors = FALSE); #deleting of not usefull fields.  websiteDataFrame <- websiteDataFrame[, c(2:4, 6)] names(websiteDataFrame) <- c('Host', 'Webpage', 'Title', 'text') #Take only first page from each website websiteDataFrame <- websiteDataFrame %>% distinct(Host, .keep_all = TRUE) %>% mutate(doc_id = Host, heading = "dfsf") %>% select(doc_id, text, heading) #websitesText <- websiteDataFrame[, 'Text'] TDWebsitesData <- performTmFunctions(websiteDataFrame)
performTmFunctions <- function(websiteText) {     #Create a document for each website     #websiteDoc <- Corpus(VectorSource(websiteText))     websiteDoc <- Corpus(DataframeSource(websiteText))     #Remove qqpbreakqq?     skipWords <- function(x) removeWords(x, c(stopwords("english"), "qqpbreakqq"))     tmFuncs <- list(tolower, removePunctuation, removeNumbers, stripWhitespace, skipWords, stemDocument)     websiteDoc <- tm_map(websiteDoc, FUN = tm_reduce, tmFuns = tmFuncs)     websiteDoc <- tm_map(websiteDoc, PlainTextDocument)     #Read for testing     # websiteDoc[[1]][1]     #2. Term documents and word of cloud     dtmWebsiteDoc <- TermDocumentMatrix(websiteDoc)     #m <- as.matrix(dtmWebsiteDoc)     #v <- sort(rowSums(m), decreasing = TRUE)     #d <- data.frame(word = names(v), count = v)     #head(d,50) }
library(tm) library(dplyr) library("SnowballC") library(e1071) #1.Read from file and preprocessing. 1 and 2 is inspired by https://www.springboard.com/blog/text-mining-in-r/ websitesFile <- 'websitesdata.txt'; websiteDataFrame <- read.table(websitesFile, sep = '\t', header = F, fill = TRUE, stringsAsFactors = FALSE); #deleting of not usefull fields.  websiteDataFrame <- websiteDataFrame[, c(2:4, 6)] names(websiteDataFrame) <- c('Host', 'Webpage', 'Title', 'text') #Take only first page from each website websiteDataFrame <- websiteDataFrame %>% distinct(Host, .keep_all = TRUE) %>% mutate(doc_id = Host, heading = "dfsf") %>% select(doc_id, text, heading) #websitesText <- websiteDataFrame[, 'Text'] TDWebsitesData <- performTmFunctions(websiteDataFrame)
TDWebsitesData
as.matrix(TDWebsitesData)
as.matrix(head(TDWebsitesData))
m <- as.matrix(TDWebsitesData)
head(m)
performTmFunctions <- function(websiteText) {     #Create a document for each website     #websiteDoc <- Corpus(VectorSource(websiteText))     myReader <- readTabular(mapping = list(content = "Text", id = "Heading"))     websiteDoc <- Corpus(DataframeSource(websiteText), readerControl = list(reader = myReader))     #Remove qqpbreakqq?     skipWords <- function(x) removeWords(x, c(stopwords("english"), "qqpbreakqq"))     tmFuncs <- list(tolower, removePunctuation, removeNumbers, stripWhitespace, skipWords, stemDocument)     websiteDoc <- tm_map(websiteDoc, FUN = tm_reduce, tmFuns = tmFuncs)     websiteDoc <- tm_map(websiteDoc, PlainTextDocument)     #Read for testing     # websiteDoc[[1]][1]     #2. Term documents and word of cloud     dtmWebsiteDoc <- TermDocumentMatrix(websiteDoc)     #m <- as.matrix(dtmWebsiteDoc)     #v <- sort(rowSums(m), decreasing = TRUE)     #d <- data.frame(word = names(v), count = v)     #head(d,50) }
library(tm) library(dplyr) library("SnowballC") library(e1071) #1.Read from file and preprocessing. 1 and 2 is inspired by https://www.springboard.com/blog/text-mining-in-r/ websitesFile <- 'websitesdata.txt'; websiteDataFrame <- read.table(websitesFile, sep = '\t', header = F, fill = TRUE, stringsAsFactors = FALSE); #deleting of not usefull fields.  websiteDataFrame <- websiteDataFrame[, c(2:4, 6)] names(websiteDataFrame) <- c('Host', 'Webpage', 'Title', 'Text') #Take only first page from each website websiteDataFrame <- websiteDataFrame %>% distinct(Host, .keep_all = TRUE) %>% mutate(doc_id = Host) %>% select(Host, Text, Heading)
head(websiteDataFrame)
TDWebsitesData <- performTmFunctions(websiteDataFrame)
websiteDataFrame <- websiteDataFrame %>% distinct(Host, .keep_all = TRUE) %>% mutate(doc_id = Host) %>% select(Host, Text, Title)
performTmFunctions <- function(websiteText) {     #Create a document for each website     #websiteDoc <- Corpus(VectorSource(websiteText))     myReader <- readTabular(mapping = list(content = "Text", id = "Title"))     websiteDoc <- Corpus(DataframeSource(websiteText), readerControl = list(reader = myReader))     #Remove qqpbreakqq?     skipWords <- function(x) removeWords(x, c(stopwords("english"), "qqpbreakqq"))     tmFuncs <- list(tolower, removePunctuation, removeNumbers, stripWhitespace, skipWords, stemDocument)     websiteDoc <- tm_map(websiteDoc, FUN = tm_reduce, tmFuns = tmFuncs)     websiteDoc <- tm_map(websiteDoc, PlainTextDocument)     #Read for testing     # websiteDoc[[1]][1]     #2. Term documents and word of cloud     dtmWebsiteDoc <- TermDocumentMatrix(websiteDoc)     #m <- as.matrix(dtmWebsiteDoc)     #v <- sort(rowSums(m), decreasing = TRUE)     #d <- data.frame(word = names(v), count = v)     #head(d,50) }
TDWebsitesData <- performTmFunctions(websiteDataFrame)
m <- as.matrix(TDWebsitesData)
head(m)
TDWebsitesData
TDWebsitesData[[1]]
TDWebsitesData[[2]]
TDWebsitesData[[3]]
TDWebsitesData[[1]][1]
TDWebsitesData[[1]][2]
TDWebsitesData[[]][2]
TDWebsitesData[[]][1]
TDWebsitesData$dimnames
websiteDataFrame %>% distinct(Host, .keep_all = TRUE) %>% mutate(doc_id = Host) %>% select(Host, Text, Title)
websiteDataFrame <- websiteDataFrame %>% distinct(Host, .keep_all = TRUE) %>% mutate(doc_id = Host) %>% select(Host, Text, Title)
head(websiteDataFrame)
head(websiteDataFrame)
performTmFunctions <- function(websiteText) {     #Create a document for each website     #websiteDoc <- Corpus(VectorSource(websiteText))     myReader <- readTabular(mapping = list(content = "Text", id = "Title"))     websiteDoc <- VCorpus(DataframeSource(websiteText), readerControl = list(reader = myReader))     #Remove qqpbreakqq?     skipWords <- function(x) removeWords(x, c(stopwords("english"), "qqpbreakqq"))     tmFuncs <- list(tolower, removePunctuation, removeNumbers, stripWhitespace, skipWords, stemDocument)     websiteDoc <- tm_map(websiteDoc, FUN = tm_reduce, tmFuns = tmFuncs)     websiteDoc <- tm_map(websiteDoc, PlainTextDocument)     #Read for testing     # websiteDoc[[1]][1]     #2. Term documents and word of cloud     dtmWebsiteDoc <- TermDocumentMatrix(websiteDoc)     #m <- as.matrix(dtmWebsiteDoc)     #v <- sort(rowSums(m), decreasing = TRUE)     #d <- data.frame(word = names(v), count = v)     #head(d,50) }
library(tm) library(dplyr) library("SnowballC") library(e1071) #1.Read from file and preprocessing. 1 and 2 is inspired by https://www.springboard.com/blog/text-mining-in-r/ websitesFile <- 'websitesdata.txt'; websiteDataFrame <- read.table(websitesFile, sep = '\t', header = F, fill = TRUE, stringsAsFactors = FALSE); #deleting of not usefull fields.  websiteDataFrame <- websiteDataFrame[, c(2:4, 6)] names(websiteDataFrame) <- c('Host', 'Webpage', 'Title', 'Text') #Take only first page from each website websiteDataFrame <- websiteDataFrame %>% distinct(Host, .keep_all = TRUE) %>% mutate(doc_id = Host) %>% select(Host, Text, Title) #websitesText <- websiteDataFrame[, 'Text']
TDWebsitesData <- performTmFunctions(websiteDataFrame)
m <- as.matrix(TDWebsitesData)
m
head(m)
library(tm) library(dplyr) library("SnowballC") library(e1071) #1.Read from file and preprocessing. 1 and 2 is inspired by https://www.springboard.com/blog/text-mining-in-r/ websitesFile <- 'websitesdata.txt'; websiteDataFrame <- read.table(websitesFile, sep = '\t', header = F, fill = TRUE, stringsAsFactors = FALSE); #deleting of not usefull fields.  websiteDataFrame <- websiteDataFrame[, c(2:4, 6)] names(websiteDataFrame) <- c('Host', 'Webpage', 'Title', 'Text') #Take only first page from each website websiteDataFrame <- websiteDataFrame %>% distinct(Host, .keep_all = TRUE) %>% mutate(doc_id = Host) %>% select(Host, Text, Title)
websiteDataFrame
websiteDataFrame$Id = 1:8
websiteDataFrame
performTmFunctions <- function(websiteText) {     #Create a document for each website     #websiteDoc <- Corpus(VectorSource(websiteText))     myReader <- readTabular(mapping = list(content = "Text", id = "Id"))     websiteDoc <- VCorpus(DataframeSource(websiteText), readerControl = list(reader = myReader))     #Remove qqpbreakqq?     skipWords <- function(x) removeWords(x, c(stopwords("english"), "qqpbreakqq"))     tmFuncs <- list(tolower, removePunctuation, removeNumbers, stripWhitespace, skipWords, stemDocument)     websiteDoc <- tm_map(websiteDoc, FUN = tm_reduce, tmFuns = tmFuncs)     websiteDoc <- tm_map(websiteDoc, PlainTextDocument)     #Read for testing     # websiteDoc[[1]][1]     #2. Term documents and word of cloud     dtmWebsiteDoc <- TermDocumentMatrix(websiteDoc)     #m <- as.matrix(dtmWebsiteDoc)     #v <- sort(rowSums(m), decreasing = TRUE)     #d <- data.frame(word = names(v), count = v)     #head(d,50) }
TDWebsitesData <- performTmFunctions(websiteDataFrame)
m <- as.matrix(TDWebsitesData)
head(m)
performTmFunctions <- function(websiteText) {     #Create a document for each website     #websiteDoc <- Corpus(VectorSource(websiteText))     myReader <- readTabular(mapping = list(content = "Text", id = "Id"))     websiteDoc <- VCorpus(DataframeSource(websiteText), readerControl = list(reader = myReader))     #Remove qqpbreakqq?     #skipWords <- function(x) removeWords(x, c(stopwords("english"), "qqpbreakqq"))     #tmFuncs <- list(tolower, removePunctuation, removeNumbers, stripWhitespace, skipWords, stemDocument)     #websiteDoc <- tm_map(websiteDoc, FUN = tm_reduce, tmFuns = tmFuncs)     #websiteDoc <- tm_map(websiteDoc, PlainTextDocument)     #Read for testing     # websiteDoc[[1]][1]     #2. Term documents and word of cloud     dtmWebsiteDoc <- TermDocumentMatrix(websiteDoc)     #m <- as.matrix(dtmWebsiteDoc)     #v <- sort(rowSums(m), decreasing = TRUE)     #d <- data.frame(word = names(v), count = v)     #head(d,50) }
performTmFunctions <- function(websiteText) {     #Create a document for each website     #websiteDoc <- Corpus(VectorSource(websiteText))     myReader <- readTabular(mapping = list(content = "Text", id = "Id"))     websiteDoc <- VCorpus(DataframeSource(websiteText), readerControl = list(reader = myReader))     #Remove qqpbreakqq?     #skipWords <- function(x) removeWords(x, c(stopwords("english"), "qqpbreakqq"))     #tmFuncs <- list(tolower, removePunctuation, removeNumbers, stripWhitespace, skipWords, stemDocument)     #websiteDoc <- tm_map(websiteDoc, FUN = tm_reduce, tmFuns = tmFuncs)     #websiteDoc <- tm_map(websiteDoc, PlainTextDocument)     #Read for testing     # websiteDoc[[1]][1]     #2. Term documents and word of cloud     dtmWebsiteDoc <- TermDocumentMatrix(websiteDoc)     #m <- as.matrix(dtmWebsiteDoc)     #v <- sort(rowSums(m), decreasing = TRUE)     #d <- data.frame(word = names(v), count = v)     #head(d,50) }
library(tm) library(dplyr) library("SnowballC") library(e1071) #1.Read from file and preprocessing. 1 and 2 is inspired by https://www.springboard.com/blog/text-mining-in-r/ websitesFile <- 'websitesdata.txt'; websiteDataFrame <- read.table(websitesFile, sep = '\t', header = F, fill = TRUE, stringsAsFactors = FALSE); #deleting of not usefull fields.  websiteDataFrame <- websiteDataFrame[, c(2:4, 6)] names(websiteDataFrame) <- c('Host', 'Webpage', 'Title', 'Text')
websiteDataFrame <- websiteDataFrame %>% distinct(Host, .keep_all = TRUE) %>% mutate(doc_id = Host) %>% select(Host, Text, Title) websiteDataFrame$Id = 1:8
TDWebsitesData <- performTmFunctions(websiteDataFrame)
m <- as.matrix(TDWebsitesData)
руфв(ь)
head(m)
library(tm) library(dplyr) library("SnowballC") library(e1071) #1.Read from file and preprocessing. 1 and 2 is inspired by https://www.springboard.com/blog/text-mining-in-r/ websitesFile <- 'websitesdata.txt'; websiteDataFrame <- read.table(websitesFile, sep = '\t', header = F, fill = TRUE, stringsAsFactors = FALSE); #deleting of not usefull fields.  websiteDataFrame <- websiteDataFrame[, c(2:4, 6)] names(websiteDataFrame) <- c('Host', 'Webpage', 'Title', 'Text') #Take only first page from each website #websiteDataFrame <- websiteDataFrame %>% distinct(Host, .keep_all = TRUE) %>% mutate(doc_id = Host) %>% select(Host, Text, Title) websiteDataFrame <- websiteDataFrame %>% distinct(Host, .keep_all = TRUE) %>% mutate(doc_id = 1:8) %>% select(doc_id, Text, Title)
websiteDataFrame
library(tm) library(dplyr) library("SnowballC") library(e1071) #1.Read from file and preprocessing. 1 and 2 is inspired by https://www.springboard.com/blog/text-mining-in-r/ websitesFile <- 'websitesdata.txt'; websiteDataFrame <- read.table(websitesFile, sep = '\t', header = F, fill = TRUE, stringsAsFactors = FALSE); #deleting of not usefull fields.  websiteDataFrame <- websiteDataFrame[, c(2:4, 6)] names(websiteDataFrame) <- c('Host', 'Webpage', 'Title', 'text') #Take only first page from each website #websiteDataFrame <- websiteDataFrame %>% distinct(Host, .keep_all = TRUE) %>% mutate(doc_id = Host) %>% select(Host, Text, Title) websiteDataFrame <- websiteDataFrame %>% distinct(Host, .keep_all = TRUE) %>% mutate(doc_id = 1:8) %>% select(doc_id, text, Title)
websiteDataFrame
websiteDoc <- Corpus(DataframeSource(websiteDataFrame))
websiteDoc
websiteDoc[[1]]
websiteDoc[[1]]$Metadata
websiteDoc[[1]]['Metadata']
head(as.matrix(TermDocumentMatrix(websiteDoc)))
library(tm) library(dplyr) library("SnowballC") library(e1071) #1.Read from file and preprocessing. 1 and 2 is inspired by https://www.springboard.com/blog/text-mining-in-r/ websitesFile <- 'websitesdata.txt'; websiteDataFrame <- read.table(websitesFile, sep = '\t', header = F, fill = TRUE, stringsAsFactors = FALSE); #deleting of not usefull fields.  websiteDataFrame <- websiteDataFrame[, c(2:4, 6)] names(websiteDataFrame) <- c('Host', 'Webpage', 'Title', 'text')
websiteDataFrame <- websiteDataFrame %>% distinct(Host, .keep_all = TRUE) %>% mutate(doc_id = 1:8 + "asd") %>% select(doc_id, text, Title)
library(tm) library(dplyr) library("SnowballC") library(e1071) #1.Read from file and preprocessing. 1 and 2 is inspired by https://www.springboard.com/blog/text-mining-in-r/ websitesFile <- 'websitesdata.txt'; websiteDataFrame <- read.table(websitesFile, sep = '\t', header = F, fill = TRUE, stringsAsFactors = FALSE); #deleting of not usefull fields.  websiteDataFrame <- websiteDataFrame[, c(2:4, 6)] names(websiteDataFrame) <- c('Host', 'Webpage', 'Title', 'text') #Take only first page from each website #websiteDataFrame <- websiteDataFrame %>% distinct(Host, .keep_all = TRUE) %>% mutate(doc_id = Host) %>% select(Host, Text, Title) websiteDataFrame <- websiteDataFrame %>% distinct(Host, .keep_all = TRUE) %>% mutate(doc_id = sprintf("hi %d", 1:8)) %>% select(doc_id, text, Title)
websiteDataFrame
websiteDoc <- Corpus(DataframeSource(websiteDataFrame))
head(as.matrix(TermDocumentMatrix(websiteDoc)))
websiteDataFrame
library(tm) library(dplyr) library("SnowballC") library(e1071) #1.Read from file and preprocessing. 1 and 2 is inspired by https://www.springboard.com/blog/text-mining-in-r/ websitesFile <- 'websitesdata.txt'; websiteDataFrame <- read.table(websitesFile, sep = '\t', header = F, fill = TRUE, stringsAsFactors = FALSE); #deleting of not usefull fields.  websiteDataFrame <- websiteDataFrame[, c(2:4, 6)] names(websiteDataFrame) <- c('Host', 'Webpage', 'Title', 'text')
websiteDataFrame %>% distinct(Host, .keep_all = TRUE) %>% mutate(doc_id = sprintf("hi %d", 1:8)) %>% select(doc_id, text, Title)
websiteDataFrame %>% distinct(Host, .keep_all = TRUE) %>% mutate(doc_id = 50:58) %>% select(doc_id, text, Title)
websiteDataFrame %>% distinct(Host, .keep_all = TRUE) %>% mutate(doc_id = 50:59) %>% select(doc_id, text, Title)
websiteDataFrame %>% distinct(Host, .keep_all = TRUE) %>% mutate(doc_id = 50:57) %>% select(doc_id, text, Title)
websiteDataFrame <- websiteDataFrame %>% distinct(Host, .keep_all = TRUE) %>% mutate(doc_id = 50:57) %>% select(doc_id, text, Title)
websiteDoc <- Corpus(DataframeSource(websiteDataFrame)) head(as.matrix(TermDocumentMatrix(websiteDoc)))
library(tm) library(dplyr) library("SnowballC") library(e1071) #1.Read from file and preprocessing. 1 and 2 is inspired by https://www.springboard.com/blog/text-mining-in-r/ websitesFile <- 'websitesdata.txt'; websiteDataFrame <- read.table(websitesFile, sep = '\t', header = F, fill = TRUE, stringsAsFactors = FALSE); #deleting of not usefull fields.  websiteDataFrame <- websiteDataFrame[, c(2:4, 6)] names(websiteDataFrame) <- c('Host', 'Webpage', 'Title', 'text') #Take only first page from each website websiteDataFrame <- websiteDataFrame %>% distinct(Host, .keep_all = TRUE) %>% mutate(id = 50:57) %>% select(id, Host, Text, Title)
library(tm) library(dplyr) library("SnowballC") library(e1071) #1.Read from file and preprocessing. 1 and 2 is inspired by https://www.springboard.com/blog/text-mining-in-r/ websitesFile <- 'websitesdata.txt'; websiteDataFrame <- read.table(websitesFile, sep = '\t', header = F, fill = TRUE, stringsAsFactors = FALSE); #deleting of not usefull fields.  websiteDataFrame <- websiteDataFrame[, c(2:4, 6)] names(websiteDataFrame) <- c('Host', 'Webpage', 'Title', 'Text') #Take only first page from each website websiteDataFrame <- websiteDataFrame %>% distinct(Host, .keep_all = TRUE) %>% mutate(id = 50:57) %>% select(id, Host, Text, Title)
websiteDataFrame
performTmFunctions <- function(websiteText) {     #Create a document for each website     #websiteDoc <- Corpus(VectorSource(websiteText))     myReader <- readTabular(mapping = list(content = "Text", id = "Id"))     websiteDoc <- VCorpus(DataframeSource(websiteText), readerControl = list(reader = myReader))     dtmWebsiteDoc <- TermDocumentMatrix(websiteDoc)     #Remove qqpbreakqq?     #skipWords <- function(x) removeWords(x, c(stopwords("english"), "qqpbreakqq"))     #tmFuncs <- list(tolower, removePunctuation, removeNumbers, stripWhitespace, skipWords, stemDocument)     #websiteDoc <- tm_map(websiteDoc, FUN = tm_reduce, tmFuns = tmFuncs)     #websiteDoc <- tm_map(websiteDoc, PlainTextDocument)     #Read for testing     # websiteDoc[[1]][1]     #2. Term documents and word of cloud     #m <- as.matrix(dtmWebsiteDoc)     #v <- sort(rowSums(m), decreasing = TRUE)     #d <- data.frame(word = names(v), count = v)     #head(d,50)     # Generate the WordCloud     #Ggplotversion     #library(ggplot2)     #library(ggrepel)     #ggplot(data = d) +     #aes(x = 1, y = 1, size = count, label = word, col = count) +     #geom_text_repel(segment.size = 0, force = 100, segment.color = NA) +     #scale_size(range = c(1, 15), guide = FALSE) +     #scale_y_continuous(breaks = NULL) +     #scale_x_continuous(breaks = NULL) +     #labs(x = '', y = '') +     #theme_classic()     #Wordcloud      #library(wordcloud)     #wordcloud(d$word, d$count, scale = c(5, 0.5), max.words = 200, random.order = FALSE, rot.per = 0.6, use.r.layout = FALSE, colors = brewer.pal(8, 'Dark2')); }
TDWebsitesData <- performTmFunctions(websiteDataFrame) m <- as.matrix(TDWebsitesData)
TDWebsitesData <- performTmFunctions(websiteDataFrame)
library(tm) library(dplyr) library("SnowballC") library(e1071) #1.Read from file and preprocessing. 1 and 2 is inspired by https://www.springboard.com/blog/text-mining-in-r/ websitesFile <- 'websitesdata.txt'; websiteDataFrame <- read.table(websitesFile, sep = '\t', header = F, fill = TRUE, stringsAsFactors = FALSE); #deleting of not usefull fields.  websiteDataFrame <- websiteDataFrame[, c(2:4, 6)] names(websiteDataFrame) <- c('Host', 'Webpage', 'Title', 'Text') #Take only first page from each website websiteDataFrame <- websiteDataFrame %>% distinct(Host, .keep_all = TRUE) %>% mutate(id = 50:57) %>% select(Id, Host, Text, Title)
library(tm) library(dplyr) library("SnowballC") library(e1071) #1.Read from file and preprocessing. 1 and 2 is inspired by https://www.springboard.com/blog/text-mining-in-r/ websitesFile <- 'websitesdata.txt'; websiteDataFrame <- read.table(websitesFile, sep = '\t', header = F, fill = TRUE, stringsAsFactors = FALSE); #deleting of not usefull fields.  websiteDataFrame <- websiteDataFrame[, c(2:4, 6)] names(websiteDataFrame) <- c('Host', 'Webpage', 'Title', 'Text') #Take only first page from each website websiteDataFrame <- websiteDataFrame %>% distinct(Host, .keep_all = TRUE) %>% mutate(Id = 50:57) %>% select(Id, Host, Text, Title)
TDWebsitesData <- performTmFunctions(websiteDataFrame) m <- as.matrix(TDWebsitesData)
head(m)
performTmFunctions <- function(websiteText) {     #Create a document for each website     #websiteDoc <- Corpus(VectorSource(websiteText))     myReader <- readTabular(mapping = list(content = "Text", id = "Id"))     websiteDoc <- VCorpus(DataframeSource(websiteText), readerControl = list(reader = myReader))     dtmWebsiteDoc <- TermDocumentMatrix(websiteDoc)     #Remove qqpbreakqq?     skipWords <- function(x) removeWords(x, c(stopwords("english"), "qqpbreakqq"))     tmFuncs <- list(content_transformer(tolower), removePunctuation, removeNumbers, stripWhitespace, skipWords, stemDocument)     websiteDoc <- tm_map(websiteDoc, FUN = tm_reduce, tmFuns = tmFuncs)     websiteDoc <- tm_map(websiteDoc, PlainTextDocument)     websiteDoc     #Read for testing     # websiteDoc[[1]][1]     #2. Term documents and word of cloud     #m <- as.matrix(dtmWebsiteDoc)     #v <- sort(rowSums(m), decreasing = TRUE)     #d <- data.frame(word = names(v), count = v)     #head(d,50)     # Generate the WordCloud     #Ggplotversion     #library(ggplot2)     #library(ggrepel)     #ggplot(data = d) +     #aes(x = 1, y = 1, size = count, label = word, col = count) +     #geom_text_repel(segment.size = 0, force = 100, segment.color = NA) +     #scale_size(range = c(1, 15), guide = FALSE) +     #scale_y_continuous(breaks = NULL) +     #scale_x_continuous(breaks = NULL) +     #labs(x = '', y = '') +     #theme_classic()     #Wordcloud      #library(wordcloud)     #wordcloud(d$word, d$count, scale = c(5, 0.5), max.words = 200, random.order = FALSE, rot.per = 0.6, use.r.layout = FALSE, colors = brewer.pal(8, 'Dark2')); }
library(tm) library(dplyr) library("SnowballC") library(e1071) #1.Read from file and preprocessing. 1 and 2 is inspired by https://www.springboard.com/blog/text-mining-in-r/ websitesFile <- 'websitesdata.txt'; websiteDataFrame <- read.table(websitesFile, sep = '\t', header = F, fill = TRUE, stringsAsFactors = FALSE); #deleting of not usefull fields.  websiteDataFrame <- websiteDataFrame[, c(2:4, 6)] names(websiteDataFrame) <- c('Host', 'Webpage', 'Title', 'Text') #Take only first page from each website websiteDataFrame <- websiteDataFrame %>% distinct(Host, .keep_all = TRUE) %>% mutate(Id = 50:57) %>% select(Id, Host, Text, Title)
websiteDataFrame
TDWebsitesData <- performTmFunctions(websiteDataFrame) m <- as.matrix(TDWebsitesData)
performTmFunctions <- function(websiteText) {     #Create a document for each website     #websiteDoc <- Corpus(VectorSource(websiteText))     myReader <- readTabular(mapping = list(content = "Text", id = "Id"))     websiteDoc <- VCorpus(DataframeSource(websiteText), readerControl = list(reader = myReader))     dtmWebsiteDoc <- TermDocumentMatrix(websiteDoc)     #Remove qqpbreakqq?     skipWords <- function(x) removeWords(x, c(stopwords("english"), "qqpbreakqq"))     #tmFuncs <- list(content_transformer(tolower), removePunctuation, removeNumbers, stripWhitespace, skipWords, stemDocument)     tmFuncs <- list(tolower, removePunctuation, removeNumbers, stripWhitespace, skipWords, stemDocument)     websiteDoc <- tm_map(websiteDoc, FUN = tm_reduce, tmFuns = tmFuncs)     websiteDoc <- tm_map(websiteDoc, PlainTextDocument)     websiteDoc     #Read for testing     # websiteDoc[[1]][1]     #2. Term documents and word of cloud     #m <- as.matrix(dtmWebsiteDoc)     #v <- sort(rowSums(m), decreasing = TRUE)     #d <- data.frame(word = names(v), count = v)     #head(d,50)     # Generate the WordCloud     #Ggplotversion     #library(ggplot2)     #library(ggrepel)     #ggplot(data = d) +     #aes(x = 1, y = 1, size = count, label = word, col = count) +     #geom_text_repel(segment.size = 0, force = 100, segment.color = NA) +     #scale_size(range = c(1, 15), guide = FALSE) +     #scale_y_continuous(breaks = NULL) +     #scale_x_continuous(breaks = NULL) +     #labs(x = '', y = '') +     #theme_classic()     #Wordcloud      #library(wordcloud)     #wordcloud(d$word, d$count, scale = c(5, 0.5), max.words = 200, random.order = FALSE, rot.per = 0.6, use.r.layout = FALSE, colors = brewer.pal(8, 'Dark2')); }
TDWebsitesData <- performTmFunctions(websiteDataFrame) m <- as.matrix(TDWebsitesData)
performTmFunctions <- function(websiteText) {     #Create a document for each website     #websiteDoc <- Corpus(VectorSource(websiteText))     myReader <- readTabular(mapping = list(content = "Text", id = "Id"))     websiteDoc <- VCorpus(DataframeSource(websiteText), readerControl = list(reader = myReader))     dtmWebsiteDoc <- TermDocumentMatrix(websiteDoc)     #Remove qqpbreakqq?     skipWords <- function(x) removeWords(x, c(stopwords("english"), "qqpbreakqq"))     #tmFuncs <- list(content_transformer(tolower), removePunctuation, removeNumbers, stripWhitespace, skipWords, stemDocument)     tmFuncs <- list(content_transformer(tolower), removePunctuation, removeNumbers, stripWhitespace, skipWords, stemDocument)     websiteDoc <- tm_map(websiteDoc, FUN = tm_reduce, tmFuns = tmFuncs)     websiteDoc <- tm_map(websiteDoc, PlainTextDocument)     websiteDoc     #Read for testing     # websiteDoc[[1]][1]     #2. Term documents and word of cloud     #m <- as.matrix(dtmWebsiteDoc)     #v <- sort(rowSums(m), decreasing = TRUE)     #d <- data.frame(word = names(v), count = v)     #head(d,50)     # Generate the WordCloud     #Ggplotversion     #library(ggplot2)     #library(ggrepel)     #ggplot(data = d) +     #aes(x = 1, y = 1, size = count, label = word, col = count) +     #geom_text_repel(segment.size = 0, force = 100, segment.color = NA) +     #scale_size(range = c(1, 15), guide = FALSE) +     #scale_y_continuous(breaks = NULL) +     #scale_x_continuous(breaks = NULL) +     #labs(x = '', y = '') +     #theme_classic()     #Wordcloud      #library(wordcloud)     #wordcloud(d$word, d$count, scale = c(5, 0.5), max.words = 200, random.order = FALSE, rot.per = 0.6, use.r.layout = FALSE, colors = brewer.pal(8, 'Dark2')); }
library(tm) library(dplyr) library("SnowballC") library(e1071) #1.Read from file and preprocessing. 1 and 2 is inspired by https://www.springboard.com/blog/text-mining-in-r/ websitesFile <- 'websitesdata.txt'; websiteDataFrame <- read.table(websitesFile, sep = '\t', header = F, fill = TRUE, stringsAsFactors = FALSE);
ebsiteDataFrame <- websiteDataFrame[, c(2:4, 6)] names(websiteDataFrame) <- c('Host', 'Webpage', 'Title', 'Text') #Take only first page from each website websiteDataFrame <- websiteDataFrame %>% distinct(Host, .keep_all = TRUE) %>% mutate(Id = 50:57) %>% select(Id, Host, Text, Title)
head(websiteDataFrame)
websiteDataFrame <- websiteDataFrame %>% distinct(Host, .keep_all = TRUE) %>% mutate(Id = 50:57) %>% select(Id, Host, Text, Title)
websiteDataFrame %>% distinct(Host, .keep_all = TRUE)
library(tm) library(dplyr) library("SnowballC") library(e1071) #1.Read from file and preprocessing. 1 and 2 is inspired by https://www.springboard.com/blog/text-mining-in-r/ websitesFile <- 'websitesdata.txt'; websiteDataFrame <- read.table(websitesFile, sep = '\t', header = F, fill = TRUE, stringsAsFactors = FALSE);
#deleting of not usefull fields.  websiteDataFrame <- websiteDataFrame[, c(2:4, 6)] names(websiteDataFrame) <- c('Host', 'Webpage', 'Title', 'Text')
websiteDataFrame %>% distinct(Host, .keep_all = TRUE     )
websiteDataFrame <- websiteDataFrame %>% distinct(Host, .keep_all = TRUE) %>% mutate(Id = 50:57) %>% select(Id, Host, Text, Title)
websiteDataFrame
performTmFunctions <- function(websiteText) {     #Create a document for each website     #websiteDoc <- Corpus(VectorSource(websiteText))     myReader <- readTabular(mapping = list(content = "Text", id = "Id"))     websiteDoc <- VCorpus(DataframeSource(websiteText), readerControl = list(reader = myReader))     dtmWebsiteDoc <- TermDocumentMatrix(websiteDoc)     #Remove qqpbreakqq?     skipWords <- function(x) removeWords(x, c(stopwords("english"), "qqpbreakqq"))     #tmFuncs <- list(content_transformer(tolower), removePunctuation, removeNumbers, stripWhitespace, skipWords, stemDocument)     tmFuncs <- list(content_transformer(tolower), removePunctuation, removeNumbers, stripWhitespace, skipWords, stemDocument)     websiteDoc <- tm_map(websiteDoc, FUN = tm_reduce, tmFuns = tmFuncs)     websiteDoc <- tm_map(websiteDoc, PlainTextDocument)     websiteDoc     #Read for testing     # websiteDoc[[1]][1]     #2. Term documents and word of cloud     #m <- as.matrix(dtmWebsiteDoc)     #v <- sort(rowSums(m), decreasing = TRUE)     #d <- data.frame(word = names(v), count = v)     #head(d,50)     # Generate the WordCloud     #Ggplotversion     #library(ggplot2)     #library(ggrepel)     #ggplot(data = d) +     #aes(x = 1, y = 1, size = count, label = word, col = count) +     #geom_text_repel(segment.size = 0, force = 100, segment.color = NA) +     #scale_size(range = c(1, 15), guide = FALSE) +     #scale_y_continuous(breaks = NULL) +     #scale_x_continuous(breaks = NULL) +     #labs(x = '', y = '') +     #theme_classic()     #Wordcloud      #library(wordcloud)     #wordcloud(d$word, d$count, scale = c(5, 0.5), max.words = 200, random.order = FALSE, rot.per = 0.6, use.r.layout = FALSE, colors = brewer.pal(8, 'Dark2')); }
TDWebsitesData <- performTmFunctions(websiteDataFrame) m <- as.matrix(TDWebsitesData)
websiteDataFrame
performTmFunctions <- function(websiteText) {     #Create a document for each website     #websiteDoc <- Corpus(VectorSource(websiteText))     myReader <- readTabular(mapping = list(content = "Text", id = "Id"))     websiteDoc <- VCorpus(DataframeSource(websiteText), readerControl = list(reader = myReader))     #Remove qqpbreakqq?     skipWords <- function(x) removeWords(x, c(stopwords("english"), "qqpbreakqq"))     #tmFuncs <- list(content_transformer(tolower), removePunctuation, removeNumbers, stripWhitespace, skipWords, stemDocument)     tmFuncs <- list(content_transformer(tolower), removePunctuation, removeNumbers, stripWhitespace, skipWords, stemDocument)     websiteDoc <- tm_map(websiteDoc, FUN = tm_reduce, tmFuns = tmFuncs)     websiteDoc <- tm_map(websiteDoc, PlainTextDocument)     websiteDoc     #Read for testing     # websiteDoc[[1]][1]     #2. Term documents and word of cloud     #m <- as.matrix(dtmWebsiteDoc)     #v <- sort(rowSums(m), decreasing = TRUE)     #d <- data.frame(word = names(v), count = v)     #head(d,50)     # Generate the WordCloud     #Ggplotversion     #library(ggplot2)     #library(ggrepel)     #ggplot(data = d) +     #aes(x = 1, y = 1, size = count, label = word, col = count) +     #geom_text_repel(segment.size = 0, force = 100, segment.color = NA) +     #scale_size(range = c(1, 15), guide = FALSE) +     #scale_y_continuous(breaks = NULL) +     #scale_x_continuous(breaks = NULL) +     #labs(x = '', y = '') +     #theme_classic()     #Wordcloud      #library(wordcloud)     #wordcloud(d$word, d$count, scale = c(5, 0.5), max.words = 200, random.order = FALSE, rot.per = 0.6, use.r.layout = FALSE, colors = brewer.pal(8, 'Dark2')); }
TDWebsitesData <- performTmFunctions(websiteDataFrame)
m <- as.matrix(TDWebsitesData)
as.matrix(TDWebsitesData)
performTmFunctions <- function(websiteText) {     #Create a document for each website     #websiteDoc <- Corpus(VectorSource(websiteText))     myReader <- readTabular(mapping = list(content = "Text", id = "Id"))     websiteDoc <- VCorpus(DataframeSource(websiteText), readerControl = list(reader = myReader))     #Remove qqpbreakqq?     skipWords <- function(x) removeWords(x, c(stopwords("english"), "qqpbreakqq"))     #tmFuncs <- list(content_transformer(tolower), removePunctuation, removeNumbers, stripWhitespace, skipWords, stemDocument)     tmFuncs <- list(content_transformer(tolower), removePunctuation, removeNumbers, stripWhitespace, skipWords, stemDocument)     websiteDoc <- tm_map(websiteDoc, FUN = tm_reduce, tmFuns = tmFuncs)     websiteDoc <- tm_map(websiteDoc, PlainTextDocument)     TermDocumentMatrix(websiteDoc)     #Read for testing     # websiteDoc[[1]][1]     #2. Term documents and word of cloud     #m <- as.matrix(dtmWebsiteDoc)     #v <- sort(rowSums(m), decreasing = TRUE)     #d <- data.frame(word = names(v), count = v)     #head(d,50)     # Generate the WordCloud     #Ggplotversion     #library(ggplot2)     #library(ggrepel)     #ggplot(data = d) +     #aes(x = 1, y = 1, size = count, label = word, col = count) +     #geom_text_repel(segment.size = 0, force = 100, segment.color = NA) +     #scale_size(range = c(1, 15), guide = FALSE) +     #scale_y_continuous(breaks = NULL) +     #scale_x_continuous(breaks = NULL) +     #labs(x = '', y = '') +     #theme_classic()     #Wordcloud      #library(wordcloud)     #wordcloud(d$word, d$count, scale = c(5, 0.5), max.words = 200, random.order = FALSE, rot.per = 0.6, use.r.layout = FALSE, colors = brewer.pal(8, 'Dark2')); }
library(tm) library(dplyr) library("SnowballC") library(e1071) #1.Read from file and preprocessing. 1 and 2 is inspired by https://www.springboard.com/blog/text-mining-in-r/ websitesFile <- 'websitesdata.txt'; websiteDataFrame <- read.table(websitesFile, sep = '\t', header = F, fill = TRUE, stringsAsFactors = FALSE); #deleting of not usefull fields.  websiteDataFrame <- websiteDataFrame[, c(2:4, 6)] names(websiteDataFrame) <- c('Host', 'Webpage', 'Title', 'Text') #Take only first page from each website websiteDataFrame <- websiteDataFrame %>% distinct(Host, .keep_all = TRUE) %>% mutate(Id = 50:57) %>% select(Id, Host, Text, Title)
#websitesText <- websiteDataFrame[, 'Text'] TDWebsitesData <- performTmFunctions(websiteDataFrame) m <- as.matrix(TDWebsitesData)
head(m)
library(tm) library(dplyr) library("SnowballC") library(e1071) #1.Read from file and preprocessing. 1 and 2 is inspired by https://www.springboard.com/blog/text-mining-in-r/ websitesFile <- 'websitesdata.txt'; websiteDataFrame <- read.table(websitesFile, sep = '\t', header = F, fill = TRUE, stringsAsFactors = FALSE); #deleting of not usefull fields.  websiteDataFrame <- websiteDataFrame[, c(2:4, 6)] names(websiteDataFrame) <- c('Host', 'Webpage', 'Title', 'Text') #Take only first page from each website websiteDataFrame <- websiteDataFrame %>% distinct(Host, .keep_all = TRUE) %>% mutate(Id = 50:57) %>% select(Id, Host, Text, Title) #websitesText <- websiteDataFrame[, 'Text'] TDWebsitesData <- performTmFunctions(websiteDataFrame) m <- as.matrix(TDWebsitesData) performTmFunctions <- function(websiteText) {     #Create a document for each website     #websiteDoc <- Corpus(VectorSource(websiteText))     myReader <- readTabular(mapping = list(content = "Text", id = "Id"))     websiteDoc <- VCorpus(DataframeSource(websiteText), readerControl = list(reader = myReader))     #Remove qqpbreakqq?     skipWords <- function(x) removeWords(x, c(stopwords("english"), "qqpbreakqq"))     #tmFuncs <- list(content_transformer(tolower), removePunctuation, removeNumbers, stripWhitespace, skipWords, stemDocument)     tmFuncs <- list(content_transformer(tolower), removePunctuation, removeNumbers, stripWhitespace, skipWords, stemDocument)     # websiteDoc <- tm_map(websiteDoc, FUN = tm_reduce, tmFuns = tmFuncs)     #websiteDoc <- tm_map(websiteDoc, PlainTextDocument)     TermDocumentMatrix(websiteDoc)     #Read for testing     # websiteDoc[[1]][1]     #2. Term documents and word of cloud     #m <- as.matrix(dtmWebsiteDoc)     #v <- sort(rowSums(m), decreasing = TRUE)     #d <- data.frame(word = names(v), count = v)     #head(d,50)     # Generate the WordCloud     #Ggplotversion     #library(ggplot2)     #library(ggrepel)     #ggplot(data = d) +     #aes(x = 1, y = 1, size = count, label = word, col = count) +     #geom_text_repel(segment.size = 0, force = 100, segment.color = NA) +     #scale_size(range = c(1, 15), guide = FALSE) +     #scale_y_continuous(breaks = NULL) +     #scale_x_continuous(breaks = NULL) +     #labs(x = '', y = '') +     #theme_classic()     #Wordcloud      #library(wordcloud)     #wordcloud(d$word, d$count, scale = c(5, 0.5), max.words = 200, random.order = FALSE, rot.per = 0.6, use.r.layout = FALSE, colors = brewer.pal(8, 'Dark2')); }
performTmFunctions <- function(websiteText) {     #Create a document for each website     #websiteDoc <- Corpus(VectorSource(websiteText))     myReader <- readTabular(mapping = list(content = "Text", id = "Id"))     websiteDoc <- VCorpus(DataframeSource(websiteText), readerControl = list(reader = myReader))     #Remove qqpbreakqq?     skipWords <- function(x) removeWords(x, c(stopwords("english"), "qqpbreakqq"))     #tmFuncs <- list(content_transformer(tolower), removePunctuation, removeNumbers, stripWhitespace, skipWords, stemDocument)     tmFuncs <- list(content_transformer(tolower), removePunctuation, removeNumbers, stripWhitespace, skipWords, stemDocument)     # websiteDoc <- tm_map(websiteDoc, FUN = tm_reduce, tmFuns = tmFuncs)     #websiteDoc <- tm_map(websiteDoc, PlainTextDocument)     TermDocumentMatrix(websiteDoc)     #Read for testing     # websiteDoc[[1]][1]     #2. Term documents and word of cloud     #m <- as.matrix(dtmWebsiteDoc)     #v <- sort(rowSums(m), decreasing = TRUE)     #d <- data.frame(word = names(v), count = v)     #head(d,50)     # Generate the WordCloud     #Ggplotversion     #library(ggplot2)     #library(ggrepel)     #ggplot(data = d) +     #aes(x = 1, y = 1, size = count, label = word, col = count) +     #geom_text_repel(segment.size = 0, force = 100, segment.color = NA) +     #scale_size(range = c(1, 15), guide = FALSE) +     #scale_y_continuous(breaks = NULL) +     #scale_x_continuous(breaks = NULL) +     #labs(x = '', y = '') +     #theme_classic()     #Wordcloud      #library(wordcloud)     #wordcloud(d$word, d$count, scale = c(5, 0.5), max.words = 200, random.order = FALSE, rot.per = 0.6, use.r.layout = FALSE, colors = brewer.pal(8, 'Dark2')); }
library(tm) library(dplyr) library("SnowballC") library(e1071) #1.Read from file and preprocessing. 1 and 2 is inspired by https://www.springboard.com/blog/text-mining-in-r/ websitesFile <- 'websitesdata.txt'; websiteDataFrame <- read.table(websitesFile, sep = '\t', header = F, fill = TRUE, stringsAsFactors = FALSE); #deleting of not usefull fields.  websiteDataFrame <- websiteDataFrame[, c(2:4, 6)] names(websiteDataFrame) <- c('Host', 'Webpage', 'Title', 'Text') #Take only first page from each website websiteDataFrame <- websiteDataFrame %>% distinct(Host, .keep_all = TRUE) %>% mutate(Id = 50:57) %>% select(Id, Host, Text, Title) #websitesText <- websiteDataFrame[, 'Text'] TDWebsitesData <- performTmFunctions(websiteDataFrame) m <- as.matrix(TDWebsitesData)
head(m)
performTmFunctions <- function(websiteText) {     #Create a document for each website     #websiteDoc <- Corpus(VectorSource(websiteText))     myReader <- readTabular(mapping = list(content = "Text", id = "Id"))     websiteDoc <- VCorpus(DataframeSource(websiteText), readerControl = list(reader = myReader))     #Remove qqpbreakqq?     skipWords <- function(x) removeWords(x, c(stopwords("english"), "qqpbreakqq"))     #tmFuncs <- list(content_transformer(tolower), removePunctuation, removeNumbers, stripWhitespace, skipWords, stemDocument)     tmFuncs <- list(removePunctuation, removeNumbers, stripWhitespace, skipWords, stemDocument)     websiteDoc <- tm_map(websiteDoc, FUN = tm_reduce, tmFuns = tmFuncs)     #websiteDoc <- tm_map(websiteDoc, PlainTextDocument)     TermDocumentMatrix(websiteDoc)     #Read for testing     # websiteDoc[[1]][1]     #2. Term documents and word of cloud     #m <- as.matrix(dtmWebsiteDoc)     #v <- sort(rowSums(m), decreasing = TRUE)     #d <- data.frame(word = names(v), count = v)     #head(d,50)     # Generate the WordCloud     #Ggplotversion     #library(ggplot2)     #library(ggrepel)     #ggplot(data = d) +     #aes(x = 1, y = 1, size = count, label = word, col = count) +     #geom_text_repel(segment.size = 0, force = 100, segment.color = NA) +     #scale_size(range = c(1, 15), guide = FALSE) +     #scale_y_continuous(breaks = NULL) +     #scale_x_continuous(breaks = NULL) +     #labs(x = '', y = '') +     #theme_classic()     #Wordcloud      #library(wordcloud)     #wordcloud(d$word, d$count, scale = c(5, 0.5), max.words = 200, random.order = FALSE, rot.per = 0.6, use.r.layout = FALSE, colors = brewer.pal(8, 'Dark2')); }
library(tm) library(dplyr) library("SnowballC") library(e1071) #1.Read from file and preprocessing. 1 and 2 is inspired by https://www.springboard.com/blog/text-mining-in-r/ websitesFile <- 'websitesdata.txt'; websiteDataFrame <- read.table(websitesFile, sep = '\t', header = F, fill = TRUE, stringsAsFactors = FALSE); #deleting of not usefull fields.  websiteDataFrame <- websiteDataFrame[, c(2:4, 6)] names(websiteDataFrame) <- c('Host', 'Webpage', 'Title', 'Text') #Take only first page from each website websiteDataFrame <- websiteDataFrame %>% distinct(Host, .keep_all = TRUE) %>% mutate(Id = 50:57) %>% select(Id, Host, Text, Title) #websitesText <- websiteDataFrame[, 'Text'] TDWebsitesData <- performTmFunctions(websiteDataFrame) m <- as.matrix(TDWebsitesData)
head(m)
performTmFunctions <- function(websiteText) {     #Create a document for each website     #websiteDoc <- Corpus(VectorSource(websiteText))     myReader <- readTabular(mapping = list(content = "Text", id = "Id"))     websiteDoc <- VCorpus(DataframeSource(websiteText), readerControl = list(reader = myReader))     #Remove qqpbreakqq?     skipWords <- function(x) removeWords(x, c(stopwords("english"), "qqpbreakqq"))     #tmFuncs <- list(content_transformer(tolower), removePunctuation, removeNumbers, stripWhitespace, skipWords, stemDocument)     tmFuncs <- list(tolower, removePunctuation, removeNumbers, stripWhitespace, skipWords, stemDocument)     websiteDoc <- tm_map(websiteDoc, FUN = tm_reduce, tmFuns = tmFuncs)     #websiteDoc <- tm_map(websiteDoc, PlainTextDocument)     TermDocumentMatrix(websiteDoc)     #Read for testing     # websiteDoc[[1]][1]     #2. Term documents and word of cloud     #m <- as.matrix(dtmWebsiteDoc)     #v <- sort(rowSums(m), decreasing = TRUE)     #d <- data.frame(word = names(v), count = v)     #head(d,50)     # Generate the WordCloud     #Ggplotversion     #library(ggplot2)     #library(ggrepel)     #ggplot(data = d) +     #aes(x = 1, y = 1, size = count, label = word, col = count) +     #geom_text_repel(segment.size = 0, force = 100, segment.color = NA) +     #scale_size(range = c(1, 15), guide = FALSE) +     #scale_y_continuous(breaks = NULL) +     #scale_x_continuous(breaks = NULL) +     #labs(x = '', y = '') +     #theme_classic()     #Wordcloud      #library(wordcloud)     #wordcloud(d$word, d$count, scale = c(5, 0.5), max.words = 200, random.order = FALSE, rot.per = 0.6, use.r.layout = FALSE, colors = brewer.pal(8, 'Dark2')); }
library(tm) library(dplyr) library("SnowballC") library(e1071) #1.Read from file and preprocessing. 1 and 2 is inspired by https://www.springboard.com/blog/text-mining-in-r/ websitesFile <- 'websitesdata.txt'; websiteDataFrame <- read.table(websitesFile, sep = '\t', header = F, fill = TRUE, stringsAsFactors = FALSE); #deleting of not usefull fields.  websiteDataFrame <- websiteDataFrame[, c(2:4, 6)] names(websiteDataFrame) <- c('Host', 'Webpage', 'Title', 'Text') #Take only first page from each website websiteDataFrame <- websiteDataFrame %>% distinct(Host, .keep_all = TRUE) %>% mutate(Id = 50:57) %>% select(Id, Host, Text, Title) #websitesText <- websiteDataFrame[, 'Text'] TDWebsitesData <- performTmFunctions(websiteDataFrame) m <- as.matrix(TDWebsitesData)
performTmFunctions <- function(websiteText) {     #Create a document for each website     #websiteDoc <- Corpus(VectorSource(websiteText))     myReader <- readTabular(mapping = list(content = "Text", id = "Id"))     websiteDoc <- VCorpus(DataframeSource(websiteText), readerControl = list(reader = myReader))     #Remove qqpbreakqq?     skipWords <- function(x) removeWords(x, c(stopwords("english"), "qqpbreakqq"))     #tmFuncs <- list(content_transformer(tolower), removePunctuation, removeNumbers, stripWhitespace, skipWords, stemDocument)     tmFuncs <- list(tolower, removePunctuation, removeNumbers, stripWhitespace, skipWords, stemDocument)     websiteDoc <- tm_map(websiteDoc, FUN = tm_reduce, tmFuns = tmFuncs)     #websiteDoc <- tm_map(websiteDoc, PlainTextDocument)     TermDocumentMatrix(websiteDoc)     #Read for testing     # websiteDoc[[1]][1]     #2. Term documents and word of cloud     #m <- as.matrix(dtmWebsiteDoc)     #v <- sort(rowSums(m), decreasing = TRUE)     #d <- data.frame(word = names(v), count = v)     #head(d,50)     # Generate the WordCloud     #Ggplotversion     #library(ggplot2)     #library(ggrepel)     #ggplot(data = d) +     #aes(x = 1, y = 1, size = count, label = word, col = count) +     #geom_text_repel(segment.size = 0, force = 100, segment.color = NA) +     #scale_size(range = c(1, 15), guide = FALSE) +     #scale_y_continuous(breaks = NULL) +     #scale_x_continuous(breaks = NULL) +     #labs(x = '', y = '') +     #theme_classic()     #Wordcloud      #library(wordcloud)     #wordcloud(d$word, d$count, scale = c(5, 0.5), max.words = 200, random.order = FALSE, rot.per = 0.6, use.r.layout = FALSE, colors = brewer.pal(8, 'Dark2')); }
library(tm) library(dplyr) library("SnowballC") library(e1071) #1.Read from file and preprocessing. 1 and 2 is inspired by https://www.springboard.com/blog/text-mining-in-r/ websitesFile <- 'websitesdata.txt'; websiteDataFrame <- read.table(websitesFile, sep = '\t', header = F, fill = TRUE, stringsAsFactors = FALSE); #deleting of not usefull fields.  websiteDataFrame <- websiteDataFrame[, c(2:4, 6)] names(websiteDataFrame) <- c('Host', 'Webpage', 'Title', 'Text') #Take only first page from each website websiteDataFrame <- websiteDataFrame %>% distinct(Host, .keep_all = TRUE) %>% mutate(Id = 50:57) %>% select(Id, Host, Text, Title) #websitesText <- websiteDataFrame[, 'Text'] TDWebsitesData <- performTmFunctions(websiteDataFrame) m <- as.matrix(TDWebsitesData)
performTmFunctions <- function(websiteText) {     #Create a document for each website     #websiteDoc <- Corpus(VectorSource(websiteText))     myReader <- readTabular(mapping = list(content = "Text", id = "Id"))     websiteDoc <- VCorpus(DataframeSource(websiteText), readerControl = list(reader = myReader))     #Remove qqpbreakqq?     skipWords <- function(x) removeWords(x, c(stopwords("english"), "qqpbreakqq"))     #tmFuncs <- list(content_transformer(tolower), removePunctuation, removeNumbers, stripWhitespace, skipWords, stemDocument)     tmFuncs <- list(removePunctuation, removeNumbers, stripWhitespace, skipWords, stemDocument)     websiteDoc <- tm_map(websiteDoc, FUN = tm_reduce, tmFuns = tmFuncs)     #websiteDoc <- tm_map(websiteDoc, PlainTextDocument)     TermDocumentMatrix(websiteDoc)     #Read for testing     # websiteDoc[[1]][1]     #2. Term documents and word of cloud     #m <- as.matrix(dtmWebsiteDoc)     #v <- sort(rowSums(m), decreasing = TRUE)     #d <- data.frame(word = names(v), count = v)     #head(d,50)     # Generate the WordCloud     #Ggplotversion     #library(ggplot2)     #library(ggrepel)     #ggplot(data = d) +     #aes(x = 1, y = 1, size = count, label = word, col = count) +     #geom_text_repel(segment.size = 0, force = 100, segment.color = NA) +     #scale_size(range = c(1, 15), guide = FALSE) +     #scale_y_continuous(breaks = NULL) +     #scale_x_continuous(breaks = NULL) +     #labs(x = '', y = '') +     #theme_classic()     #Wordcloud      #library(wordcloud)     #wordcloud(d$word, d$count, scale = c(5, 0.5), max.words = 200, random.order = FALSE, rot.per = 0.6, use.r.layout = FALSE, colors = brewer.pal(8, 'Dark2')); }
library(tm) library(dplyr) library("SnowballC") library(e1071) #1.Read from file and preprocessing. 1 and 2 is inspired by https://www.springboard.com/blog/text-mining-in-r/ websitesFile <- 'websitesdata.txt'; websiteDataFrame <- read.table(websitesFile, sep = '\t', header = F, fill = TRUE, stringsAsFactors = FALSE); #deleting of not usefull fields.  websiteDataFrame <- websiteDataFrame[, c(2:4, 6)] names(websiteDataFrame) <- c('Host', 'Webpage', 'Title', 'Text') #Take only first page from each website websiteDataFrame <- websiteDataFrame %>% distinct(Host, .keep_all = TRUE) %>% mutate(Id = 50:57) %>% select(Id, Host, Text, Title) #websitesText <- websiteDataFrame[, 'Text'] TDWebsitesData <- performTmFunctions(websiteDataFrame) m <- as.matrix(TDWebsitesData)
head(m)
m
create_container ? ? ??create_container
?create_container
?train_model
??train_model
library(tm) library(dplyr) library("SnowballC") performTmFunctions <- function(websiteText) {     #Create a document for each website     #websiteDoc <- Corpus(VectorSource(websiteText))     myReader <- readTabular(mapping = list(content = "Text", id = "Id"))     websiteDoc <- VCorpus(DataframeSource(websiteText), readerControl = list(reader = myReader))     #Remove qqpbreakqq?     skipWords <- function(x) removeWords(x, c(stopwords("english"), "qqpbreakqq"))     #tmFuncs <- list(content_transformer(tolower), removePunctuation, removeNumbers, stripWhitespace, skipWords, stemDocument)     tmFuncs <- list(removePunctuation, removeNumbers, stripWhitespace, skipWords, stemDocument)     websiteDoc <- tm_map(websiteDoc, FUN = tm_reduce, tmFuns = tmFuncs)     TermDocumentMatrix(websiteDoc)     #Read for testing     # websiteDoc[[1]][1] }
websitesFile <- 'websitesdata.txt'; websiteDataFrame <- read.table(websitesFile, sep = '\t', header = F, fill = TRUE, stringsAsFactors = FALSE); #deleting of not usefull fields.  websiteDataFrame <- websiteDataFrame[, c(2:4, 6)] names(websiteDataFrame) <- c('Host', 'Webpage', 'Title', 'Text') #Take only first page from  each website websiteDataFrame <- websiteDataFrame %>% distinct(Host, .keep_all = TRUE) %>% mutate(Id = 50:57) %>% select(Id, Host, Text, Title) #websitesText <- websiteDataFrame[, 'Text'] TDWebsitesData <- performTmFunctions(websiteDataFrame) m <- as.matrix(TDWebsitesData)
head(m)
head(m)
dim(m)
dim(m)[2]
1:dim(m)[2]
1:dim(m)[2]
 c(1,2,1,1,2,2,2,1)
industryCodes <- c(1, 2, 1, 1, 2, 2, 2, 1)
industryCodes
head(m)
svmFit <- svm(m, industryCodes, kernel = "linear")
library(e1071)
svmFit <- svm(m, industryCodes, kernel = "linear")
t(m)
svmFit <- svm(t(m), industryCodes, kernel = "linear")
svmFit
sum(svmFit)
class(svmFit)
svmFit <- svm(t(m), industryCodes, kernel = "linear")
svmFit$SV
svmFit$index
svmFit$coefs
websiteDataFrame
library(tm) library(dplyr) library("SnowballC") performTmFunctions <- function(websiteText) {     #Create a document for each website     #websiteDoc <- Corpus(VectorSource(websiteText))     myReader <- readTabular(mapping = list(content = "Text", id = "Id"))     websiteDoc <- VCorpus(DataframeSource(websiteText), readerControl = list(reader = myReader))     #Remove qqpbreakqq?     skipWords <- function(x) removeWords(x, c(stopwords("english"), "qqpbreakqq"))     #tmFuncs <- list(content_transformer(tolower), removePunctuation, removeNumbers, stripWhitespace, skipWords, stemDocument)     tmFuncs <- list(removePunctuation, removeNumbers, stripWhitespace, skipWords, stemDocument)     websiteDoc <- tm_map(websiteDoc, FUN = tm_reduce, tmFuns = tmFuncs)     TermDocumentMatrix(websiteDoc)     #Read for testing     # websiteDoc[[1]][1] } #1.Read from file and preprocessing. 1 and 2 is inspired by https://www.springboard.com/blog/text-mining-in-r/ websitesFile <- 'websitesdata.txt'; websiteDataFrame <- read.table(websitesFile, sep = '\t', header = F, fill = TRUE, stringsAsFactors = FALSE); #deleting of not usefull fields.  websiteDataFrame <- websiteDataFrame[, c(2:4, 6)] names(websiteDataFrame) <- c('Host', 'Webpage', 'Title', 'Text') library(tm) library(dplyr) library("SnowballC") performTmFunctions <- function(websiteText) {     #Create a document for each website     #websiteDoc <- Corpus(VectorSource(websiteText))     myReader <- readTabular(mapping = list(content = "Text", id = "Id"))     websiteDoc <- VCorpus(DataframeSource(websiteText), readerControl = list(reader = myReader))     #Remove qqpbreakqq?     skipWords <- function(x) removeWords(x, c(stopwords("english"), "qqpbreakqq"))     #tmFuncs <- list(content_transformer(tolower), removePunctuation, removeNumbers, stripWhitespace, skipWords, stemDocument)     tmFuncs <- list(removePunctuation, removeNumbers, stripWhitespace, skipWords, stemDocument)     websiteDoc <- tm_map(websiteDoc, FUN = tm_reduce, tmFuns = tmFuncs)     TermDocumentMatrix(websiteDoc)     #Read for testing     # websiteDoc[[1]][1] } #1.Read from file and preprocessing. 1 and 2 is inspired by https://www.springboard.com/blog/text-mining-in-r/ websitesFile <- 'websitesdata.txt'; websiteDataFrame <- read.table(websitesFile, sep = '\t', header = F, fill = TRUE, stringsAsFactors = FALSE); #deleting of not usefull fields.  websiteDataFrame <- websiteDataFrame[, c(2:4, 6)] names(websiteDataFrame) <- c('Host', 'Webpage', 'Title', 'Text') library(tm) library(dplyr) library("SnowballC") performTmFunctions <- function(websiteText) {     #Create a document for each website     #websiteDoc <- Corpus(VectorSource(websiteText))     myReader <- readTabular(mapping = list(content = "Text", id = "Id"))     websiteDoc <- VCorpus(DataframeSource(websiteText), readerControl = list(reader = myReader))     #Remove qqpbreakqq?     skipWords <- function(x) removeWords(x, c(stopwords("english"), "qqpbreakqq"))     #tmFuncs <- list(content_transformer(tolower), removePunctuation, removeNumbers, stripWhitespace, skipWords, stemDocument)     tmFuncs <- list(removePunctuation, removeNumbers, stripWhitespace, skipWords, stemDocument)     websiteDoc <- tm_map(websiteDoc, FUN = tm_reduce, tmFuns = tmFuncs)     TermDocumentMatrix(websiteDoc)     #Read for testing     # websiteDoc[[1]][1] } #1.Read from file and preprocessing. 1 and 2 is inspired by https://www.springboard.com/blog/text-mining-in-r/ websitesFile <- 'websitesdata.txt'; websiteDataFrame <- read.table(websitesFile, sep = '\t', header = F, fill = TRUE, stringsAsFactors = FALSE); #deleting of not usefull fields.  websiteDataFrame <- websiteDataFrame[, c(2:4, 6)] names(websiteDataFrame) <- c('Host', 'Webpage', 'Title', 'Text')
websiteDataFrame <- websiteDataFrame %>% distinct(Host, .keep_all = TRUE) %>% mutate(Id = 50:57) %>% select(Id, Host, Text, Title)
websiteDataFrame
companiesCodesFile <- 'CompaniesCodes.csv'; companiesDataFrame <- read.table(companiesCodesFile, sep = ',', header = F, fill = TRUE, stringsAsFactors = FALSE);
>companiesCodesFile <- 'CompaniesCodes.csv';
companiesCodesFile <- 'CompaniesCodes.csv';
companiesCodesFile\
companiesCodesFile
companiesDataFrame <- read.table(companiesCodesFile, sep = ',', header = F, fill = TRUE, stringsAsFactors = FALSE);
read.table(companiesCodesFile, sep = ',', header = F, fill = TRUE, stringsAsFactors = FALSE)
#2.Read companies info and merge companiesCodesFile <- 'CompaniesCodes.txt'; companiesDataFrame <- read.table(companiesCodesFile, sep = '\t', header = F, fill = TRUE, stringsAsFactors = FALSE);
head(companiesDataFrame)
head(companiesDataFrame %>% select(V2, V4))
companiesDataFrame %>% select(V2, V4)
companiesDataFrame <- companiesDataFrame %>% select(V2, V4)
companiesDataFrame[1,2]
companiesDataFrame[1,3]
companiesDataFrame[1,2]
companiesDataFrame[2,2]
companiesDataFrame[3,2]
companiesDataFrame[4,2]
companiesDataFrame[5,2]
companiesDataFrame[6,2]
companiesDataFrame[1.1]
companiesDataFrame[1,1]
head(websiteDataFrame)
library(tm) library(dplyr) library("SnowballC") performTmFunctions <- function(websiteText) {     #Create a document for each website     #websiteDoc <- Corpus(VectorSource(websiteText))     myReader <- readTabular(mapping = list(content = "Text", id = "Id"))     websiteDoc <- VCorpus(DataframeSource(websiteText), readerControl = list(reader = myReader))     #Remove qqpbreakqq?     skipWords <- function(x) removeWords(x, c(stopwords("english"), "qqpbreakqq"))     #tmFuncs <- list(content_transformer(tolower), removePunctuation, removeNumbers, stripWhitespace, skipWords, stemDocument)     tmFuncs <- list(removePunctuation, removeNumbers, stripWhitespace, skipWords, stemDocument)     websiteDoc <- tm_map(websiteDoc, FUN = tm_reduce, tmFuns = tmFuncs)     TermDocumentMatrix(websiteDoc)     #Read for testing     # websiteDoc[[1]][1] } #1.Read data file and preprocessing.  websitesFile <- 'websitesdata.txt'; websiteDataFrame <- read.table(websitesFile, sep = '\t', header = F, fill = TRUE, stringsAsFactors = FALSE); #deleting of not usefull fields.  websiteDataFrame <- websiteDataFrame[, c(2:4, 6)] names(websiteDataFrame) <- c('Host', 'Webpage', 'Title', 'Text') #Take only first page from  each website websiteDataFrame <- websiteDataFrame %>% distinct(Host, .keep_all = TRUE) %>% select(Id, Host, Webpage, Text, Title) #2.Read companies info and merge companiesCodesFile <- 'CompaniesCodes.txt'; companiesDataFrame <- read.table(companiesCodesFile, sep = '\t', header = F, fill = TRUE, stringsAsFactors = FALSE); companiesDataFrame <- companiesDataFrame %>% select(V2, V4)
library(tm) library(dplyr) library("SnowballC") performTmFunctions <- function(websiteText) {     #Create a document for each website     #websiteDoc <- Corpus(VectorSource(websiteText))     myReader <- readTabular(mapping = list(content = "Text", id = "Id"))     websiteDoc <- VCorpus(DataframeSource(websiteText), readerControl = list(reader = myReader))     #Remove qqpbreakqq?     skipWords <- function(x) removeWords(x, c(stopwords("english"), "qqpbreakqq"))     #tmFuncs <- list(content_transformer(tolower), removePunctuation, removeNumbers, stripWhitespace, skipWords, stemDocument)     tmFuncs <- list(removePunctuation, removeNumbers, stripWhitespace, skipWords, stemDocument)     websiteDoc <- tm_map(websiteDoc, FUN = tm_reduce, tmFuns = tmFuncs)     TermDocumentMatrix(websiteDoc)     #Read for testing     # websiteDoc[[1]][1] } #1.Read data file and preprocessing.  websitesFile <- 'websitesdata.txt'; websiteDataFrame <- read.table(websitesFile, sep = '\t', header = F, fill = TRUE, stringsAsFactors = FALSE); #deleting of not usefull fields.  websiteDataFrame <- websiteDataFrame[, c(2:4, 6)] names(websiteDataFrame) <- c('Host', 'Webpage', 'Title', 'Text') #Take only first page from  each website websiteDataFrame <- websiteDataFrame %>% distinct(Host, .keep_all = TRUE) %>% select(Host, Webpage, Text, Title) #2.Read companies info and merge companiesCodesFile <- 'CompaniesCodes.txt'; companiesDataFrame <- read.table(companiesCodesFile, sep = '\t', header = F, fill = TRUE, stringsAsFactors = FALSE); companiesDataFrame <- companiesDataFrame %>% select(V2, V4)
head(companiesDataFrame)
head(websiteDataFrame)
mergedData <- websiteDataFrame %>% inner_join(companiesDataFrame, by = c("Webpage" = "V2"))
mergedData
names(mergedData)
library(tm) library(dplyr) library("SnowballC") performTmFunctions <- function(websiteText) {     #Create a document for each website     #websiteDoc <- Corpus(VectorSource(websiteText))     myReader <- readTabular(mapping = list(content = "Text", id = "Id"))     websiteDoc <- VCorpus(DataframeSource(websiteText), readerControl = list(reader = myReader))     #Remove qqpbreakqq?     skipWords <- function(x) removeWords(x, c(stopwords("english"), "qqpbreakqq"))     #tmFuncs <- list(content_transformer(tolower), removePunctuation, removeNumbers, stripWhitespace, skipWords, stemDocument)     tmFuncs <- list(removePunctuation, removeNumbers, stripWhitespace, skipWords, stemDocument)     websiteDoc <- tm_map(websiteDoc, FUN = tm_reduce, tmFuns = tmFuncs)     TermDocumentMatrix(websiteDoc)     #Read for testing     # websiteDoc[[1]][1] } #1.Read data file and preprocessing.  websitesFile <- 'websitesdata.txt'; websiteDataFrame <- read.table(websitesFile, sep = '\t', header = F, fill = TRUE, stringsAsFactors = FALSE); #deleting of not usefull fields.  websiteDataFrame <- websiteDataFrame[, c(2:4, 6)] names(websiteDataFrame) <- c('Host', 'Webpage', 'Title', 'Text') #Take only first page from  each website websiteDataFrame <- websiteDataFrame %>% distinct(Host, .keep_all = TRUE) %>% select(Host, Webpage, Text, Title) #2.Read companies info and merge companiesCodesFile <- 'CompaniesCodes.txt'; companiesDataFrame <- read.table(companiesCodesFile, sep = '\t', header = F, fill = TRUE, stringsAsFactors = FALSE); companiesDataFrame <- companiesDataFrame %>% select(V2, V3) #3. Join companies data and webdata mergedData <- websiteDataFrame %>% inner_join(companiesDataFrame, by = c("Webpage" = "V2")) #websitesText <- websiteDataFrame[, 'Text'] TDWebsitesData <- performTmFunctions(websiteDataFrame) m <- as.matrix(TDWebsitesData)
library(tm) library(dplyr) library("SnowballC") performTmFunctions <- function(websiteText) {     #Create a document for each website     #websiteDoc <- Corpus(VectorSource(websiteText))     myReader <- readTabular(mapping = list(content = "Text", id = "Id"))     websiteDoc <- VCorpus(DataframeSource(websiteText), readerControl = list(reader = myReader))     #Remove qqpbreakqq?     skipWords <- function(x) removeWords(x, c(stopwords("english"), "qqpbreakqq"))     #tmFuncs <- list(content_transformer(tolower), removePunctuation, removeNumbers, stripWhitespace, skipWords, stemDocument)     tmFuncs <- list(removePunctuation, removeNumbers, stripWhitespace, skipWords, stemDocument)     websiteDoc <- tm_map(websiteDoc, FUN = tm_reduce, tmFuns = tmFuncs)     TermDocumentMatrix(websiteDoc)     #Read for testing     # websiteDoc[[1]][1] } #1.Read data file and preprocessing.  websitesFile <- 'websitesdata.txt'; websiteDataFrame <- read.table(websitesFile, sep = '\t', header = F, fill = TRUE, stringsAsFactors = FALSE); #deleting of not usefull fields.  websiteDataFrame <- websiteDataFrame[, c(2:4, 6)] names(websiteDataFrame) <- c('Host', 'Webpage', 'Title', 'Text') #Take only first page from  each website websiteDataFrame <- websiteDataFrame %>% distinct(Host, .keep_all = TRUE) %>% select(Host, Webpage, Text, Title) #2.Read companies info and merge companiesCodesFile <- 'CompaniesCodes.txt'; companiesDataFrame <- read.table(companiesCodesFile, sep = '\t', header = F, fill = TRUE, stringsAsFactors = FALSE); companiesDataFrame <- companiesDataFrame %>% select(V2, V3) #3. Join companies data and webdata mergedData <- websiteDataFrame %>% inner_join(companiesDataFrame, by = c("Webpage" = "V2"))
names(websiteDataFrame)
#Take only first page from  each website websiteDataFrame <- websiteDataFrame %>% distinct(Host, .keep_all = TRUE) %>% select(Host, Webpage, Text, Title) #2.Read companies info and merge companiesCodesFile <- 'CompaniesCodes.txt'; companiesDataFrame <- read.table(companiesCodesFile, sep = '\t', header = F, fill = TRUE, stringsAsFactors = FALSE); companiesDataFrame <- companiesDataFrame %>% select(V2, V3)
head(companiesDataFrame)
websiteDataFrame %>% inner_join(companiesDataFrame, by = c("Webpage" = "V2"))
TDWebsitesData <- performTmFunctions(websiteDataFrame) m <- as.matrix(TDWebsitesData)
mergedData <- websiteDataFrame %>% inner_join(companiesDataFrame, by = c("Webpage" = "V2")) #websitesText <- websiteDataFrame[, 'Text'] TDWebsitesData <- performTmFunctions(websiteDataFrame)
websiteDataFrame
websiteText <- websiteDataFrame myReader <- readTabular(mapping = list(content = "Text"))
websiteDoc <- VCorpus(DataframeSource(websiteText), readerControl = list(reader = myReader))
skipWords <- function(x) removeWords(x, c(stopwords("english"), "qqpbreakqq")) #tmFuncs <- list(content_transformer(tolower), removePunctuation, removeNumbers, stripWhitespace, skipWords, stemDocument) tmFuncs <- list(removePunctuation, removeNumbers, stripWhitespace, skipWords, stemDocument) websiteDoc <- tm_map(websiteDoc, FUN = tm_reduce, tmFuns = tmFuncs)
TermDocumentMatrix(websiteDoc)
performTmFunctions <- function(websiteText) {     #Create a document for each website     #websiteDoc <- Corpus(VectorSource(websiteText))     myReader <- readTabular(mapping = list(content = "Text"))     websiteDoc <- VCorpus(DataframeSource(websiteText), readerControl = list(reader = myReader))     #Remove qqpbreakqq?     skipWords <- function(x) removeWords(x, c(stopwords("english"), "qqpbreakqq"))     #tmFuncs <- list(content_transformer(tolower), removePunctuation, removeNumbers, stripWhitespace, skipWords, stemDocument)     tmFuncs <- list(removePunctuation, removeNumbers, stripWhitespace, skipWords, stemDocument)     websiteDoc <- tm_map(websiteDoc, FUN = tm_reduce, tmFuns = tmFuncs)     TermDocumentMatrix(websiteDoc)     #Read for testing     # websiteDoc[[1]][1] }
TDWebsitesData <- performTmFunctions(websiteDataFrame)
m <- as.matrix(TDWebsitesData)
m
head(m)
mergedData
mergedData$V
mergedData$V3
mergedData[,"V3"]
mergedData[1:5,"V3"]
mergedData
head(websiteDataFrame)
m
library(tm) library(dplyr) library("SnowballC") performTmFunctions <- function(websiteText) {     #Create a document for each website     #websiteDoc <- Corpus(VectorSource(websiteText))     myReader <- readTabular(mapping = list(content = "Text"))     websiteDoc <- VCorpus(DataframeSource(websiteText), readerControl = list(reader = myReader))     #Remove qqpbreakqq?     skipWords <- function(x) removeWords(x, c(stopwords("english"), "qqpbreakqq"))     #tmFuncs <- list(content_transformer(tolower), removePunctuation, removeNumbers, stripWhitespace, skipWords, stemDocument)     tmFuncs <- list(removePunctuation, removeNumbers, stripWhitespace, skipWords, stemDocument)     websiteDoc <- tm_map(websiteDoc, FUN = tm_reduce, tmFuns = tmFuncs)     TermDocumentMatrix(websiteDoc)     #Read for testing     # websiteDoc[[1]][1] } #1.Read data file and preprocessing.  websitesFile <- 'Input\\WebsitesData.txt'; websiteDataFrame <- read.table(websitesFile, sep = ',', header = F, fill = TRUE, stringsAsFactors = FALSE); #deleting of not usefull fields.  websiteDataFrame <- websiteDataFrame[, c(2:4, 6)] names(websiteDataFrame) <- c('Host', 'Webpage', 'Title', 'Text') #Take only first page from  each website websiteDataFrame <- websiteDataFrame %>% distinct(Host, .keep_all = TRUE) %>% select(Host, Webpage, Text, Title)
dim(websiteDataFrame)
dim(websiteDataFrame %>% filter(!(Title == "null" | Title == "" | Text == "null" | Text == "")))
websiteDataFrame <- websiteDataFrame %>% filter(!(Title == "null" | Title == "" | Text == "null" | Text == ""))
dim(websiteDataFrame)
companiesCodesFile <- 'Input\\CompaniesCodes.txt'; companiesCodeDataFrame <- read.table(companiesCodesFile, sep = '\t', header = F, fill = TRUE, stringsAsFactors = FALSE); companiesCodeDataFrame <- companiesCodeDataFrame %>% select(V2, CompanyCode = V3, CompanyCodeDescription = V4)
companiesCodeDataFrame
dim(companiesCodeDataFrame)
dim(companiesCodeDataFrame %>% filter(!(CompanyCode == "null" | CompanyCode == "" | CompanyCode == "null" | CompanyCode == "")))
dim(companiesCodeDataFrame %>% filter(!(CompanyCode == "null" | CompanyCode == "" | CompanyCode == "null" | CompanyCode == "" | V2 == "null" | V2 == "")))
companiesCodesFile <- 'Input\\CompaniesCodes.txt'; companiesCodeDataFrame <- read.table(companiesCodesFile, sep = '\t', header = F, fill = TRUE, stringsAsFactors = FALSE); companiesCodeDataFrame <- companiesCodeDataFrame %>% select(V2, CompanyCode = V3, CompanyCodeDescription = V4) %>% filter(!(CompanyCode == "null" | CompanyCode == "" | CompanyCode == "null" | CompanyCode == "" | V2 == "null" | V2 == ""))
dim(companiesCodeDataFrame)
mergedData <- websiteDataFrame %>% inner_join(companiesCodeDataFrame, by = c("Webpage" = "V2")) #Write resulted dataframe to the disk for future processing write.table(mergedData, "Output\\MergedCompaniesData.txt", sep = "\t", row.names = FALSE)
library(tm) library(dplyr) library("SnowballC") performTmFunctions <- function(websiteText) {     #Create a document for each website     #websiteDoc <- Corpus(VectorSource(websiteText))     myReader <- readTabular(mapping = list(content = "Text"))     websiteDoc <- VCorpus(DataframeSource(websiteText), readerControl = list(reader = myReader))     #Remove qqpbreakqq?     skipWords <- function(x) removeWords(x, c(stopwords("english"), "qqpbreakqq"))     #tmFuncs <- list(content_transformer(tolower), removePunctuation, removeNumbers, stripWhitespace, skipWords, stemDocument)     tmFuncs <- list(removePunctuation, removeNumbers, stripWhitespace, skipWords, stemDocument)     websiteDoc <- tm_map(websiteDoc, FUN = tm_reduce, tmFuns = tmFuncs)     TermDocumentMatrix(websiteDoc)     #Read for testing     # websiteDoc[[1]][1] } #1.Read data file and preprocessing.  websitesFile <- 'Input\\WebsitesData.txt'; websiteDataFrame <- read.table(websitesFile, sep = ',', header = F, fill = TRUE, stringsAsFactors = FALSE); #deleting of not usefull fields.  websiteDataFrame <- websiteDataFrame[, c(2:4, 6)] names(websiteDataFrame) <- c('Host', 'Webpage', 'Title', 'Text') #Take only first page from  each website websiteDataFrame <- websiteDataFrame %>% distinct(Host, .keep_all = TRUE) %>% filter(!(Title == "null" | Title == "" | Text == "null" | Text == "")) %>% select(Host, Webpage, Text, Title) #2.Read companies info and merge companiesCodesFile <- 'Input\\CompaniesCodes.txt'; companiesCodeDataFrame <- read.table(companiesCodesFile, sep = '\t', header = F, fill = TRUE, stringsAsFactors = FALSE); companiesCodeDataFrame <- companiesCodeDataFrame %>% select(V2, CompanyCode = V3, CompanyCodeDescription = V4) %>% filter(!(CompanyCode == "null" | CompanyCode == "" | CompanyCode == "null" | CompanyCode == "" | V2 == "null" | V2 == "")) #3. Join companies data and webdata mergedData <- websiteDataFrame %>% inner_join(companiesCodeDataFrame, by = c("Webpage" = "V2")) %>% filter(!(CompanyCode == "null" | CompanyCode == "" | CompanyCode == "null" | CompanyCode == "" | Webpage == "null" | Webpage == "")) %>% distinct(Webpage, .keep_all = TRUE)
write.table(mergedData, "Output\\MergedCompaniesData.txt", sep = "\t", row.names = FALSE)
library(tm) library(dplyr) library("SnowballC") performTmFunctions <- function(websiteText) {     #Create a document for each website     #websiteDoc <- Corpus(VectorSource(websiteText))     myReader <- readTabular(mapping = list(content = "Text"))     websiteDoc <- VCorpus(DataframeSource(websiteText), readerControl = list(reader = myReader))     #Remove qqpbreakqq?     skipWords <- function(x) removeWords(x, c(stopwords("english"), "qqpbreakqq"))     #tmFuncs <- list(content_transformer(tolower), removePunctuation, removeNumbers, stripWhitespace, skipWords, stemDocument)     tmFuncs <- list(removePunctuation, removeNumbers, stripWhitespace, skipWords, stemDocument)     websiteDoc <- tm_map(websiteDoc, FUN = tm_reduce, tmFuns = tmFuncs)     TermDocumentMatrix(websiteDoc)     #Read for testing     # websiteDoc[[1]][1] } #1.Read data file and preprocessing.  websitesFile <- 'Input\\WebsitesData.txt'; websiteDataFrame <- read.table(websitesFile, sep = ',', header = F, fill = TRUE, stringsAsFactors = FALSE); #deleting of not usefull fields.  websiteDataFrame <- websiteDataFrame[, c(2:4, 6)] names(websiteDataFrame) <- c('Host', 'Webpage', 'Title', 'Text') #Take only first page from  each website websiteDataFrame <- websiteDataFrame %>% distinct(Host, .keep_all = TRUE) %>% filter(!(Title == "null" | Title == "" | Text == "null" | Text == "")) %>% select(Host, Webpage, Text, Title)
library(tm) library(dplyr) library("SnowballC") performTmFunctions <- function(websiteText) {     #Create a document for each website     #websiteDoc <- Corpus(VectorSource(websiteText))     myReader <- readTabular(mapping = list(content = "Text"))     websiteDoc <- VCorpus(DataframeSource(websiteText), readerControl = list(reader = myReader))     #Remove qqpbreakqq?     skipWords <- function(x) removeWords(x, c(stopwords("english"), "qqpbreakqq"))     #tmFuncs <- list(content_transformer(tolower), removePunctuation, removeNumbers, stripWhitespace, skipWords, stemDocument)     tmFuncs <- list(removePunctuation, removeNumbers, stripWhitespace, skipWords, stemDocument)     websiteDoc <- tm_map(websiteDoc, FUN = tm_reduce, tmFuns = tmFuncs)     TermDocumentMatrix(websiteDoc)     #Read for testing     # websiteDoc[[1]][1] } #1.Read data file and preprocessing.  websitesFile <- 'Input\\WebsitesData.txt'; websiteDataFrame <- read.table(websitesFile, sep = ',', header = F, fill = TRUE, stringsAsFactors = FALSE); #deleting of not usefull fields.  websiteDataFrame <- websiteDataFrame[, c(2:4, 6)] names(websiteDataFrame) <- c('Host', 'Webpage', 'Title', 'Text') #Take only first page from  each website
websiteDataFrame <- websiteDataFrame %>% distinct(Host, .keep_all = TRUE) %>% filter(!(Title == "null" | Title == "" | Text == "null" | Text == "")) %>% select(Host, Webpage, Text, Title)
websiteDataFrame <- websiteDataFrame %>% filter(grepl("anglo-ltd.com", Webpage))
websiteDataFrame
library(tm) library(dplyr) library("SnowballC") performTmFunctions <- function(websiteText) {     #Create a document for each website     #websiteDoc <- Corpus(VectorSource(websiteText))     myReader <- readTabular(mapping = list(content = "Text"))     websiteDoc <- VCorpus(DataframeSource(websiteText), readerControl = list(reader = myReader))     #Remove qqpbreakqq?     skipWords <- function(x) removeWords(x, c(stopwords("english"), "qqpbreakqq"))     #tmFuncs <- list(content_transformer(tolower), removePunctuation, removeNumbers, stripWhitespace, skipWords, stemDocument)     tmFuncs <- list(removePunctuation, removeNumbers, stripWhitespace, skipWords, stemDocument)     websiteDoc <- tm_map(websiteDoc, FUN = tm_reduce, tmFuns = tmFuncs)     TermDocumentMatrix(websiteDoc)     #Read for testing     # websiteDoc[[1]][1] } #1.Read data file and preprocessing.  websitesFile <- 'Input\\WebsitesData.txt'; websiteDataFrame <- read.table(websitesFile, sep = ',', header = F, fill = TRUE, stringsAsFactors = FALSE); #deleting of not usefull fields.  websiteDataFrame <- websiteDataFrame[, c(2:4, 6)] names(websiteDataFrame) <- c('Host', 'Webpage', 'Title', 'Text') #Take only first page from  each website websiteDataFrame <- websiteDataFrame %>% distinct(Host, .keep_all = TRUE) %>% filter(!(Title == "null" | Title == "" | Text == "null" | Text == "")) %>% select(Host, Webpage, Text, Title)
write.table(websiteDataFrame %>% filter(grepl("anglo-ltd.com", Webpage)), "Output\\check.txt", sep = ",", row.names = FALSE)
write.table(websiteDataFrame, "Output\\websiteDataCheck.txt", sep = ",", row.names = FALSE)
companiesCodesFile <- 'Input\\CompaniesCodes.txt'; companiesCodeDataFrame <- read.table(companiesCodesFile, sep = '\t', header = F, fill = TRUE, stringsAsFactors = FALSE); companiesCodeDataFrame <- companiesCodeDataFrame %>% select(V2, CompanyCode = V3, CompanyCodeDescription = V4) %>% filter(!(CompanyCode == "null" | CompanyCode == "" | CompanyCode == "null" | CompanyCode == "" | V2 == "null" | V2 == ""))
write.table(companiesCodeDataFrame, "Output\\companiesCodeDataFrame.txt", sep = ",", row.names = FALSE)
companiesCodesFile <- 'Input\\CompaniesCodes.txt'; companiesCodeDataFrame <- read.table(companiesCodesFile, sep = '\t', header = F, fill = TRUE, stringsAsFactors = FALSE);
head(companiesCodeDataFrame)
head(companiesCodeDataFrame)
head(companiesCodeDataFrame)
companiesCodesFile <- 'Input\\CompaniesCodes.txt'; companiesCodeDataFrame <- read.table(companiesCodesFile, sep = '\t', header = F, stringsAsFactors = FALSE); companiesCodeDataFrame <- companiesCodeDataFrame %>% select(V2, CompanyCode = V3, CompanyCodeDescription = V4) %>% filter(!(CompanyCode == "null" | CompanyCode == "" | CompanyCode == "null" | CompanyCode == "" | V2 == "null" | V2 == ""))
companiesCodesFile <- 'Input\\CompaniesCodes.txt'; companiesCodeDataFrame <- read.table(companiesCodesFile, sep = '\t', header = F, fill = TRUE, stringsAsFactors = FALSE);
head(companiesCodeDataFrame)
companiesCodeDataFrame[1,]
companiesCodesFile <- 'Input\\CompaniesCodes.txt'; companiesCodeDataFrame <- read.table(companiesCodesFile, sep = '\t', header = F, fill = TRUE, stringsAsFactors = FALSE);
companiesCodesFile <- 'Input\\CompaniesCodes.txt';
companiesCodeDataFrame <- read.table(companiesCodesFile, sep = '\t', header = F, fill = F, stringsAsFactors = FALSE);
read.table(companiesCodesFile, sep = '\t', header = F, fill = T, stringsAsFactors = FALSE);
companiesCodeDataFrame <- read.table(companiesCodesFile, sep = '\t', header = F, fill = T, stringsAsFactors = FALSE);
companiesCodeDataFrame <- read.table(companiesCodesFile, sep = '\t', header = F, fill = T, stringsAsFactors = FALSE);
companiesCodeDataFrame <- read.table(companiesCodesFile, sep = '|', header = F, fill = T, stringsAsFactors = FALSE);
?read.csv
companiesCodeDataFrame <- read.csv(companiesCodesFile, sep = '|', header = F, fill = T, stringsAsFactors = FALSE);
companiesCodeDataFrame <- read.csv(companiesCodesFile, sep = '|', stringsAsFactors = FALSE);
companiesCodeDataFrame <- read.csv(companiesCodesFile, sep = '|');
companiesCodeDataFrame <- read.csv(companiesCodesFile, sep = '|', fileEncoding ='UTF-8' );
companiesCodeDataFrame <- read.csv(companiesCodesFile, sep = '|');
companiesCodeDataFrame
head(companiesCodeDataFrame)
companiesCodeDataFrame <- read.table(companiesCodesFile, sep = '\t', header = F, fill = T, stringsAsFactors = FALSE);
companiesCodeDataFrame <- read.table(companiesCodesFile, sep = '\t', header = T, fill = T, stringsAsFactors = FALSE)
companiesCodeDataFrame <- read.table(companiesCodesFile, sep = '\t', header = T, fill = T, stringsAsFactors = FALSE, fileEncoding = "UTF-16LE");
head(companiesCodeDataFrame)
companiesCodeDataFrame[1,4]
companiesCodeDataFrame[1,3]
companiesCodeDataFrame[1,2]
companiesCodeDataFrame[1,1]
companiesCodeDataFrame[1,5]
companiesCodeDataFrame[1,6]
companiesCodeDataFrame[1,]
companiesCodesFile <- 'Input\\CompaniesCodes.txt'; companiesCodeDataFrame <- read.table(companiesCodesFile, sep = '\t', header = T, fill = T, stringsAsFactors = FALSE, fileEncoding = "UTF-16LE");
companiesCodeDataFrame[1,5]
companiesCodeDataFrame[1,4]
companiesCodeDataFrame[1,1]
companiesCodeDataFrame[1,]
companiesCodeDataFrame[1,4]
companiesCodeDataFrame[1,1]
companiesCodeDataFrame[1,]
companiesCodeDataFrame[2,]
companiesCodeDataFrame[3,]
companiesCodeDataFrame[4,]
companiesCodeDataFrame[1,]
companiesCodeDataFrame[4,]
companiesCodeDataFrame <- read.table(companiesCodesFile, sep = '\t', header = T, fill = T, stringsAsFactors = FALSE, fileEncoding = "UTF-16LE");
head(companiesCodeDataFrame)
companiesCodeDataFrame <- read.table(companiesCodesFile, sep = '\t', header = T, fill = T, stringsAsFactors = FALSE);
companiesCodeDataFrame <- read.table(companiesCodesFile, sep = '\t', header = T, fill = T, stringsAsFactors = FALSE, fileEncoding = "UTF-16LE");
companiesCodeDataFrame <- read.table(companiesCodesFile, sep = '\t', header = T, fill = T, stringsAsFactors = FALSE);
companiesCodeDataFrame <- read.table(companiesCodesFile, sep = '\t', header = T, fill = T, stringsAsFactors = FALSE, fileEncoding = "UTF-16LE");
companiesCodeDataFrame <- read.table(companiesCodesFile, sep = '\t', header = T, fill = T, stringsAsFactors = FALSE);
companiesCodeDataFrame <- read.table(companiesCodesFile, sep = '\t', header = T, fill = T, stringsAsFactors = FALSE, skipNul = TRUE);
companiesCodeDataFrame <- read.table(companiesCodesFile, sep = '\t', header = T, fill = T, stringsAsFactors = FALSE, skipNul = TRUE, quote = "")
head(companiesCodeDataFrame)
companiesCodeDataFrame <- read.table(companiesCodesFile, sep = '\t', header = T, fill = T, stringsAsFactors = FALSE,  quote = "")
write.table(companiesCodeDataFrame, "Output\\websiteDataCheck.txt", sep = ",", row.names = FALSE)
write.table(companiesCodeDataFrame, "Output\\websiteDataCheck.txt", sep = ",", row.names = FALSE)
head(companiesCodeDataFrame)
companiesCodeDataFrame <- read.table(companiesCodesFile, sep = '\t', header = T, fill = T, stringsAsFactors = FALSE, skipNul = TRUE, quote = "");
write.table(companiesCodeDataFrame, "Output\\websiteDataCheck.txt", sep = ",", row.names = FALSE)
ompaniesCodeDataFrame <- companiesCodeDataFrame %>% select(V2, CompanyCode = V3, CompanyCodeDescription = V4) %>% filter(!(CompanyCode == "null" | CompanyCode == "" | CompanyCode == "null" | CompanyCode == "" | V2 == "null" | V2 == ""))
companiesCodeDataFrame <- read.table(companiesCodesFile, sep = '\t', header = F, fill = T, stringsAsFactors = FALSE, skipNul = TRUE, quote = "");
head(companiesCodeDataFrame)
companiesCodeDataFrame <- read.table(companiesCodesFile, sep = '\t', header = F, fill = T, stringsAsFactors = FALSE, skipNul = TRUE, quote = "");
head(companiesCodeDataFrame)
write.table(companiesCodeDataFrame, "Output\\websiteDataCheck.txt", sep = ",", row.names = FALSE)
library(tm) library(dplyr) library("SnowballC") performTmFunctions <- function(websiteText) {     #Create a document for each website     #websiteDoc <- Corpus(VectorSource(websiteText))     myReader <- readTabular(mapping = list(content = "Text"))     websiteDoc <- VCorpus(DataframeSource(websiteText), readerControl = list(reader = myReader))     #Remove qqpbreakqq?     skipWords <- function(x) removeWords(x, c(stopwords("english"), "qqpbreakqq"))     #tmFuncs <- list(content_transformer(tolower), removePunctuation, removeNumbers, stripWhitespace, skipWords, stemDocument)     tmFuncs <- list(removePunctuation, removeNumbers, stripWhitespace, skipWords, stemDocument)     websiteDoc <- tm_map(websiteDoc, FUN = tm_reduce, tmFuns = tmFuncs)     TermDocumentMatrix(websiteDoc)     #Read for testing     # websiteDoc[[1]][1] } #1.Read data file and preprocessing.  websitesFile <- 'Input\\WebsitesData.txt'; websiteDataFrame <- read.table(websitesFile, sep = ',', header = F, fill = TRUE, stringsAsFactors = FALSE); #deleting of not usefull fields.  websiteDataFrame <- websiteDataFrame[, c(2:4, 6)] names(websiteDataFrame) <- c('Host', 'Webpage', 'Title', 'Text') #Take only first page from  each website websiteDataFrame <- websiteDataFrame %>% distinct(Host, .keep_all = TRUE) %>% filter(!(Title == "null" | Title == "" | Text == "null" | Text == "")) %>% select(Host, Webpage, Text, Title)
companiesCodesFile <- 'Input\\CompaniesCodes.txt'; companiesCodeDataFrame <- read.table(companiesCodesFile, sep = '\t', header = F, fill = T, stringsAsFactors = FALSE, skipNul = TRUE, quote = ""); companiesCodeDataFrame <- companiesCodeDataFrame %>% select(V2, CompanyCode = V3, CompanyCodeDescription = V4) %>% filter(!(CompanyCode == "null" | CompanyCode == "" | CompanyCode == "null" | CompanyCode == "" | V2 == "null" | V2 == ""))
mergedData <- websiteDataFrame %>% inner_join(companiesCodeDataFrame, by = c("Webpage" = "V2")) %>% filter(!(CompanyCode == "null" | CompanyCode == "" | CompanyCode == "null" | CompanyCode == "" | Webpage == "null" | Webpage == "")) %>% distinct(Webpage, .keep_all = TRUE)
write.table(mergedData, "Output\\MergedCompaniesData.txt", sep = ",", row.names = FALSE)
websitesFile <- 'Output\\MergedCompaniesData.txt'; classificationFullData <- read.table(websitesFile, sep = ',', header = F, fill = TRUE, stringsAsFactors = FALSE);
websitesFile <- 'Output\\MergedCompaniesData.txt'; classificationFullData <- read.table(websitesFile, sep = ',', header = F, fill = T, stringsAsFactors = FALSE, skipNul = TRUE, quote = "");
library(e1071) performTmFunctions <- function(websiteText) {     #Create a document for each website     #websiteDoc <- Corpus(VectorSource(websiteText))     myReader <- readTabular(mapping = list(content = "Text"))     websiteDoc <- VCorpus(DataframeSource(websiteText), readerControl = list(reader = myReader))     #Remove qqpbreakqq?     skipWords <- function(x) removeWords(x, c(stopwords("english"), "qqpbreakqq"))     #tmFuncs <- list(content_transformer(tolower), removePunctuation, removeNumbers, stripWhitespace, skipWords, stemDocument)     tmFuncs <- list(removePunctuation, removeNumbers, stripWhitespace, skipWords, stemDocument)     websiteDoc <- tm_map(websiteDoc, FUN = tm_reduce, tmFuns = tmFuncs)     TermDocumentMatrix(websiteDoc)     #Read for testing     # websiteDoc[[1]][1] }
head(classificationFullData)
head(mergedData)
classificationFullData <- mergedData
classificationFullData$CompanyCode
TDWebsitesData <- performTmFunctions(classificationFullData) m <- as.matrix(TDWebsitesData)
head(t(m))
dim(classificationFullData)
head(classificationFullData)
trainData <- classificationFullData %>% sample_n(550, replace = FALSE) textData <- anti_join(classificationFullData, trainData, by = "Host")
dim(trainData)
TDWebsitesData <- performTmFunctions(trainData) train_m <- as.matrix(TDWebsitesData) industryCodesTrain <- trainData$CompanyCode svmFit <- svm(t(train_m), industryCodesTrain, kernel = "linear")
class(industryCodesTrain)
TDWebsitesData <- performTmFunctions(trainData) train_m <- as.matrix(TDWebsitesData) industryCodesTrain <- as.factor(trainData$CompanyCode) svmFit <- svm(t(train_m), industryCodesTrain, kernel = "linear", type = "C - classification")
TDWebsitesData <- performTmFunctions(trainData) train_m <- as.matrix(TDWebsitesData) industryCodesTrain <- as.factor(trainData$CompanyCode) svmFit <- svm(t(train_m), industryCodesTrain, kernel = "linear", type = "C")
TDWebsitesData <- performTmFunctions(testData) test_m <- as.matrix(TDWebsitesData) industryCodesTrain <- as.factor(testData$CompanyCode) svmFit <- predict(svmFit, test_m)
testData <- anti_join(classificationFullData, trainData, by = "Host")
TDWebsitesData <- performTmFunctions(testData) test_m <- as.matrix(TDWebsitesData) industryCodesTrain <- as.factor(testData$CompanyCode) svmFit <- predict(svmFit, test_m)
TDWebsitesData <- performTmFunctions(testData) test_m <- as.matrix(TDWebsitesData)
industryCodesTrain <- as.factor(testData$CompanyCode)
head(t(test_m))
svmPred <- predict(svmFit, t(test_m))
test_m = factor(t(test_m), levels = c(t(train_m)))
svmPred <- predict(svmFit, t(test_m))
head(test_m)
testData <- anti_join(classificationFullData, trainData, by = "Host")
TDWebsitesData <- performTmFunctions(testData) test_m <- as.matrix(TDWebsitesData) industryCodesTrain <- as.factor(testData$CompanyCode)
head(test_m)
train_m$Terms
train_m[,Terms]
train_m[,]$Terms
train_m[,1]
head(train_m)
names(train_m)
names(t(train_m))
t(train_m)
library(e1071) performTmFunctions <- function(websiteText) {     #Create a document for each website     #websiteDoc <- Corpus(VectorSource(websiteText))     myReader <- readTabular(mapping = list(content = "Text"))     websiteDoc <- VCorpus(DataframeSource(websiteText), readerControl = list(reader = myReader))     #Remove qqpbreakqq?     skipWords <- function(x) removeWords(x, c(stopwords("english"), "qqpbreakqq"))     #tmFuncs <- list(content_transformer(tolower), removePunctuation, removeNumbers, stripWhitespace, skipWords, stemDocument)     tmFuncs <- list(removePunctuation, removeNumbers, stripWhitespace, skipWords, stemDocument)     websiteDoc <- tm_map(websiteDoc, FUN = tm_reduce, tmFuns = tmFuncs)     TermDocumentMatrix(websiteDoc)     #Read for testing     # websiteDoc[[1]][1] } #websitesText <- websiteDataFrame[, 'Text'] #websitesFile <- 'Output\\MergedCompaniesData.txt'; #classificationFullData <- read.table(websitesFile, sep = ',', header = F, fill = T, stringsAsFactors = FALSE, skipNul = TRUE, quote = ""); classificationFullData <- mergedData #dim(classificationFullData)  trainData <- classificationFullData %>% sample_n(550, replace = FALSE) testData <- anti_join(classificationFullData, trainData, by = "Host")
library(tm) library(dplyr) library("SnowballC")
library(e1071) performTmFunctions <- function(websiteText) {     #Create a document for each website     #websiteDoc <- Corpus(VectorSource(websiteText))     myReader <- readTabular(mapping = list(content = "Text"))     websiteDoc <- VCorpus(DataframeSource(websiteText), readerControl = list(reader = myReader))     #Remove qqpbreakqq?     skipWords <- function(x) removeWords(x, c(stopwords("english"), "qqpbreakqq"))     #tmFuncs <- list(content_transformer(tolower), removePunctuation, removeNumbers, stripWhitespace, skipWords, stemDocument)     tmFuncs <- list(removePunctuation, removeNumbers, stripWhitespace, skipWords, stemDocument)     websiteDoc <- tm_map(websiteDoc, FUN = tm_reduce, tmFuns = tmFuncs)     TermDocumentMatrix(websiteDoc)     #Read for testing     # websiteDoc[[1]][1] } #websitesText <- websiteDataFrame[, 'Text'] #websitesFile <- 'Output\\MergedCompaniesData.txt'; #classificationFullData <- read.table(websitesFile, sep = ',', header = F, fill = T, stringsAsFactors = FALSE, skipNul = TRUE, quote = ""); classificationFullData <- mergedData #dim(classificationFullData)  trainData <- classificationFullData %>% sample_n(550, replace = FALSE) testData <- anti_join(classificationFullData, trainData, by = "Host")
library(e1071) performTmFunctions <- function(websiteText) {     #Create a document for each website     #websiteDoc <- Corpus(VectorSource(websiteText))     myReader <- readTabular(mapping = list(content = "Text"))     websiteDoc <- VCorpus(DataframeSource(websiteText), readerControl = list(reader = myReader))     #Remove qqpbreakqq?     skipWords <- function(x) removeWords(x, c(stopwords("english"), "qqpbreakqq"))     #tmFuncs <- list(content_transformer(tolower), removePunctuation, removeNumbers, stripWhitespace, skipWords, stemDocument)     tmFuncs <- list(removePunctuation, removeNumbers, stripWhitespace, skipWords, stemDocument)     websiteDoc <- tm_map(websiteDoc, FUN = tm_reduce, tmFuns = tmFuncs)     TermDocumentMatrix(websiteDoc)     #Read for testing     # websiteDoc[[1]][1] } #websitesText <- websiteDataFrame[, 'Text'] #websitesFile <- 'Output\\MergedCompaniesData.txt'; #classificationFullData <- read.table(websitesFile, sep = ',', header = F, fill = T, stringsAsFactors = FALSE, skipNul = TRUE, quote = ""); classificationFullData <- mergedData #dim(classificationFullData)  trainData <- classificationFullData %>% sample_n(550, replace = FALSE) testData <- anti_join(classificationFullData, trainData, by = "Host")
library(tm) library(dplyr) library("SnowballC")
library(e1071) performTmFunctions <- function(websiteText) {     #Create a document for each website     #websiteDoc <- Corpus(VectorSource(websiteText))     myReader <- readTabular(mapping = list(content = "Text"))     websiteDoc <- VCorpus(DataframeSource(websiteText), readerControl = list(reader = myReader))     #Remove qqpbreakqq?     skipWords <- function(x) removeWords(x, c(stopwords("english"), "qqpbreakqq"))     #tmFuncs <- list(content_transformer(tolower), removePunctuation, removeNumbers, stripWhitespace, skipWords, stemDocument)     tmFuncs <- list(removePunctuation, removeNumbers, stripWhitespace, skipWords, stemDocument)     websiteDoc <- tm_map(websiteDoc, FUN = tm_reduce, tmFuns = tmFuncs)     TermDocumentMatrix(websiteDoc)     #Read for testing     # websiteDoc[[1]][1] }
classificationFullData <- read.table(websitesFile, sep = ',', header = F, fill = T, stringsAsFactors = FALSE, skipNul = TRUE, quote = "");
websitesFile <- 'Output\\MergedCompaniesData.txt'; classificationFullData <- read.table(websitesFile, sep = ',', header = F, fill = T, stringsAsFactors = FALSE, skipNul = TRUE, quote = "");
head(classificationFullData)
head(classificationFullData)
websitesFile <- 'Output\\MergedCompaniesData.txt'; classificationFullData <- read.table(websitesFile, sep = '\t', header = F, fill = T, stringsAsFactors = FALSE, skipNul = TRUE, quote = "");
head(classificationFullData)
names(classificationFullData)
classificationFullData <- read.table(websitesFile, sep = '\t', header = T, fill = T, stringsAsFactors = FALSE, skipNul = TRUE, quote = "");
names(classificationFullData)
head(classificationFullData)
classificationFullData <- read.table(websitesFile, sep = '\t', header = T, fill = T, stringsAsFactors = FALSE, quote = "");
head(classificationFullData)
classificationFullData <- read.table(websitesFile, sep = '\t', header = T, stringsAsFactors = FALSE, quote = "");
classificationFullData <- read.table(websitesFile, sep = '\t', header = T, fill = T, stringsAsFactors = FALSE);
websitesFile <- 'Output\\MergedCompaniesData.txt'; classificationFullData <- read.table(websitesFile, sep = '\t', header = T, fill = T, stringsAsFactors = FALSE, quote = "");
classificationFullData
names(classificationFullData)
sum(is.na(classificationFullData$X.1))
sum(classificationFullData$X.1 == "null")
sum(classificationFullData$X.1 != "null")
classificationFullData$X.1
sum(classificationFullData$X.1 == "null" | classificationFullData$X.1 == "")
dim(classificationFullData)
sum(classificationFullData$X == "null" | classificationFullData$X == "")
sum(classificationFullData$X2 == "null" | classificationFullData$X2 == "")
sum(classificationFullData$X2 == "null" | classificationFullData$X2 == "")
classificationFullData$X2
classificationFullData$X.4
classificationFullData$X.5
classificationFullData$X.4
names(classificationFullData)
names(classificationFullData[1:6])
classificationFullData <- read.table(websitesFile, sep = '\t', header = T, fill = T, stringsAsFactors = FALSE, quote = "");
names(classificationFullData)
classificationFullData <- classificationFullData[1:6,]
names(classificationFullData)
head(classificationFullData)
dim(classificationFullData)
websitesFile <- 'Output\\MergedCompaniesData.txt'; classificationFullData <- read.table(websitesFile, sep = '\t', header = T, fill = T, stringsAsFactors = FALSE, quote = "");
classificationFullData <- classificationFullData[, 1:6]
names(classificationFullData)
head(classificationFullData)
head(classificationFullData, 15)
head(classificationFullData)
head(classificationFullData, 15)
head(classificationFullData, 25)
dim(classificationFullData)
trainData <- classificationFullData %>% sample_n(550, replace = FALSE)
testData <- anti_join(classificationFullData, trainData, by = "Host")
TDWebsitesData <- performTmFunctions(trainData) train_m <- as.matrix(TDWebsitesData) industryCodesTrain <- as.factor(trainData$CompanyCode)
head(industryCodesTrain)
svmFit <- svm(t(train_m), industryCodesTrain, kernel = "linear", type = "C")
library(e1071) performTmFunctions <- function(websiteText) {     #Create a document for each website     #websiteDoc <- Corpus(VectorSource(websiteText))     myReader <- readTabular(mapping = list(content = "Text"))     websiteDoc <- VCorpus(DataframeSource(websiteText), readerControl = list(reader = myReader))     #Remove qqpbreakqq?     skipWords <- function(x) removeWords(x, c(stopwords("english"), "qqpbreakqq"))     #tmFuncs <- list(content_transformer(tolower), removePunctuation, removeNumbers, stripWhitespace, skipWords, stemDocument)     tmFuncs <- list(removePunctuation, removeNumbers, stripWhitespace, skipWords, stemDocument)     websiteDoc <- tm_map(websiteDoc, FUN = tm_reduce, tmFuns = tmFuncs)     DocumentTermMatrix(websiteDoc)     #Read for testing     # websiteDoc[[1]][1] } #websitesText <- websiteDataFrame[, 'Text'] websitesFile <- 'Output\\MergedCompaniesData.txt'; classificationFullData <- read.table(websitesFile, sep = '\t', header = T, fill = T, stringsAsFactors = FALSE, quote = ""); #take out the trash classificationFullData <- classificationFullData[, 1:6] #classificationFullData <- mergedData #dim(classificationFullData)  trainData <- classificationFullData %>% sample_n(550, replace = FALSE) testData <- anti_join(classificationFullData, trainData, by = "Host")
library(e1071) library(tm) library(dplyr) performTmFunctions <- function(websiteText) {     #Create a document for each website     #websiteDoc <- Corpus(VectorSource(websiteText))     myReader <- readTabular(mapping = list(content = "Text"))     websiteDoc <- VCorpus(DataframeSource(websiteText), readerControl = list(reader = myReader))     #Remove qqpbreakqq?     skipWords <- function(x) removeWords(x, c(stopwords("english"), "qqpbreakqq"))     #tmFuncs <- list(content_transformer(tolower), removePunctuation, removeNumbers, stripWhitespace, skipWords, stemDocument)     tmFuncs <- list(removePunctuation, removeNumbers, stripWhitespace, skipWords, stemDocument)     websiteDoc <- tm_map(websiteDoc, FUN = tm_reduce, tmFuns = tmFuncs)     DocumentTermMatrix(websiteDoc)     #Read for testing     # websiteDoc[[1]][1] } #websitesText <- websiteDataFrame[, 'Text'] websitesFile <- 'Output\\MergedCompaniesData.txt'; classificationFullData <- read.table(websitesFile, sep = '\t', header = T, fill = T, stringsAsFactors = FALSE, quote = ""); #take out the trash classificationFullData <- classificationFullData[, 1:6] #classificationFullData <- mergedData #dim(classificationFullData)  trainData <- classificationFullData %>% sample_n(550, replace = FALSE) testData <- anti_join(classificationFullData, trainData, by = "Host")
performTmFunctions <- function(websiteText, inputDtm) {     #Create a document for each website     #websiteDoc <- Corpus(VectorSource(websiteText))     myReader <- readTabular(mapping = list(content = "Text"))     websiteDoc <- VCorpus(DataframeSource(websiteText), readerControl = list(reader = myReader))     #Remove qqpbreakqq?     skipWords <- function(x) removeWords(x, c(stopwords("english"), "qqpbreakqq"))     #tmFuncs <- list(content_transformer(tolower), removePunctuation, removeNumbers, stripWhitespace, skipWords, stemDocument)     tmFuncs <- list(removePunctuation, removeNumbers, stripWhitespace, skipWords, stemDocument)     websiteDoc <- tm_map(websiteDoc, FUN = tm_reduce, tmFuns = tmFuncs)     if (missing(inputDtm)) {         DocumentTermMatrix(websiteDoc, control = list(wordLengths = c(3, 7))))     } else {         DocumentTermMatrix(websiteDoc, control = list(dictionary = Terms(inputDtm), wordLengths = c(3, 7))))     }     #Read for testing     # websiteDoc[[1]][1] }
library(e1071) library(tm) library(dplyr) performTmFunctions <- function(websiteText, inputDtm) {     #Create a document for each website     #websiteDoc <- Corpus(VectorSource(websiteText))     myReader <- readTabular(mapping = list(content = "Text"))     websiteDoc <- VCorpus(DataframeSource(websiteText), readerControl = list(reader = myReader))     #Remove qqpbreakqq?     skipWords <- function(x) removeWords(x, c(stopwords("english"), "qqpbreakqq"))     #tmFuncs <- list(content_transformer(tolower), removePunctuation, removeNumbers, stripWhitespace, skipWords, stemDocument)     tmFuncs <- list(removePunctuation, removeNumbers, stripWhitespace, skipWords, stemDocument)     websiteDoc <- tm_map(websiteDoc, FUN = tm_reduce, tmFuns = tmFuncs)     if (missing(inputDtm)) {         DocumentTermMatrix(websiteDoc, control = list(wordLengths = c(3, 7)))     } else {         DocumentTermMatrix(websiteDoc, control = list(dictionary = Terms(inputDtm), wordLengths = c(3, 7)))     }     #Read for testing     # websiteDoc[[1]][1] }
websitesFile <- 'Output\\MergedCompaniesData.txt'; classificationFullData <- read.table(websitesFile, sep = '\t', header = T, fill = T, stringsAsFactors = FALSE, quote = ""); #take out the trash classificationFullData <- classificationFullData[, 1:6] #classificationFullData <- mergedData #dim(classificationFullData)  trainData <- classificationFullData %>% sample_n(550, replace = FALSE) testData <- anti_join(classificationFullData, trainData, by = "Host")
TDWebsitesData <- performTmFunctions(trainData) train_m <- as.matrix(TDWebsitesData) industryCodesTrain <- as.factor(trainData$CompanyCode)
head(industryCodesTrain)
str(industryCodesTrain)
library(e1071) library(tm) library(dplyr) performTmFunctions <- function(websiteText, inputTdm) {     #Create a document for each website     #websiteDoc <- Corpus(VectorSource(websiteText))     myReader <- readTabular(mapping = list(content = "Text"))     websiteDoc <- VCorpus(DataframeSource(websiteText), readerControl = list(reader = myReader))     #Remove qqpbreakqq?     skipWords <- function(x) removeWords(x, c(stopwords("english"), "qqpbreakqq"))     #tmFuncs <- list(content_transformer(tolower), removePunctuation, removeNumbers, stripWhitespace, skipWords, stemDocument)     tmFuncs <- list(removePunctuation, removeNumbers, stripWhitespace, skipWords, stemDocument)     websiteDoc <- tm_map(websiteDoc, FUN = tm_reduce, tmFuns = tmFuncs)     if (missing(inputDtm)) {         TermDocumentMatrix(websiteDoc, control = list(wordLengths = c(3, 7)))     } else {         TermDocumentMatrix(websiteDoc, control = list(dictionary = Terms(inputTdm), wordLengths = c(3, 7)))     }     #Read for testing     # websiteDoc[[1]][1] }
ebsitesFile <- 'Output\\MergedCompaniesData.txt'; classificationFullData <- read.table(websitesFile, sep = '\t', header = T, fill = T, stringsAsFactors = FALSE, quote = ""); #take out the trash classificationFullData <- classificationFullData[, 1:6]
#websitesText <- websiteDataFrame[, 'Text'] websitesFile <- 'Output\\MergedCompaniesData.txt'; classificationFullData <- read.table(websitesFile, sep = '\t', header = T, fill = T, stringsAsFactors = FALSE, quote = ""); #take out the trash classificationFullData <- classificationFullData[, 1:6]
trainData <- classificationFullData %>% sample_n(550, replace = FALSE) testData <- anti_join(classificationFullData, trainData, by = "Host")
TDWebsitesData <- performTmFunctions(trainData) train_m <- as.matrix(TDWebsitesData) industryCodesTrain <- as.factor(trainData$CompanyCode)
TDWebsitesData <- performTmFunctions(trainData)
performTmFunctions <- function(websiteText, inputTdm) {     #Create a document for each website     #websiteDoc <- Corpus(VectorSource(websiteText))     myReader <- readTabular(mapping = list(content = "Text"))     websiteDoc <- VCorpus(DataframeSource(websiteText), readerControl = list(reader = myReader))     #Remove qqpbreakqq?     skipWords <- function(x) removeWords(x, c(stopwords("english"), "qqpbreakqq"))     #tmFuncs <- list(content_transformer(tolower), removePunctuation, removeNumbers, stripWhitespace, skipWords, stemDocument)     tmFuncs <- list(removePunctuation, removeNumbers, stripWhitespace, skipWords, stemDocument)     websiteDoc <- tm_map(websiteDoc, FUN = tm_reduce, tmFuns = tmFuncs)     if (missing(inputTdm)) {         TermDocumentMatrix(websiteDoc, control = list(wordLengths = c(3, 7)))     } else {         TermDocumentMatrix(websiteDoc, control = list(dictionary = Terms(inputTdm), wordLengths = c(3, 7)))     }     #Read for testing     # websiteDoc[[1]][1] }
TDWebsitesData <- performTmFunctions(trainData) train_m <- as.matrix(TDWebsitesData)
head(train_m)
industryCodesTrain <- as.factor(trainData$CompanyCode) svmFit <- svm(t(train_m), industryCodesTrain, kernel = "linear", type = "C")
library(e1071) library(tm) library(dplyr) performTmFunctions <- function(websiteText, inputTdm) {     #Create a document for each website     #websiteDoc <- Corpus(VectorSource(websiteText))     myReader <- readTabular(mapping = list(content = "Text"))     websiteDoc <- VCorpus(DataframeSource(websiteText), readerControl = list(reader = myReader))     #Remove qqpbreakqq?     skipWords <- function(x) removeWords(x, c(stopwords("english"), "qqpbreakqq"))     #tmFuncs <- list(content_transformer(tolower), removePunctuation, removeNumbers, stripWhitespace, skipWords, stemDocument)     tmFuncs <- list(removePunctuation, removeNumbers, stripWhitespace, skipWords, stemDocument)     websiteDoc <- tm_map(websiteDoc, FUN = tm_reduce, tmFuns = tmFuncs)     if (missing(inputTdm)) {         TermDocumentMatrix(websiteDoc, control = list(wordLengths = c(3, 7)))     } else {         TermDocumentMatrix(websiteDoc, control = list(dictionary = Terms(inputTdm), wordLengths = c(3, 7)))     }     #Read for testing     # websiteDoc[[1]][1] } #websitesText <- websiteDataFrame[, 'Text'] websitesFile <- 'Output\\MergedCompaniesData.txt'; classificationFullData <- read.table(websitesFile, sep = '\t', header = T, fill = T, stringsAsFactors = FALSE, quote = ""); #take out the trash classificationFullData <- classificationFullData[, 1:6] #classificationFullData <- mergedData #dim(classificationFullData)  trainData <- classificationFullData %>% sample_n(550, replace = FALSE) testData <- anti_join(classificationFullData, trainData, by = "Host")
head(classificationFullData)
trainDataTDM <- performTmFunctions(trainData) train_m <- as.matrix(trainDataTDM) industryCodesTrain <- as.factor(trainData$CompanyCode) svmFit <- svm(t(train_m), industryCodesTrain, kernel = "linear", type = "C")
testDataTDM <- performTmFunctions(testData, TDWebsitesData)\
testDataTDM <- performTmFunctions(testData, TDWebsitesData)
testDataTDM <- performTmFunctions(testData, trainDataTDM)
test_m <- as.matrix(testDataTDM)
head(test_m)
industryCodesTrain <- as.factor(testData$CompanyCode)
svmPred <- predict(svmFit, t(test_m))
svmPred
str(svmPred)
head(svmPred)
svmPred$.Data
str(svmPred)
head(svmPred)
svmPred[1]
svmPred[2]
svmPred[3]
svmPred[4]
svmPred[5]
svmPred[6]
svmPred[7]
svmPred[8]
svmPred[]
svmPred[4]
svmPred[5]
svmPred[6]
industryCodesTest
industryCodesTest <- as.factor(testData$CompanyCode)
head(industryCodesTest)
summary(svmPred)
summary(svmPred[1:3])
summary(svmPred[1:2])
?accuracy
??accuracy
confusionMatrixSVM <- table(pred = svmPred, true = test_m)
dim(svmPred)
dim(svmPred)
svmPred
svmPred[]
svmPred[,]
svmPred[]
dim(svmPred[]     )
class(svmPred)
class(test_m)
head(test_m)
head(industryCodesTest)
confusionMatrixSVM <- table(pred = svmPred, true = industryCodesTest)
head(confusionMatrixSVM)
classificationErrorSVM <- 1 - sum(svmPred == industryCodesTest) / length(svmPred)
sum(svmPred == industryCodesTest)
str(industryCodesTest)
dim(industryCodesTest)
industryCodesTest <- factor(industryCodesTest, levels = svmPred)
industryCodesTest
str(industryCodesTest)
industryCodesTest <- factor(industryCodesTest, levels = levels(svmPred))
str(industryCodesTest)
classificationErrorSVM <- 1 - sum(svmPred == industryCodesTest) / length(svmPred)
classificationErrorSVM
library(e1071) library(tm) library(dplyr) performTmFunctions <- function(websiteText, inputTdm) {     #Create a document for each website     #websiteDoc <- Corpus(VectorSource(websiteText))     myReader <- readTabular(mapping = list(content = "Text"))     websiteDoc <- VCorpus(DataframeSource(websiteText), readerControl = list(reader = myReader))     #Remove qqpbreakqq?     skipWords <- function(x) removeWords(x, c(stopwords("english"), "qqpbreakqq"))     #tmFuncs <- list(content_transformer(tolower), removePunctuation, removeNumbers, stripWhitespace, skipWords, stemDocument)     tmFuncs <- list(removePunctuation, removeNumbers, stripWhitespace, skipWords, stemDocument)     websiteDoc <- tm_map(websiteDoc, FUN = tm_reduce, tmFuns = tmFuncs)     if (missing(inputTdm)) {         TermDocumentMatrix(websiteDoc, control = list(wordLengths = c(3, 7)))     } else {         TermDocumentMatrix(websiteDoc, control = list(dictionary = Terms(inputTdm), wordLengths = c(3, 7)))     }     #Read for testing     # websiteDoc[[1]][1] }
websitesFile <- 'Output\\MergedCompaniesData.txt'; classificationFullData <- read.table(websitesFile, sep = '\t', header = T, fill = T, stringsAsFactors = FALSE, quote = ""); #take out the trash classificationFullData <- classificationFullData[, 1:6] #classificationFullData <- mergedData #dim(classificationFullData)  trainData <- classificationFullData %>% sample_n(550, replace = FALSE) testData <- anti_join(classificationFullData, trainData, by = "Host")
trainDataTDM <- performTmFunctions(trainData) train_m <- as.matrix(trainDataTDM) industryCodesTrain <- as.factor(trainData$CompanyCode) svmFit <- svm(t(train_m), industryCodesTrain, kernel = "linear", type = "C")
head(svmFit)
testDataTDM <- performTmFunctions(testData, trainDataTDM) test_m <- as.matrix(testDataTDM) industryCodesTest <- as.factor(testData$CompanyCode) #test_m = factor(t(test_m), levels = c(t(train_m))) svmPred <- predict(svmFit, t(test_m))
head(svmPred)
confusionMatrixSVM <- table(pred = svmPred, true = industryCodesTest)
head(confusionMatrixSVM)
confusionMatrixSVM
confusionMatrixSVM[,1]
confusionMatrixSVM[,2]
confusionMatrixSVM[1,.]
confusionMatrixSVM
head(confusionMatrixSVM)
head(confusionMatrixSVM,15)
head(confusionMatrixSVM,25)
head(confusionMatrixSVM,50)
head(confusionMatrixSVM,30)
dim(svmPred)
str(svmPred)
str(industryCodesTest)
length(svmPred)
length(industryCodesTest)
length(industryCodesTrain)
str(industryCodesTrain)
library(e1071) library(tm) library(dplyr) performTmFunctions <- function(websiteText, inputTdm) {     #Create a document for each website     #websiteDoc <- Corpus(VectorSource(websiteText))     myReader <- readTabular(mapping = list(content = "Text"))     websiteDoc <- VCorpus(DataframeSource(websiteText), readerControl = list(reader = myReader))     #Remove qqpbreakqq?     skipWords <- function(x) removeWords(x, c(stopwords("english"), "qqpbreakqq"))     #tmFuncs <- list(content_transformer(tolower), removePunctuation, removeNumbers, stripWhitespace, skipWords, stemDocument)     tmFuncs <- list(removePunctuation, removeNumbers, stripWhitespace, skipWords, stemDocument)     websiteDoc <- tm_map(websiteDoc, FUN = tm_reduce, tmFuns = tmFuncs)     if (missing(inputTdm)) {         TermDocumentMatrix(websiteDoc, control = list(wordLengths = c(3, 7)))     } else {         TermDocumentMatrix(websiteDoc, control = list(dictionary = Terms(inputTdm), wordLengths = c(3, 7)))     }     #Read for testing     # websiteDoc[[1]][1] } #websitesText <- websiteDataFrame[, 'Text'] websitesFile <- 'Output\\MergedCompaniesData.txt'; classificationFullData <- read.table(websitesFile, sep = '\t', header = T, fill = T, stringsAsFactors = FALSE, quote = ""); #take out the trash classificationFullData <- classificationFullData[, 1:6] #classificationFullData <- mergedData #dim(classificationFullData)  trainData <- classificationFullData %>% sample_n(550, replace = FALSE) testData <- anti_join(classificationFullData, trainData, by = "Host")
trainData$CompanyCode %in% testData$CompanyCode
testData[1,]
dim(trainData)
trainData <- trainData[trainData$CompanyCode %in% testData$CompanyCode,]
dim(trainData)
trainDataTDM <- performTmFunctions(trainData) train_m <- as.matrix(trainDataTDM) industryCodesTrain <- as.factor(trainData$CompanyCode) svmFit <- svm(t(train_m), industryCodesTrain, kernel = "linear", type = "C")
testDataTDM <- performTmFunctions(testData, trainDataTDM) test_m <- as.matrix(testDataTDM) industryCodesTest <- as.factor(testData$CompanyCode) #test_m = factor(t(test_m), levels = c(t(train_m))) svmPred <- predict(svmFit, t(test_m))
classificationErrorSVM <- 1 - sum(svmPred == industryCodesTest) / length(svmPred)
classificationErrorSVM
head(table(pred = svmPred, true = industryCodesTest))
head(industryCodesTest)
str(industryCodesTest)
length(industryCodesTest)
length(industryCodesTrain)
length(industryCodesTrain)
classificationErrorSVM
testData <- trainData industryCodesTest <- industryCodesTrain testDataTDM <- performTmFunctions(testData, trainDataTDM) test_m <- as.matrix(testDataTDM) industryCodesTest <- as.factor(testData$CompanyCode) #test_m = factor(t(test_m), levels = c(t(train_m))) svmPred <- predict(svmFit, t(test_m))
1 - sum(svmPred == industryCodesTest) / length(svmPred)
testData <- trainData testDataTDM <- performTmFunctions(testData, trainDataTDM) test_m <- as.matrix(testDataTDM) industryCodesTest <- as.factor(testData$CompanyCode) #test_m = factor(t(test_m), levels = c(t(train_m))) svmPred <- predict(svmFit, t(test_m)) #Calculate confusionMatrix #confusionMatrixSVM <- table(pred = svmPred, true = industryCodesTest) #ClassificationError industryCodesTest <- factor(industryCodesTest, levels = levels(svmPred)) classificationErrorSVM <- 1 - sum(svmPred == industryCodesTest) / length(svmPred)
classificationErrorSVM
head(trainData)
library(e1071) library(tm) library(dplyr) performTmFunctions <- function(websiteText, inputTdm) {     #Create a document for each website     #websiteDoc <- Corpus(VectorSource(websiteText))     myReader <- readTabular(mapping = list(content = "Text"))     websiteDoc <- VCorpus(DataframeSource(websiteText), readerControl = list(reader = myReader))     #Remove qqpbreakqq?     skipWords <- function(x) removeWords(x, c(stopwords("english"), "qqpbreakqq"))     #tmFuncs <- list(content_transformer(tolower), removePunctuation, removeNumbers, stripWhitespace, skipWords, stemDocument)     tmFuncs <- list(removePunctuation, removeNumbers, stripWhitespace, skipWords, stemDocument)     websiteDoc <- tm_map(websiteDoc, FUN = tm_reduce, tmFuns = tmFuncs)     if (missing(inputTdm)) {         TermDocumentMatrix(websiteDoc, control = list(wordLengths = c(3, 7)))     } else {         TermDocumentMatrix(websiteDoc, control = list(dictionary = Terms(inputTdm), wordLengths = c(3, 7)))     } } websitesFile <- 'Output\\MergedCompaniesData.txt'; classificationFullData <- read.table(websitesFile, sep = '\t', header = T, fill = T, stringsAsFactors = FALSE, quote = ""); #take out the trash classificationFullData <- classificationFullData[, 1:6] #classificationFullData <- mergedData #dim(classificationFullData)  trainData <- classificationFullData %>% sample_n(550, replace = FALSE) testData <- anti_join(classificationFullData, trainData, by = "Host") #Subselect trainData with factors that from testData trainData <- trainData[trainData$CompanyCode %in% testData$CompanyCode,] #Train trainDataTDM <- performTmFunctions(trainData) train_m <- as.matrix(trainDataTDM) industryCodesTrain <- as.factor(trainData$CompanyCode) #Test #testData <- trainData testDataTDM <- performTmFunctions(testData, trainDataTDM) test_m <- as.matrix(testDataTDM) industryCodesTest <- as.factor(testData$CompanyCode) #test_m = factor(t(test_m), levels = c(t(train_m)))
head(industryCodesTest)
head(test_m)
library(e1071) library(tm) library(dplyr) performTmFunctions <- function(websiteText, inputTdm) {     #Create a document for each website     #websiteDoc <- Corpus(VectorSource(websiteText))     myReader <- readTabular(mapping = list(content = "Text"))     websiteDoc <- VCorpus(DataframeSource(websiteText), readerControl = list(reader = myReader))     #Remove qqpbreakqq?     skipWords <- function(x) removeWords(x, c(stopwords("english"), "qqpbreakqq"))     #tmFuncs <- list(content_transformer(tolower), removePunctuation, removeNumbers, stripWhitespace, skipWords, stemDocument)     tmFuncs <- list(removePunctuation, removeNumbers, stripWhitespace, skipWords, stemDocument)     websiteDoc <- tm_map(websiteDoc, FUN = tm_reduce, tmFuns = tmFuncs)     if (missing(inputTdm)) {         TermDocumentMatrix(websiteDoc, control = list(wordLengths = c(3, 7)))     } else {         TermDocumentMatrix(websiteDoc, control = list(dictionary = Terms(inputTdm), wordLengths = c(3, 7)))     } } websitesFile <- 'Output\\MergedCompaniesData.txt'; classificationFullData <- read.table(websitesFile, sep = '\t', header = T, fill = T, stringsAsFactors = FALSE, quote = ""); #take out the trash classificationFullData <- classificationFullData[, 1:6] #classificationFullData <- mergedData #dim(classificationFullData)  trainData <- classificationFullData %>% sample_n(550, replace = FALSE) testData <- anti_join(classificationFullData, trainData, by = "Host") #Subselect trainData with factors that from testData trainData <- trainData[trainData$CompanyCode %in% testData$CompanyCode,] #Train trainDataTDM <- performTmFunctions(trainData) trainMatrix <- as.matrix(trainDataTDM) industryCodesTrain <- as.factor(trainData$CompanyCode) #Test #testData <- trainData testDataTDM <- performTmFunctions(testData, trainDataTDM) testMatrix <- as.matrix(testDataTDM) industryCodesTest <- as.factor(testData$CompanyCode) #test_m = factor(t(test_m), levels = c(t(train_m)))
head(trainMatrix)
head(trainMatrix[1,])
head(trainMatrix[,1])
pca <- prcomp(trainMatrix,                  center = TRUE,                  scale. = TRUE)
pca <- prcomp(trainMatrix,                  center = TRUE)
summary(pca)
plot(pca, type = "l")
summary(trainDataPCA)
str(trainDataPCA)
trainDataPCA <- prcomp(trainMatrix,                  center = TRUE)
head(trainDataPCA)
trainDataPCA <- prcomp(t(trainMatrix),                  center = TRUE)
head(trainDataPCA)
trainDataPCA <- prcomp(t(trainMatrix),                  center = TRUE)
trainDataPCA[1, 2]
trainDataPCA[1,1]
head(trainDataPCA,2)
head(t(trainMatrix))
head(trainMatrix)
trainDataPCA <- prcomp(t(trainMatrix),                  center = TRUE)
head(trainDataPCA)
head(trainDataPCA$x)
trainDataPCAModel <- prcomp(t(trainMatrix),                  center = TRUE)
head(trainDataPCAModel$x)
head(trainDataPCAModel$x[1,5])
head(trainDataPCAModel$x[1:5,5])
head(trainDataPCAModel$x[1:5,4])
head(trainDataPCAModel$x[1:5,1:5])
head(trainDataPCAModel$x[1:10,1:5])
head(trainDataPCAModel$x[1:10,1:5])
head(trainDataPCAModel$x[1:9,1:5])
head(trainDataPCAModel$x[1:9,1:5])
head(trainDataPCAModel$x[1:15,1:5])
head(trainDataPCAModel$x[1:9,1:3])
head(trainDataPCAModel$x[1:9,1:5])
head(trainDataPCAModel$x[1:9,1:7])
head(trainDataPCA)
trainDataPCAModel <- prcomp(t(trainMatrix),                  center = TRUE)
trainDataPCA <- trainDataPCAModel$x[, 1:7];
head(trainDataPCA)
trainDataPCA <- trainDataPCAModel$x[, 1:7];
head(trainDataPCA)
str(trainDataPCA)
head(trainDataPCA)
head(trainDataPCA)
predict(trainDataPCAModel, testData)
testDataPCA <- predict(trainDataPCAModel, t(testData))
head(t(testData))
testDataPCA <- predict(trainDataPCAModel, t(testMatrix))
head(testDataPCA)
svmFit <- svm(trainDataPCA, industryCodesTrain, kernel = "linear", type = "C") svmPred <- predict(svmFit, testDataPCA)
head(trainDataPCA)
head(industryCodesTrain)
dim(industryCodesTrain)
length(industryCodesTrain)
dim(trainDataPCA)
svmFit <- svm(trainDataPCA, industryCodesTrain, kernel = "linear", type = "C")
svmPred <- predict(svmFit, testDataPCA)
testDataPCA <- predict(trainDataPCAModel, t(testMatrix))[, 1:7]
svmPred <- predict(svmFit, testDataPCA)
classificationErrorSVM <- 1 - sum(svmPred == industryCodesTest) / length(svmPred)
sum(industryCodesTest %IN% industryCodesTrain)
sum(industryCodesTest %in% industryCodesTrain)
dim(industryCodesTest)
leng(industryCodesTest)
lenght(industryCodesTest)
length(industryCodesTest)
sum(industryCodesTest %in% industryCodesTrain)
sum(!(industryCodesTest %in% industryCodesTrain))
industryCodesTest[industryCodesTest %in% industryCodesTrain]
industryCodesTest[!(industryCodesTest %in% industryCodesTrain)]
library(e1071) library(tm) library(dplyr) performTmFunctions <- function(websiteText, inputTdm) {     #Create a document for each website     #websiteDoc <- Corpus(VectorSource(websiteText))     myReader <- readTabular(mapping = list(content = "Text"))     websiteDoc <- VCorpus(DataframeSource(websiteText), readerControl = list(reader = myReader))     #Remove qqpbreakqq?     skipWords <- function(x) removeWords(x, c(stopwords("english"), "qqpbreakqq"))     #tmFuncs <- list(content_transformer(tolower), removePunctuation, removeNumbers, stripWhitespace, skipWords, stemDocument)     tmFuncs <- list(removePunctuation, removeNumbers, stripWhitespace, skipWords, stemDocument)     websiteDoc <- tm_map(websiteDoc, FUN = tm_reduce, tmFuns = tmFuncs)     if (missing(inputTdm)) {         TermDocumentMatrix(websiteDoc, control = list(wordLengths = c(3, 7)))     } else {         TermDocumentMatrix(websiteDoc, control = list(dictionary = Terms(inputTdm), wordLengths = c(3, 7)))     } } websitesFile <- 'Output\\MergedCompaniesData.txt'; classificationFullData <- read.table(websitesFile, sep = '\t', header = T, fill = T, stringsAsFactors = FALSE, quote = ""); #take out the trash classificationFullData <- classificationFullData[, 1:6] #classificationFullData <- mergedData #dim(classificationFullData)  trainData <- classificationFullData %>% sample_n(550, replace = FALSE) testData <- anti_join(classificationFullData, trainData, by = "Host") #Subselect trainData with factors that from testData trainData <- trainData[trainData$CompanyCode %in% testData$CompanyCode,] #Train trainDataTDM <- performTmFunctions(trainData) trainMatrix <- as.matrix(trainDataTDM) industryCodesTrain <- as.factor(trainData$CompanyCode) #Test #testData <- trainData testDataTDM <- performTmFunctions(testData, trainDataTDM) testMatrix <- as.matrix(testDataTDM) industryCodesTest <- as.factor(testData$CompanyCode) #test_m = factor(t(test_m), levels = c(t(train_m)))
dim(industryCodesTrain)
industryCodesTrain[industryCodesTrain %in% industryCodesTest]
industryCodesTrain[!(industryCodesTrain %in% industryCodesTest)]
trainDataPCAModel <- prcomp(t(trainMatrix), center = TRUE)
trainDataPCA <- trainDataPCAModel$x[, 1:7]; testDataPCA <- predict(trainDataPCAModel, t(testMatrix))[, 1:7]
svmFit <- svm(trainDataPCA, industryCodesTrain, kernel = "linear", type = "C")
svmPred <- predict(svmFit, testDataPCA)
classificationErrorSVM <- 1 - sum(svmPred == industryCodesTest) / length(svmPred)
industryCodesTrain[industryCodesTrain %in% industryCodesTest]
industryCodesTrain[!(industryCodesTrain %in% industryCodesTest)]
industryCodesTrain[!(industryCodesTrain %in% svmPred)]
head(testDataPCA)
head(trainDataPCA)
industryCodesTrain$levels
industryCodesTrain[]$levels
industryCodesTrain[,]levels
industryCodesTrain[]$levels
industryCodesTrain[!(industryCodesTrain %in% industryCodesTest)]
industryCodesTest[!(industryCodesTest %in% industryCodesTrain)]
library(e1071) library(tm) library(dplyr) performTmFunctions <- function(websiteText, inputTdm) {     #Create a document for each website     #websiteDoc <- Corpus(VectorSource(websiteText))     myReader <- readTabular(mapping = list(content = "Text"))     websiteDoc <- VCorpus(DataframeSource(websiteText), readerControl = list(reader = myReader))     #Remove qqpbreakqq?     skipWords <- function(x) removeWords(x, c(stopwords("english"), "qqpbreakqq"))     #tmFuncs <- list(content_transformer(tolower), removePunctuation, removeNumbers, stripWhitespace, skipWords, stemDocument)     tmFuncs <- list(removePunctuation, removeNumbers, stripWhitespace, skipWords, stemDocument)     websiteDoc <- tm_map(websiteDoc, FUN = tm_reduce, tmFuns = tmFuncs)     if (missing(inputTdm)) {         TermDocumentMatrix(websiteDoc, control = list(wordLengths = c(3, 7)))     } else {         TermDocumentMatrix(websiteDoc, control = list(dictionary = Terms(inputTdm), wordLengths = c(3, 7)))     } } websitesFile <- 'Output\\MergedCompaniesData.txt'; classificationFullData <- read.table(websitesFile, sep = '\t', header = T, fill = T, stringsAsFactors = FALSE, quote = ""); #take out the trash classificationFullData <- classificationFullData[, 1:6] #classificationFullData <- mergedData #dim(classificationFullData)  trainData <- classificationFullData %>% sample_n(550, replace = FALSE) testData <- anti_join(classificationFullData, trainData, by = "Host") #Subselect trainData with factors that from testData trainData <- trainData[trainData$CompanyCode %in% testData$CompanyCode,] testData <- testData[testData$CompanyCode %in% trainData$CompanyCode,] #Train trainDataTDM <- performTmFunctions(trainData) trainMatrix <- as.matrix(trainDataTDM) industryCodesTrain <- as.factor(trainData$CompanyCode) #Test #testData <- trainData testDataTDM <- performTmFunctions(testData, trainDataTDM) testMatrix <- as.matrix(testDataTDM) industryCodesTest <- as.factor(testData$CompanyCode) #test_m = factor(t(test_m), levels = c(t(train_m)))
trainDataPCAModel <- prcomp(t(trainMatrix), center = TRUE) trainDataPCA <- trainDataPCAModel$x[, 1:7]; testDataPCA <- predict(trainDataPCAModel, t(testMatrix))[, 1:7]
svmFit <- svm(trainDataPCA, industryCodesTrain, kernel = "linear", type = "C") svmPred <- predict(svmFit, testDataPCA)
classificationErrorSVM <- 1 - sum(svmPred == industryCodesTest) / length(svmPred)
classificationErrorSVM
library(e1071) library(tm) library(dplyr) performTmFunctions <- function(websiteText, inputTdm) {     #Create a document for each website     #websiteDoc <- Corpus(VectorSource(websiteText))     myReader <- readTabular(mapping = list(content = "Text"))     websiteDoc <- VCorpus(DataframeSource(websiteText), readerControl = list(reader = myReader))     #Remove qqpbreakqq?     skipWords <- function(x) removeWords(x, c(stopwords("english"), "qqpbreakqq"))     #tmFuncs <- list(content_transformer(tolower), removePunctuation, removeNumbers, stripWhitespace, skipWords, stemDocument)     tmFuncs <- list(removePunctuation, removeNumbers, stripWhitespace, skipWords, stemDocument)     websiteDoc <- tm_map(websiteDoc, FUN = tm_reduce, tmFuns = tmFuncs)     if (missing(inputTdm)) {         DocumentTermMatrix(websiteDoc, control = list(wordLengths = c(4, 9)))     } else {         DocumentTermMatrix(websiteDoc, control = list(dictionary = Terms(inputTdm), wordLengths = c(4, 9)))     } } websitesFile <- 'Output\\MergedCompaniesData.txt'; classificationFullData <- read.table(websitesFile, sep = '\t', header = T, fill = T, stringsAsFactors = FALSE, quote = ""); #take out the trash classificationFullData <- classificationFullData[, 1:6] #classificationFullData <- mergedData #dim(classificationFullData)  trainData <- classificationFullData %>% sample_n(550, replace = FALSE) testData <- anti_join(classificationFullData, trainData, by = "Host") #Subselect trainData with factors that from testData trainData <- trainData[trainData$CompanyCode %in% testData$CompanyCode,] testData <- testData[testData$CompanyCode %in% trainData$CompanyCode,] #Train trainDataTDM <- performTmFunctions(trainData) trainMatrix <- as.matrix(trainDataTDM) industryCodesTrain <- as.factor(trainData$CompanyCode) #Test #testData <- trainData testDataTDM <- performTmFunctions(testData, trainDataTDM) testMatrix <- as.matrix(testDataTDM) industryCodesTest <- as.factor(testData$CompanyCode) #test_m = factor(t(test_m), levels = c(t(train_m))) trainDataPreprocessed <- trainMatrix testDataPreprocessed <- testMatrix #word of cloud and statistics #fullMatrix <- rbind(trainMatrix, testMatrix) #fullMatrixIndexies <- colSums(fullMatrix) / nrow(fullMatrix) #fullMatrix <- fullMatrix[, fullMatrixIndexies > 0.05] #dim(fullMatrix) #library(wordcloud) #wordcloud(colnames(fullMatrix), colSums(fullMatrix), scale = c(5, 0.5), max.words = 200, random.order = FALSE, rot.per = 0.6, use.r.layout = FALSE, colors = brewer.pal(8, 'Dark2'));
head(industryCodesTrain)
head(trainDataPreprocessed)
?knn
?knn
?knn
??knn
library(class) # KNN model
?knn
knnPred <- knn(trainDataPreprocessed, testDataPreprocessed, industryCodesTrain)
library(e1071) library(tm) library(dplyr) performTmFunctions <- function(websiteText, inputTdm) {     #Create a document for each website     #websiteDoc <- Corpus(VectorSource(websiteText))     myReader <- readTabular(mapping = list(content = "Text"))     websiteDoc <- VCorpus(DataframeSource(websiteText), readerControl = list(reader = myReader))     #Remove qqpbreakqq?     skipWords <- function(x) removeWords(x, c(stopwords("english"), "qqpbreakqq"))     #tmFuncs <- list(content_transformer(tolower), removePunctuation, removeNumbers, stripWhitespace, skipWords, stemDocument)     tmFuncs <- list(removePunctuation, removeNumbers, stripWhitespace, skipWords, stemDocument)     websiteDoc <- tm_map(websiteDoc, FUN = tm_reduce, tmFuns = tmFuncs)     if (missing(inputTdm)) {         DocumentTermMatrix(websiteDoc, control = list(wordLengths = c(4, 9)))     } else {         DocumentTermMatrix(websiteDoc, control = list(dictionary = Terms(inputTdm), wordLengths = c(4, 9)))     } } websitesFile <- 'Output\\MergedCompaniesData.txt'; classificationFullData <- read.table(websitesFile, sep = '\t', header = T, fill = T, stringsAsFactors = FALSE, quote = ""); #take out the trash classificationFullData <- classificationFullData[, 1:6] #classificationFullData <- mergedData #dim(classificationFullData)  trainData <- classificationFullData %>% sample_n(550, replace = FALSE) testData <- anti_join(classificationFullData, trainData, by = "Host") #Subselect trainData with factors that from testData trainData <- trainData[trainData$CompanyCode %in% testData$CompanyCode,] testData <- testData[testData$CompanyCode %in% trainData$CompanyCode,] #Train trainDataTDM <- performTmFunctions(trainData) trainMatrix <- as.matrix(trainDataTDM) industryCodesTrain <- as.factor(trainData$CompanyCode) #Test #testData <- trainData testDataTDM <- performTmFunctions(testData, trainDataTDM) testMatrix <- as.matrix(testDataTDM) industryCodesTest <- as.factor(testData$CompanyCode) #test_m = factor(t(test_m), levels = c(t(train_m))) trainDataPreprocessed <- trainMatrix testDataPreprocessed <- testMatrix #word of cloud and statistics #fullMatrix <- rbind(trainMatrix, testMatrix) #fullMatrixIndexies <- colSums(fullMatrix) / nrow(fullMatrix) #fullMatrix <- fullMatrix[, fullMatrixIndexies > 0.05] #dim(fullMatrix) #library(wordcloud) #wordcloud(colnames(fullMatrix), colSums(fullMatrix), scale = c(5, 0.5), max.words = 200, random.order = FALSE, rot.per = 0.6, use.r.layout = FALSE, colors = brewer.pal(8, 'Dark2'));
library(class) # KNN model knnPred <- knn(trainDataPreprocessed, testDataPreprocessed, industryCodesTrain)
str(knnPred)
1 - sum(knnPred == industryCodesTest) / length(knnPred)
trainDataPCAModel <- prcomp(trainMatrix, center = TRUE) trainDataPCA <- trainDataPCAModel$x[, 1:15]; testDataPCA <- predict(trainDataPCAModel, testMatrix)[, 1:15] trainDataPreprocessed <- trainDataPCA testDataPreprocessed <- testDataPCA
knnPred <- knn(trainDataPreprocessed, testDataPreprocessed, industryCodesTrain) #Calculate confusionMatrix #confusionMatrixSVM <- table(pred = svmPred, true = industryCodesTest) #ClassificationError classificationErrorKnn <- 1 - sum(knnPred == industryCodesTest) / length(knnPred)
classificationErrorKnn
trainDataPCAModel <- prcomp(trainMatrix, center = TRUE) trainDataPCA <- trainDataPCAModel$x[, 1:8]; testDataPCA <- predict(trainDataPCAModel, testMatrix)[, 1:8] trainDataPreprocessed <- trainDataPCA testDataPreprocessed <- testDataPCA
knnPred <- knn(trainDataPreprocessed, testDataPreprocessed, industryCodesTrain) #Calculate confusionMatrix #confusionMatrixSVM <- table(pred = svmPred, true = industryCodesTest) #ClassificationError classificationErrorKnn <- 1 - sum(knnPred == industryCodesTest) / length(knnPred)
classificationErrorKnn
library(tm) library(dplyr) library("SnowballC") #1.Read data file and preprocessing.  websitesFile <- 'Input\\WebsitesData.txt'; websiteDataFrame <- read.table(websitesFile, sep = ',', header = F, fill = TRUE, stringsAsFactors = FALSE); #deleting of not usefull fields.  websiteDataFrame <- websiteDataFrame[, c(2:4, 6)] names(websiteDataFrame) <- c('Host', 'Webpage', 'Title', 'Text')
write.table(head(websiteDataFrame), "Output\\websiteDataFrame.txt", sep = ",", row.names = FALSE, quote = TRUE)
write.table(head(websiteDataFrame, 10000), "Output\\websiteDataFrame.txt", sep = ",", row.names = FALSE, quote = TRUE)
websiteDataFrame[,35]
websiteDataFrame[35,]
websiteDataFrame[36,]
websiteDataFrame[34:36,]
websiteDataFrame[32:39,]
websiteDataFrame[32:36,]
websiteDataFrame[33:37,]
websiteDataFrame[33:34,]
websiteDataFrame[32:34,]
websiteDataFrame[32:33,]
websiteDataFrame[35:36,]
websiteDataFrame[35:36,]
websiteDataFrame[35:36,]$Title
websiteDataFrame <- websiteDataFrame %>% distinct(Host, .keep_all = TRUE) %>% filter(!(Host == "null" | Host == "" | Title == "null" | Title == "" | Text == "null" | Text == "")) %>% select(Host, Webpage, Text, Title)
websiteDataFrame[1:5]
websiteDataFrame[1:5,]
websiteDataFrame[1:5,]$Host
websiteDataFrame[1:10,]$Host
write.table(head(websiteDataFrame,100), "Output\\websiteDataFrame2.txt", sep = ",", row.names = FALSE, quote = TRUE)
websiteDataFrame[94,]
websiteDataFrame[95,]
websiteDataFrame[96,]
websiteDataFrame[95,]
websiteDataFrame[91,]
websiteDataFrame[90,]
websiteDataFrame[89,]
websiteDataFrame[88,]
websiteDataFrame[90,]
websiteDataFrame[89,]
websiteDataFrame[89,]
websiteDataFrame[88:92,]
websiteDataFrame[89,]$Title
websitesFile <- 'Input\\WebsitesData.txt'; websiteDataFrame <- read.table(websitesFile, sep = ',', header = F, fill = TRUE, stringsAsFactors = FALSE);
websiteDataFrame[290:300,1]
websiteDataFrame[290:300,2]
websiteDataFrame[280:300,2]
websiteDataFrame[280:320,2]
websiteDataFrame[280:320,3]
websiteDataFrame[280:320,4]
websiteDataFrame[280:320,5]
websiteDataFrame[280:320,3]
websiteDataFrame[280:320,2]
websiteDataFrame[280:350,2]
websiteDataFrame[240:350,2]
library(tm) library(dplyr) library("SnowballC") #1.Read data file and preprocessing.  websitesFile <- 'Input\\WebsitesData.txt'; websiteDataFrame <- read.table(websitesFile, sep = ',', header = F, stringsAsFactors = FALSE);
websiteDataFrame <- read.table(websitesFile, sep = ',', header = F, stringsAsFactors = FALSE);
read.table(websitesFile, sep = ',', header = F, stringsAsFactors = FALSE, nrows = 25)
read.table(websitesFile, sep = ',', header = F, stringsAsFactors = FALSE, nrows = 40)
read.table(websitesFile, sep = ',', header = F, stringsAsFactors = FALSE, nrows = 32)
read.table(websitesFile, sep = ',', header = F, stringsAsFactors = FALSE, nrows = 32)[2]
read.table(websitesFile, sep = ',', header = F, stringsAsFactors = FALSE, nrows = 32)[3]
read.table(websitesFile, sep = ',', header = F, stringsAsFactors = FALSE, nrows = 33)[3]
read.table(websitesFile, sep = ',', header = F, stringsAsFactors = FALSE, nrows = 32)[3]
read.table(websitesFile, sep = ',', header = F, stringsAsFactors = FALSE, nrows = 33)[3]
read.table(websitesFile, sep = ',', header = F, stringsAsFactors = FALSE, nrows = 33)[3]
read.table(websitesFile, sep = ',', header = F, stringsAsFactors = FALSE, nrows = 32)[3]
require(plyr)
websiteDataFrame <- read.table(websitesFile, sep = ',', header = F, stringsAsFactors = FALSE, nrows = 3);\
websiteDataFrame <- read.table(websitesFile, sep = ',', header = F, stringsAsFactors = FALSE, nrows = 3);
dim(websiteDataFrame)
